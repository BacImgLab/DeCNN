{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b7fc09-e4d5-4cd8-bf61-ed10a8bd25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re  # 引入正则表达式库，用于提取数字\n",
    "import torch\n",
    "import random\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Model.ResNet50_model import resnet50\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter as swriter\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3d4786-9505-4734-83a7-fa8f26c82079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores available: 24\n",
      "Is CUDA available: True\n",
      "Total RAM: 15.49 GB\n",
      "Available RAM: 13.33 GB\n",
      "Total GPUs: 1\n",
      "Device 0: NVIDIA GeForce RTX 4070 Laptop GPU - Total memory: 8585.216 MB\n",
      "device: cuda\n",
      "(8, 9)\n",
      "Total CUDA cores: 4608\n"
     ]
    }
   ],
   "source": [
    "cpu_cores = os.cpu_count() \n",
    "print(f\"Number of CPU cores available: {cpu_cores}\")\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"Is CUDA available: {cuda_available}\")\n",
    "\n",
    "memory = psutil.virtual_memory()\n",
    "total_memory_gb = memory.total / (1024 ** 3)  # 转换为GB\n",
    "available_memory_gb = memory.available / (1024 ** 3)  # 转换为GB\n",
    "\n",
    "print(f\"Total RAM: {total_memory_gb:.2f} GB\")\n",
    "print(f\"Available RAM: {available_memory_gb:.2f} GB\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Total GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)} - Total memory: {torch.cuda.get_device_properties(i).total_memory / 1e6} MB\")\n",
    "else:\n",
    "    print(\"No CUDA GPUs are available\")\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print('device:', device)\n",
    "print(torch.cuda.get_device_capability(device))\n",
    "properties = torch.cuda.get_device_properties(device)\n",
    "\n",
    "sm_count = properties.multi_processor_count\n",
    "cores_per_sm = 128  # 每个 SM 有多少个 CUDA 核心\n",
    "total_cores = sm_count * cores_per_sm\n",
    "\n",
    "print(f\"Total CUDA cores: {total_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9e1784-ffd0-4ed3-a17e-9e0aa9924d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCropAndResize(transforms.Resize):\n",
    "    def __init__(self, original_size=120, scale_min=0.92, scale_max=1.00, interpolation=Image.BILINEAR):\n",
    "        # 2D图片使用BICUBIC(三维双线性插值：针对3D图片)会出现1：插值后的图只显示轮廓(见0714不深入) \n",
    "        super().__init__(size=(original_size, original_size), interpolation=interpolation)\n",
    "        self.original_size = original_size\n",
    "        self.scale_min = scale_min\n",
    "        self.scale_max = scale_max\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # 计算裁剪尺寸\n",
    "        scale = random.uniform(self.scale_min, self.scale_max)\n",
    "        crop_size = int(self.original_size * scale)\n",
    "\n",
    "        # 中心裁剪\n",
    "        center_crop = transforms.CenterCrop(crop_size)\n",
    "        img = center_crop(img)\n",
    "\n",
    "        # 调整尺寸回原始尺寸\n",
    "        img = super().__call__(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "# 下面自定义的数据集中__len__和__getitem__是必须被实现的方法，因为它们定义了数据集基本行为：大小和如何被索引\n",
    "class BacteriaDataset(Dataset): # 通过继承torch的dataset,使得自定义的数据集可配合torch的其它数据工具一起使用，如dataloader，使得自定义数据集兼容标准数据加载到模型的流程\n",
    "    def __init__(self, folder_path, label, transform=None, target_transform=None): # transfrom对图像的转换。 target_transform是对标签的转换：例如独热编码和标签映射等\n",
    "        self.folder_path = folder_path\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.images = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.tif')]\n",
    "\n",
    "    def __len__(self): # 该方法返回数据集元素总数，pytorch的dataloader依此决定数据集的遍历次数\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx): # 该方法根据索引idx返回数据集中的单个元素，如使用dataloader迭代数据集时.__getitem__方法的调用还可如右所示，直接datasets[3][i]\n",
    "        image_path = self.images[idx]\n",
    "        image = np.array(Image.open(image_path))  # .convert('L'):Convert to grayscale\n",
    "        image = (image/np.max(image) * 255).astype(np.uint8) # 把我的16位变成0~1再转换为8位np, 再给下面处理，不然直接给下面16位的它会给我丢失细节\n",
    "\n",
    "        # image = Image.fromarray(array, mode = 'I;16')\n",
    "        image = Image.fromarray(image, mode='L') # mode='L' 会转换为8位的PIL,所以上一些我先转成8位np以免细节丢失\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.label\n",
    "\n",
    "# 定义变换\n",
    "transform = transforms.Compose([\n",
    "    # transforms.RandomRotation(180),  # 360度旋转\n",
    "    # RandomCropAndResize(),  # 缩放操作\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "transform1 = transforms.Compose([\n",
    "    transforms.RandomRotation(180),  # 360度旋转\n",
    "    RandomCropAndResize(original_size=150),  # 缩放操作\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6efb9c71-9476-4d1f-b484-31b0587ca32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/e/wsl_share/Machine_learning/DR/DR_latest/HJ/Cellwalltest_20241219'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "train_data_path = os.path.join(os.getcwd(),'Cellwalltest_20241219')\n",
    "train_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4409c94-9037-48c5-a71c-c0c112c06d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.BacteriaDataset object at 0x7fc6c0292440>, <__main__.BacteriaDataset object at 0x7fc6cd5db790>, <__main__.BacteriaDataset object at 0x7fc6c1a2e4d0>, <__main__.BacteriaDataset object at 0x7fc6c1a2e530>, <__main__.BacteriaDataset object at 0x7fc7ac5c0970>]\n"
     ]
    }
   ],
   "source": [
    "folders = ['stage1', 'stage2', 'stage3', 'stage4', 'stage5']\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "# folders = ['error']\n",
    "# labels = [6]\n",
    "datasets = [BacteriaDataset(f\"/mnt/e/wsl_share/Machine_learning/DR/DR_latest/HJ/Cellwalltest_20241219/{folder}\", label, transform=transform) for folder, label in zip(folders, labels)]\n",
    "print(datasets)\n",
    "\n",
    "# 扩增数据并保存\n",
    "def amplify_and_save(dataset, folder_name, amplification=20):\n",
    "    # save_path = f\"./dr_cell_cycle/{folder_name}_amplification\" #0605\n",
    "    save_path = f\"/mnt/e/wsl_share/Machine_learning/DR/DR_latest/HJ/Cellwalltest_20241219/{folder_name}_amplification\" #\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True) # DataLoader已经调用了数据集的__getitem__方法，该方法包括转换，因此不需要再次转换成张量\n",
    "    for i, (image, label) in enumerate(loader, 1): # 这里image是torch.tensor (1,1,120,120)且范围是0~1 ,这是关键点，Uint16位被读取成data\n",
    "        for j in range(amplification): # (image,_) in enumerate\n",
    "            # image1 = np.array(image).reshape(140,140) #H*W\n",
    "            # image2 = (image1/np.max(image1) * 255).astype(np.uint8) # 因为transforms内的函数ToTensor接受8位的PIL或ndarray，所以这里先变成ndarray8位\n",
    "            \n",
    "            # image3 = Image.fromarray(image2, mode = 'L') #H*W # image = Image.fromarray(array, mode = 'I;16')改成这样会出错，因为transforms接受8位，头痛\n",
    "            \n",
    "            # image3 = Image.fromarray(image2, mode = 'L')\n",
    "\n",
    "            # 这里image本身是tensor情况下，transform1内的ToTensor要被注释掉\n",
    "            image4 = (transform1(image)).numpy().squeeze() # ToTensor会增加一个C维度，先C=1*H*W，再变成H*W\n",
    "            # transformed_image = dataset.transform(image.squeeze(0))  # 若图片不是张量，这里进行转换。\n",
    "            imageidx = (i-1)*amplification + j + 1\n",
    "            save_image(image4, f\"{save_path}/{folder_name}_amp_{imageidx}.tif\") # {}.tif被我去掉了\n",
    "\n",
    "def save_image(array, filename):\n",
    "    if array.dtype != np.uint16:\n",
    "        array = (array*65535).astype(np.uint16)\n",
    "    \n",
    "    image = Image.fromarray(array, mode = 'I;16')\n",
    "    image.save(filename,format = 'TIFF')\n",
    "\n",
    "Amplifications = [7, 6, 8, 50, 250]  \n",
    "# 为每个数据集执行扩增和保存，使用enumerate来获取循环迭代序号i\n",
    "for i, (dataset, folder) in enumerate(zip(datasets, folders), start=1):\n",
    "    amplification_value = Amplifications[i-1]\n",
    "    amplify_and_save(dataset, folder, amplification=amplification_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4a92d97-64b2-48fd-a312-bdbdc8123b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面自定义的数据集中__len__和__getitem__是必须被实现的方法，因为它们定义了数据集基本行为：大小和如何被索引\n",
    "class BacteriaDataset(Dataset): # 通过继承torch的dataset,使得自定义的数据集可配合torch的其它数据工具一起使用，如dataloader，使得自定义数据集兼容标准数据加载到模型的流程\n",
    "    def __init__(self, folder_path, label, transform=None, target_transform=None): # transfrom对图像的转换。 target_transform是对标签的转换：例如独热编码和标签映射等\n",
    "        self.folder_path = folder_path\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.images = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.tif')]\n",
    "\n",
    "    def __len__(self): # 该方法返回数据集元素总数，pytorch的dataloader依此决定数据集的遍历次数\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx): # 该方法根据索引idx返回数据集中的单个元素，如使用dataloader迭代数据集时.__getitem__方法的调用还可如右所示，直接datasets[3][i]\n",
    "        image_path = self.images[idx]\n",
    "        image = np.array(Image.open(image_path))  # .convert('L'):Convert to grayscale\n",
    "        image = (image/np.max(image) * 255).astype(np.uint8)\n",
    "        image = Image.fromarray(image, mode='L') # mode='L' 会转换为8位的PIL,所以上一些我先转成8位np以免细节丢失\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.label\n",
    "\n",
    "# 训练集的数据预处理变换\n",
    "transform = transforms.Compose([\n",
    "    \n",
    "    transforms.ToTensor(),  # 最后转换为tensor\n",
    "])\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 50% 的概率水平翻转\n",
    "    # transforms.RandomHorizontalFlip(), # 不填参数默认p=0.5\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # 50% 的概率垂直翻转\n",
    "    transforms.RandomRotation(degrees=(0, 180)),  # 随机旋转0到180度\n",
    "    # transforms.RandomRotation(180),  # 360度旋转,这时单一参数被理解为-180 到 + 180\n",
    "    \n",
    "    transforms.RandomResizedCrop(224, scale=(0.90, 1.0),  ratio = (1, 1)),\n",
    "    \n",
    "    # 使用Lambda创建自定义操作，来调用任何函数并指定用于该操作的参数。\n",
    "    transforms.Lambda(lambda x: transforms.functional.adjust_brightness(x, brightness_factor=random.uniform(0.8, 1.2))),\n",
    "    transforms.Lambda(lambda x: transforms.functional.adjust_contrast(x, contrast_factor=random.uniform(0.8, 1.2))),\n",
    "    \n",
    "    # transforms.RandomCrop(size=(100, 100), padding=None),  # 随机裁剪到100x100大小\n",
    "    # RandomCropAndResize(),  # 缩放操作\n",
    "    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),  # 随机仿射变换 轻微地旋转 (±10度)，在宽度和高度上平移最多 10%，缩放比例在 80% 到 120% 之间变化，并且可能轻微地沿 x 或 y 轴倾斜±10°\n",
    "    \n",
    "    # transforms.Grayscale(num_output_channels=3),  # 将单通道转换为三通道\n",
    "    # transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "    transforms.Normalize(mean=0.5, std=0.5),\n",
    "    # transforms.ToTensor()\n",
    "])\n",
    "# ColorJitter只对RGB生效，对灰度图不生效。\n",
    "# transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 颜色抖动.brightness=0.2表示0.8到1.2随机调整亮度。对比，饱和和色调同理是± 参数 \n",
    "\n",
    "\n",
    "# 验证集的数据预处理\n",
    "transform_val = transforms.Compose([\n",
    "    # 将输入图像大小调整为224*224，PIL的size返回的是w，h。注意transforms.Resize只接受PIL和torchtensor，不接受cv2和io读取的ndarray数据\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomRotation(degrees=(0, 180)),\n",
    "    # transforms.Grayscale(num_output_channels=3),  # 将单通道转换为三通道\n",
    "    transforms.Normalize(mean=0.5, std=0.5),\n",
    "    # transforms.ToTensor()\n",
    "    # transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d79dc937-2645-4bbf-ad10-048d0c0acd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['stage1_amplification', 'stage2_amplification', 'stage3_amplification', 'stage4_amplification', 'stage5_amplification']\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "oxhdatasets = [BacteriaDataset(f\"/mnt/e/wsl_share/Machine_learning/DR/DR_latest/HJ/Cellwalltest_20241219/{folder}\", label, transform=transform) for folder, label in zip(folders, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2009a5f-6017-4ec6-907f-8e382e2a86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, ConcatDataset\n",
    "# data_loader = DataLoader(dataset, batch_size=50, shuffle=False, num_workers=8)\n",
    "# 定义一个函数来分割数据集并合并\n",
    "def create_train_test_datasets(datasets, test_split=0.2):\n",
    "    train_datasets = []\n",
    "    test_datasets = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        num_total = len(dataset)\n",
    "        num_test = int(num_total * test_split)\n",
    "        num_train = num_total - num_test\n",
    "\n",
    "        train_subset, test_subset = random_split(dataset, [num_train, num_test])\n",
    "        train_datasets.append(train_subset)\n",
    "        test_datasets.append(test_subset)\n",
    "\n",
    "    # 合并所有训练集和测试集\n",
    "    train_data = ConcatDataset(train_datasets)\n",
    "    test_data = ConcatDataset(test_datasets)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = create_train_test_datasets(oxhdatasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "242f5742-68a2-4d65-b965-b438b3318892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage3\n"
     ]
    }
   ],
   "source": [
    "# DataLoader迭代时动态应用变换\n",
    "def train_collate_fn(batch):\n",
    "    transformed_batch = [(transform_train(img), label) for img, label in batch]\n",
    "    return torch.utils.data.dataloader.default_collate(transformed_batch)\n",
    "\n",
    "def test_collate_fn(batch):\n",
    "    transformed_batch = [(transform_val(img), label) for img, label in batch]\n",
    "    return torch.utils.data.dataloader.default_collate(transformed_batch)\n",
    "\n",
    "# 创建DataLoader时指定collate_fn\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=48, shuffle=True, collate_fn=train_collate_fn, num_workers = 8)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=48, shuffle=False, collate_fn=test_collate_fn, num_workers = 8)\n",
    "class_names = ['stage1', 'stage2', 'stage3', 'stage4', 'stage5']\n",
    "print(class_names[labels[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc820ff1-107a-428a-87b6-f7ff7bdc2c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch1 2\n",
      "test_batch1 2\n",
      "9058\n",
      "2263\n"
     ]
    }
   ],
   "source": [
    "train_batch1 = next(iter(train_loader)) # train_img, train_label = next(iter(train_loader)) 分别读取图片和标签\n",
    "test_batch1 = next(iter(DataLoader(dataset=test_data, batch_size=48, shuffle=False, collate_fn=test_collate_fn, num_workers = 8)))#每次加载还得数据进行数据增强\n",
    "print('train_batch1', len(train_batch1))\n",
    "print('test_batch1', len(test_batch1))\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14ec0190-832a-44de-89c1-74a6aac89e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 1, 224, 224])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76c9e943-89e4-4078-b7d8-64244a5b8e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 1 1 3 1 5 3 2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = test_batch1[0][:9]\n",
    "img = img/2 + 0.5\n",
    "img = img.permute(0,2,3,1).numpy()\n",
    "class_label = train_batch1[1][:9].numpy()\n",
    "print(class_label+1)\n",
    "plt.figure() # plt.figure(dpi = 100)\n",
    "for i in range(img.shape[0]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(img[i], cmap='gray')\n",
    "    plt.axis('off') # plt.xticks([])  plt.yticks([]) 不显示坐标轴刻度\n",
    "    plt.title(class_names[class_label[i]])\n",
    "plt.tight_layout(pad=0, w_pad = 5 , h_pad = 1) # 紧凑轻量化布局\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27b8923a-cb47-48d0-987b-807702eceac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/e/wsl_share/Machine_learning/DR/DR_latest/HJ'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cadd8128-5912-42b1-a4a6-4c9763e75172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oxhnet.fc.in_features: 2048\n",
      "oxhnet.conv1.in_features: 3\n",
      "after revied, oxhnet.fc:  Linear(in_features=2048, out_features=5, bias=True)\n",
      "after revied, oxhnet.conv1:  Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightpath = '/root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth'# 1000分类预训练网络参数\n",
    "savepath = '/mnt/e/wsl_share/Machine_learning/DR/DR_latest/HJ/Model/'\n",
    "oxhnet = resnet50(num_classes = 1000) # 接受预训练的网络1000分类的网络权重参数\n",
    "oxhnet.load_state_dict(torch.load(weightpath, map_location=device))\n",
    "print('oxhnet.fc.in_features:', oxhnet.fc.in_features)\n",
    "print('oxhnet.conv1.in_features:', oxhnet.conv1.in_channels)\n",
    "in_channel = oxhnet.fc.in_features\n",
    "in_channel0 = oxhnet.conv1.in_channels\n",
    "oxhnet.fc = nn.Linear(in_channel, 5) # 只有全连接层的参数全部初始化了，其余层仍旧保留迁移过来的权重\n",
    "oxhnet.conv1 = nn.Conv2d(1, 64,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "print('after revied, oxhnet.fc: ', oxhnet.fc)\n",
    "print('after revied, oxhnet.conv1: ', oxhnet.conv1)\n",
    "oxhnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ba3f05b-ae6e-4d22-8d37-a1d4c895d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化tensorboard\n",
    "writer = swriter('runs/20241221_ResNet50_5features_ForDR(CellWall)')\n",
    "dummy_input = torch.randn(1,1,224,224).to(device=device, dtype=torch.float32)\n",
    "# 模型计算图添加到tensorboard\n",
    "writer.add_graph(oxhnet, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e627d53-3c04-4167-91c2-e4d4f5cd424c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ \n",
      " epoch: 0\n",
      "step:0 loss:1.6035271883010864\n",
      "step:1 loss:1.6302465200424194\n",
      "step:2 loss:1.3690825700759888\n",
      "step:3 loss:1.3540573120117188\n",
      "step:4 loss:1.2592564821243286\n",
      "step:5 loss:1.2845605611801147\n",
      "step:6 loss:1.079199194908142\n",
      "step:7 loss:1.0090237855911255\n",
      "step:8 loss:1.0924924612045288\n",
      "step:9 loss:0.8852604031562805\n",
      "step:10 loss:1.1348447799682617\n",
      "step:11 loss:0.6175020337104797\n",
      "step:12 loss:0.7166128754615784\n",
      "step:13 loss:0.7342467904090881\n",
      "step:14 loss:0.9203248023986816\n",
      "step:15 loss:1.0118359327316284\n",
      "step:16 loss:0.7615394592285156\n",
      "step:17 loss:0.7947375178337097\n",
      "step:18 loss:0.6418563723564148\n",
      "step:19 loss:0.6670253872871399\n",
      "step:20 loss:0.9450673460960388\n",
      "step:21 loss:0.6639299988746643\n",
      "step:22 loss:0.7289277911186218\n",
      "step:23 loss:0.6399421691894531\n",
      "step:24 loss:0.9038012027740479\n",
      "step:25 loss:0.8408329486846924\n",
      "step:26 loss:0.7358719706535339\n",
      "step:27 loss:0.754784882068634\n",
      "step:28 loss:0.8503828048706055\n",
      "step:29 loss:0.9582095146179199\n",
      "step:30 loss:0.7272964119911194\n",
      "step:31 loss:0.6855250000953674\n",
      "step:32 loss:0.800893247127533\n",
      "step:33 loss:0.6559322476387024\n",
      "step:34 loss:1.0641251802444458\n",
      "step:35 loss:1.0984313488006592\n",
      "step:36 loss:0.6865226626396179\n",
      "step:37 loss:0.7505607604980469\n",
      "step:38 loss:0.4257132112979889\n",
      "step:39 loss:0.7121500968933105\n",
      "step:40 loss:0.7150483131408691\n",
      "step:41 loss:0.6800361275672913\n",
      "step:42 loss:0.5491033792495728\n",
      "step:43 loss:0.5409673452377319\n",
      "step:44 loss:0.5657687783241272\n",
      "step:45 loss:0.6446678638458252\n",
      "step:46 loss:0.5055676102638245\n",
      "step:47 loss:0.5463781952857971\n",
      "step:48 loss:0.5578150153160095\n",
      "step:49 loss:0.5332308411598206\n",
      "step:50 loss:1.0128264427185059\n",
      "step:51 loss:0.691612184047699\n",
      "step:52 loss:0.6918759942054749\n",
      "step:53 loss:0.6148528456687927\n",
      "step:54 loss:0.9162323474884033\n",
      "step:55 loss:0.68551105260849\n",
      "step:56 loss:0.7877891063690186\n",
      "step:57 loss:0.6250117421150208\n",
      "step:58 loss:0.720332682132721\n",
      "step:59 loss:0.6633652448654175\n",
      "step:60 loss:0.6598546504974365\n",
      "step:61 loss:0.7109112739562988\n",
      "step:62 loss:0.8833937048912048\n",
      "step:63 loss:0.6203487515449524\n",
      "step:64 loss:0.6569925546646118\n",
      "step:65 loss:0.7112665772438049\n",
      "step:66 loss:0.5276623368263245\n",
      "step:67 loss:0.5597721338272095\n",
      "step:68 loss:0.6535472273826599\n",
      "step:69 loss:0.5084403157234192\n",
      "step:70 loss:0.4991956651210785\n",
      "step:71 loss:0.5179165005683899\n",
      "step:72 loss:0.6385488510131836\n",
      "step:73 loss:0.5382310748100281\n",
      "step:74 loss:0.5043063759803772\n",
      "step:75 loss:0.5861735939979553\n",
      "step:76 loss:0.5288892984390259\n",
      "step:77 loss:0.7622023224830627\n",
      "step:78 loss:0.4602707624435425\n",
      "step:79 loss:0.5157200694084167\n",
      "step:80 loss:0.6680881381034851\n",
      "step:81 loss:0.7849707007408142\n",
      "step:82 loss:0.7294159531593323\n",
      "step:83 loss:0.378246933221817\n",
      "step:84 loss:0.470324844121933\n",
      "step:85 loss:0.4495116174221039\n",
      "step:86 loss:0.5509077906608582\n",
      "step:87 loss:0.48395082354545593\n",
      "step:88 loss:0.7150156497955322\n",
      "step:89 loss:0.7555748820304871\n",
      "step:90 loss:0.5002667307853699\n",
      "step:91 loss:0.6236212253570557\n",
      "step:92 loss:0.7343688011169434\n",
      "step:93 loss:0.5908113121986389\n",
      "step:94 loss:0.4706628620624542\n",
      "step:95 loss:0.39961060881614685\n",
      "step:96 loss:0.6788008809089661\n",
      "step:97 loss:0.5862445831298828\n",
      "step:98 loss:0.5129981637001038\n",
      "step:99 loss:0.5300940871238708\n",
      "step:100 loss:0.5476932525634766\n",
      "step:101 loss:0.4660189151763916\n",
      "step:102 loss:0.37637069821357727\n",
      "step:103 loss:0.7147007584571838\n",
      "step:104 loss:0.37738433480262756\n",
      "step:105 loss:0.4797523021697998\n",
      "step:106 loss:0.49012836813926697\n",
      "step:107 loss:0.5911641716957092\n",
      "step:108 loss:0.4989347755908966\n",
      "step:109 loss:0.6257793307304382\n",
      "step:110 loss:0.8685155510902405\n",
      "step:111 loss:0.5985997319221497\n",
      "step:112 loss:0.43859735131263733\n",
      "step:113 loss:0.44777944684028625\n",
      "step:114 loss:0.6362510919570923\n",
      "step:115 loss:0.47008803486824036\n",
      "step:116 loss:0.37631526589393616\n",
      "step:117 loss:0.5376529097557068\n",
      "step:118 loss:0.7042757868766785\n",
      "step:119 loss:0.49572908878326416\n",
      "step:120 loss:0.41799333691596985\n",
      "step:121 loss:0.6615133285522461\n",
      "step:122 loss:0.7678702473640442\n",
      "step:123 loss:0.35887131094932556\n",
      "step:124 loss:0.5386247634887695\n",
      "step:125 loss:0.35066890716552734\n",
      "step:126 loss:0.5751237869262695\n",
      "step:127 loss:0.4438653886318207\n",
      "step:128 loss:0.34492120146751404\n",
      "step:129 loss:0.4888499677181244\n",
      "step:130 loss:0.41223108768463135\n",
      "step:131 loss:0.4051666557788849\n",
      "step:132 loss:0.5297499299049377\n",
      "step:133 loss:0.44512131810188293\n",
      "step:134 loss:0.41657331585884094\n",
      "step:135 loss:0.5255210399627686\n",
      "step:136 loss:0.312717467546463\n",
      "step:137 loss:0.5356754660606384\n",
      "step:138 loss:0.522401750087738\n",
      "step:139 loss:0.33450761437416077\n",
      "step:140 loss:0.5752873420715332\n",
      "step:141 loss:0.428401917219162\n",
      "step:142 loss:0.485411673784256\n",
      "step:143 loss:0.5126129984855652\n",
      "step:144 loss:0.7829003930091858\n",
      "step:145 loss:0.4771672785282135\n",
      "step:146 loss:0.3521934449672699\n",
      "step:147 loss:0.4907402992248535\n",
      "step:148 loss:0.5150998830795288\n",
      "step:149 loss:0.32331064343452454\n",
      "step:150 loss:0.38889774680137634\n",
      "step:151 loss:0.4777888059616089\n",
      "step:152 loss:0.445218950510025\n",
      "step:153 loss:0.5293193459510803\n",
      "step:154 loss:0.5656747221946716\n",
      "step:155 loss:0.4419287145137787\n",
      "step:156 loss:0.4985753297805786\n",
      "step:157 loss:0.36265525221824646\n",
      "step:158 loss:0.4607026278972626\n",
      "step:159 loss:0.6031165719032288\n",
      "step:160 loss:0.3958611786365509\n",
      "step:161 loss:0.3681011199951172\n",
      "step:162 loss:0.589579701423645\n",
      "step:163 loss:0.4910348653793335\n",
      "step:164 loss:0.3097992539405823\n",
      "step:165 loss:0.32352080941200256\n",
      "step:166 loss:0.3852798044681549\n",
      "step:167 loss:0.3951821029186249\n",
      "step:168 loss:0.46815192699432373\n",
      "step:169 loss:0.3887176811695099\n",
      "step:170 loss:0.47791627049446106\n",
      "step:171 loss:0.7480129599571228\n",
      "step:172 loss:0.5513911843299866\n",
      "step:173 loss:0.4845602214336395\n",
      "step:174 loss:0.5233290195465088\n",
      "step:175 loss:0.28733593225479126\n",
      "step:176 loss:0.4147154986858368\n",
      "step:177 loss:0.438130259513855\n",
      "step:178 loss:0.2700625956058502\n",
      "step:179 loss:0.33733490109443665\n",
      "step:180 loss:0.5157616138458252\n",
      "step:181 loss:0.5662822723388672\n",
      "step:182 loss:0.5605146884918213\n",
      "step:183 loss:0.5676230788230896\n",
      "step:184 loss:0.3742399513721466\n",
      "step:185 loss:0.49922534823417664\n",
      "step:186 loss:0.3957430124282837\n",
      "step:187 loss:0.5803207755088806\n",
      "step:188 loss:0.38724687695503235\n",
      "total_train_loss:0.6172496535359545, total_test_acc:0.7852408307556341\n",
      "------------------------------ \n",
      " epoch: 1\n",
      "step:0 loss:0.48150888085365295\n",
      "step:1 loss:0.4214167296886444\n",
      "step:2 loss:0.5972679257392883\n",
      "step:3 loss:0.3982318639755249\n",
      "step:4 loss:0.6154454350471497\n",
      "step:5 loss:0.28637778759002686\n",
      "step:6 loss:0.37371668219566345\n",
      "step:7 loss:0.5798749923706055\n",
      "step:8 loss:0.4383816719055176\n",
      "step:9 loss:0.7404253482818604\n",
      "step:10 loss:0.3856205940246582\n",
      "step:11 loss:0.45416465401649475\n",
      "step:12 loss:0.5082233548164368\n",
      "step:13 loss:0.27695006132125854\n",
      "step:14 loss:0.356991171836853\n",
      "step:15 loss:0.6030634641647339\n",
      "step:16 loss:0.43998849391937256\n",
      "step:17 loss:0.6187126040458679\n",
      "step:18 loss:0.4852491617202759\n",
      "step:19 loss:0.4393748939037323\n",
      "step:20 loss:0.38207241892814636\n",
      "step:21 loss:0.4333830177783966\n",
      "step:22 loss:0.4454847574234009\n",
      "step:23 loss:0.5649388432502747\n",
      "step:24 loss:0.5706350207328796\n",
      "step:25 loss:0.363168865442276\n",
      "step:26 loss:0.4223088026046753\n",
      "step:27 loss:0.42819496989250183\n",
      "step:28 loss:0.3567298650741577\n",
      "step:29 loss:0.4534302055835724\n",
      "step:30 loss:0.47592905163764954\n",
      "step:31 loss:0.5040411353111267\n",
      "step:32 loss:0.3768807649612427\n",
      "step:33 loss:0.3583489954471588\n",
      "step:34 loss:0.3922957181930542\n",
      "step:35 loss:0.6027817130088806\n",
      "step:36 loss:0.5559694170951843\n",
      "step:37 loss:0.5019914507865906\n",
      "step:38 loss:0.5290666222572327\n",
      "step:39 loss:0.6220590472221375\n",
      "step:40 loss:0.46932652592658997\n",
      "step:41 loss:0.514744222164154\n",
      "step:42 loss:0.37583911418914795\n",
      "step:43 loss:0.387382835149765\n",
      "step:44 loss:0.4762878119945526\n",
      "step:45 loss:0.39212194085121155\n",
      "step:46 loss:0.6417139172554016\n",
      "step:47 loss:0.5191346406936646\n",
      "step:48 loss:0.3205060660839081\n",
      "step:49 loss:0.4894585609436035\n",
      "step:50 loss:0.551397979259491\n",
      "step:51 loss:0.48581621050834656\n",
      "step:52 loss:0.46712565422058105\n",
      "step:53 loss:0.4318571090698242\n",
      "step:54 loss:0.3962491452693939\n",
      "step:55 loss:0.5080183148384094\n",
      "step:56 loss:0.534240186214447\n",
      "step:57 loss:0.37011513113975525\n",
      "step:58 loss:0.34659436345100403\n",
      "step:59 loss:0.4519273340702057\n",
      "step:60 loss:0.41788578033447266\n",
      "step:61 loss:0.6298726797103882\n",
      "step:62 loss:0.42513933777809143\n",
      "step:63 loss:0.5207497477531433\n",
      "step:64 loss:0.264330118894577\n",
      "step:65 loss:0.5656751990318298\n",
      "step:66 loss:0.6248824596405029\n",
      "step:67 loss:1.040678858757019\n",
      "step:68 loss:0.47658559679985046\n",
      "step:69 loss:0.5475185513496399\n",
      "step:70 loss:0.3170698583126068\n",
      "step:71 loss:0.4460875988006592\n",
      "step:72 loss:0.5182662010192871\n",
      "step:73 loss:0.4497361481189728\n",
      "step:74 loss:0.3406801223754883\n",
      "step:75 loss:0.2744070589542389\n",
      "step:76 loss:0.3189052641391754\n",
      "step:77 loss:0.25121602416038513\n",
      "step:78 loss:0.32787632942199707\n",
      "step:79 loss:0.36789950728416443\n",
      "step:80 loss:0.3761170208454132\n",
      "step:81 loss:0.4173973500728607\n",
      "step:82 loss:0.5093781352043152\n",
      "step:83 loss:0.3757331669330597\n",
      "step:84 loss:0.5984317064285278\n",
      "step:85 loss:0.3988856077194214\n",
      "step:86 loss:0.43467581272125244\n",
      "step:87 loss:0.32320377230644226\n",
      "step:88 loss:0.4866830110549927\n",
      "step:89 loss:0.5137754678726196\n",
      "step:90 loss:0.44585904479026794\n",
      "step:91 loss:0.5978655815124512\n",
      "step:92 loss:0.5098799467086792\n",
      "step:93 loss:0.30786627531051636\n",
      "step:94 loss:0.46936360001564026\n",
      "step:95 loss:0.629085123538971\n",
      "step:96 loss:0.32510140538215637\n",
      "step:97 loss:0.480653315782547\n",
      "step:98 loss:0.25233176350593567\n",
      "step:99 loss:0.7172175049781799\n",
      "step:100 loss:0.4254004657268524\n",
      "step:101 loss:0.5667871832847595\n",
      "step:102 loss:0.44185927510261536\n",
      "step:103 loss:0.3787064254283905\n",
      "step:104 loss:0.33695247769355774\n",
      "step:105 loss:0.48211777210235596\n",
      "step:106 loss:0.37857285141944885\n",
      "step:107 loss:0.37111708521842957\n",
      "step:108 loss:0.3002784550189972\n",
      "step:109 loss:0.2974607050418854\n",
      "step:110 loss:0.549310028553009\n",
      "step:111 loss:0.311509370803833\n",
      "step:112 loss:0.27636614441871643\n",
      "step:113 loss:0.34583595395088196\n",
      "step:114 loss:0.25726598501205444\n",
      "step:115 loss:0.37722042202949524\n",
      "step:116 loss:0.2799255847930908\n",
      "step:117 loss:0.320658415555954\n",
      "step:118 loss:0.1959913969039917\n",
      "step:119 loss:0.4624797999858856\n",
      "step:120 loss:0.5961159467697144\n",
      "step:121 loss:0.29662734270095825\n",
      "step:122 loss:0.3582041263580322\n",
      "step:123 loss:0.33855342864990234\n",
      "step:124 loss:0.2719908356666565\n",
      "step:125 loss:0.4660751521587372\n",
      "step:126 loss:0.3824242055416107\n",
      "step:127 loss:0.2706374228000641\n",
      "step:128 loss:0.4719589054584503\n",
      "step:129 loss:0.4469141960144043\n",
      "step:130 loss:0.5431063771247864\n",
      "step:131 loss:0.35161733627319336\n",
      "step:132 loss:0.33446574211120605\n",
      "step:133 loss:0.7217448353767395\n",
      "step:134 loss:0.6305238008499146\n",
      "step:135 loss:0.5309299826622009\n",
      "step:136 loss:0.4406464397907257\n",
      "step:137 loss:0.46370312571525574\n",
      "step:138 loss:0.43129685521125793\n",
      "step:139 loss:0.5808502435684204\n",
      "step:140 loss:0.6119549870491028\n",
      "step:141 loss:0.20624524354934692\n",
      "step:142 loss:0.6803388595581055\n",
      "step:143 loss:0.5451793670654297\n",
      "step:144 loss:0.3615816533565521\n",
      "step:145 loss:0.35324063897132874\n",
      "step:146 loss:0.3863585293292999\n",
      "step:147 loss:0.5737835764884949\n",
      "step:148 loss:0.39104488492012024\n",
      "step:149 loss:0.38339483737945557\n",
      "step:150 loss:0.35980868339538574\n",
      "step:151 loss:0.2868129014968872\n",
      "step:152 loss:0.3213135302066803\n",
      "step:153 loss:0.3759954869747162\n",
      "step:154 loss:0.4949398338794708\n",
      "step:155 loss:0.38806167244911194\n",
      "step:156 loss:0.48131707310676575\n",
      "step:157 loss:0.4376786947250366\n",
      "step:158 loss:0.2822640836238861\n",
      "step:159 loss:0.27290982007980347\n",
      "step:160 loss:0.4087722599506378\n",
      "step:161 loss:0.3571528494358063\n",
      "step:162 loss:0.419027715921402\n",
      "step:163 loss:0.6029557585716248\n",
      "step:164 loss:0.4811302721500397\n",
      "step:165 loss:0.3250296413898468\n",
      "step:166 loss:0.33742785453796387\n",
      "step:167 loss:0.2924865484237671\n",
      "step:168 loss:0.6127468943595886\n",
      "step:169 loss:0.739422082901001\n",
      "step:170 loss:0.36652424931526184\n",
      "step:171 loss:0.39080873131752014\n",
      "step:172 loss:0.31084343791007996\n",
      "step:173 loss:0.31044837832450867\n",
      "step:174 loss:0.5366794466972351\n",
      "step:175 loss:0.5527694225311279\n",
      "step:176 loss:0.30861106514930725\n",
      "step:177 loss:0.46474191546440125\n",
      "step:178 loss:0.31353840231895447\n",
      "step:179 loss:0.43879425525665283\n",
      "step:180 loss:0.487386018037796\n",
      "step:181 loss:0.42251476645469666\n",
      "step:182 loss:0.40431180596351624\n",
      "step:183 loss:0.37480637431144714\n",
      "step:184 loss:0.44591450691223145\n",
      "step:185 loss:0.3549489676952362\n",
      "step:186 loss:0.2876281142234802\n",
      "step:187 loss:0.40693172812461853\n",
      "step:188 loss:0.6074349880218506\n",
      "total_train_loss:0.4420517844722626, total_test_acc:0.6120194432169687\n",
      "------------------------------ \n",
      " epoch: 2\n",
      "step:0 loss:0.4560699462890625\n",
      "step:1 loss:0.47126686573028564\n",
      "step:2 loss:0.30699804425239563\n",
      "step:3 loss:0.4405054748058319\n",
      "step:4 loss:0.4527418613433838\n",
      "step:5 loss:0.38230469822883606\n",
      "step:6 loss:0.4354858100414276\n",
      "step:7 loss:0.37726590037345886\n",
      "step:8 loss:0.3679454028606415\n",
      "step:9 loss:0.32897740602493286\n",
      "step:10 loss:0.28486132621765137\n",
      "step:11 loss:0.5011457800865173\n",
      "step:12 loss:0.31156009435653687\n",
      "step:13 loss:0.4134575426578522\n",
      "step:14 loss:0.4046965539455414\n",
      "step:15 loss:0.5488811135292053\n",
      "step:16 loss:0.44442978501319885\n",
      "step:17 loss:0.36708298325538635\n",
      "step:18 loss:0.2802642285823822\n",
      "step:19 loss:0.2999320328235626\n",
      "step:20 loss:0.27464911341667175\n",
      "step:21 loss:0.46666160225868225\n",
      "step:22 loss:0.33996355533599854\n",
      "step:23 loss:0.29900574684143066\n",
      "step:24 loss:0.2188837081193924\n",
      "step:25 loss:0.46682801842689514\n",
      "step:26 loss:0.44185957312583923\n",
      "step:27 loss:0.28887927532196045\n",
      "step:28 loss:0.25878095626831055\n",
      "step:29 loss:0.34177717566490173\n",
      "step:30 loss:0.3266696035861969\n",
      "step:31 loss:0.5644354224205017\n",
      "step:32 loss:0.3680003881454468\n",
      "step:33 loss:0.2173062413930893\n",
      "step:34 loss:0.2966601848602295\n",
      "step:35 loss:0.3112776577472687\n",
      "step:36 loss:0.40255674719810486\n",
      "step:37 loss:0.4402298629283905\n",
      "step:38 loss:0.23646415770053864\n",
      "step:39 loss:0.30541887879371643\n",
      "step:40 loss:0.31671398878097534\n",
      "step:41 loss:0.5337206721305847\n",
      "step:42 loss:0.28650546073913574\n",
      "step:43 loss:0.33064180612564087\n",
      "step:44 loss:0.49514174461364746\n",
      "step:45 loss:0.5073229074478149\n",
      "step:46 loss:0.3989778757095337\n",
      "step:47 loss:0.38854190707206726\n",
      "step:48 loss:0.5485324263572693\n",
      "step:49 loss:0.3844184875488281\n",
      "step:50 loss:0.5095598697662354\n",
      "step:51 loss:0.45472705364227295\n",
      "step:52 loss:0.29620102047920227\n",
      "step:53 loss:0.27617278695106506\n",
      "step:54 loss:0.3582271635532379\n",
      "step:55 loss:0.48975852131843567\n",
      "step:56 loss:0.5029074549674988\n",
      "step:57 loss:0.3087471127510071\n",
      "step:58 loss:0.34285029768943787\n",
      "step:59 loss:0.35967743396759033\n",
      "step:60 loss:0.3408255875110626\n",
      "step:61 loss:0.2786504626274109\n",
      "step:62 loss:0.27474498748779297\n",
      "step:63 loss:0.3269791305065155\n",
      "step:64 loss:0.3104776442050934\n",
      "step:65 loss:0.4733326733112335\n",
      "step:66 loss:0.3444068133831024\n",
      "step:67 loss:0.3649040162563324\n",
      "step:68 loss:0.33823326230049133\n",
      "step:69 loss:0.2264225035905838\n",
      "step:70 loss:0.3436480760574341\n",
      "step:71 loss:0.28172168135643005\n",
      "step:72 loss:0.2555219233036041\n",
      "step:73 loss:0.33785244822502136\n",
      "step:74 loss:0.4724853038787842\n",
      "step:75 loss:0.2406994253396988\n",
      "step:76 loss:0.6528797745704651\n",
      "step:77 loss:0.2872779071331024\n",
      "step:78 loss:0.5523411631584167\n",
      "step:79 loss:0.38814935088157654\n",
      "step:80 loss:0.510590136051178\n",
      "step:81 loss:0.38620445132255554\n",
      "step:82 loss:0.22458024322986603\n",
      "step:83 loss:0.5564443469047546\n",
      "step:84 loss:0.5897043347358704\n",
      "step:85 loss:0.3224213421344757\n",
      "step:86 loss:0.3104160726070404\n",
      "step:87 loss:0.494064599275589\n",
      "step:88 loss:0.5061255097389221\n",
      "step:89 loss:0.3004612326622009\n",
      "step:90 loss:0.5304913520812988\n",
      "step:91 loss:0.4058232009410858\n",
      "step:92 loss:0.5217379927635193\n",
      "step:93 loss:0.6714877486228943\n",
      "step:94 loss:0.3844015598297119\n",
      "step:95 loss:0.5402345061302185\n",
      "step:96 loss:0.3899504840373993\n",
      "step:97 loss:0.22843991219997406\n",
      "step:98 loss:0.410770446062088\n",
      "step:99 loss:0.4529060125350952\n",
      "step:100 loss:0.43180403113365173\n",
      "step:101 loss:0.3777763843536377\n",
      "step:102 loss:0.32959845662117004\n",
      "step:103 loss:0.34877637028694153\n",
      "step:104 loss:0.5414501428604126\n",
      "step:105 loss:0.37793317437171936\n",
      "step:106 loss:0.3747241795063019\n",
      "step:107 loss:0.2519703507423401\n",
      "step:108 loss:0.2357373982667923\n",
      "step:109 loss:0.36119386553764343\n",
      "step:110 loss:0.3936821520328522\n",
      "step:111 loss:0.3012445867061615\n",
      "step:112 loss:0.614294707775116\n",
      "step:113 loss:0.14414432644844055\n",
      "step:114 loss:0.48310044407844543\n",
      "step:115 loss:0.22149600088596344\n",
      "step:116 loss:0.2371334582567215\n",
      "step:117 loss:0.5430127382278442\n",
      "step:118 loss:0.2388116717338562\n",
      "step:119 loss:0.25898876786231995\n",
      "step:120 loss:0.25026121735572815\n",
      "step:121 loss:0.39399006962776184\n",
      "step:122 loss:0.3087809383869171\n",
      "step:123 loss:0.2056717723608017\n",
      "step:124 loss:0.2755437195301056\n",
      "step:125 loss:0.2959073483943939\n",
      "step:126 loss:0.3177730143070221\n",
      "step:127 loss:0.3592047393321991\n",
      "step:128 loss:0.6235724091529846\n",
      "step:129 loss:0.4123678505420685\n",
      "step:130 loss:0.30686476826667786\n",
      "step:131 loss:0.4147759974002838\n",
      "step:132 loss:0.27071696519851685\n",
      "step:133 loss:0.465567946434021\n",
      "step:134 loss:0.20677419006824493\n",
      "step:135 loss:0.4247184097766876\n",
      "step:136 loss:0.18605829775333405\n",
      "step:137 loss:0.41506707668304443\n",
      "step:138 loss:0.37286677956581116\n",
      "step:139 loss:0.44442105293273926\n",
      "step:140 loss:0.2834089696407318\n",
      "step:141 loss:0.31168991327285767\n",
      "step:142 loss:0.2829207181930542\n",
      "step:143 loss:0.42316174507141113\n",
      "step:144 loss:0.4126444160938263\n",
      "step:145 loss:0.35562029480934143\n",
      "step:146 loss:0.4046888053417206\n",
      "step:147 loss:0.46321752667427063\n",
      "step:148 loss:0.54288250207901\n",
      "step:149 loss:0.39078521728515625\n",
      "step:150 loss:0.5344421863555908\n",
      "step:151 loss:0.2323784977197647\n",
      "step:152 loss:0.34182465076446533\n",
      "step:153 loss:0.2902221083641052\n",
      "step:154 loss:0.55270916223526\n",
      "step:155 loss:0.5680342316627502\n",
      "step:156 loss:0.4256959855556488\n",
      "step:157 loss:0.23505443334579468\n",
      "step:158 loss:0.24635565280914307\n",
      "step:159 loss:0.5281948447227478\n",
      "step:160 loss:0.33635079860687256\n",
      "step:161 loss:0.3184070289134979\n",
      "step:162 loss:0.3880338668823242\n",
      "step:163 loss:0.4817483127117157\n",
      "step:164 loss:0.37459611892700195\n",
      "step:165 loss:0.23509038984775543\n",
      "step:166 loss:0.33267614245414734\n",
      "step:167 loss:0.3950682580471039\n",
      "step:168 loss:0.28801336884498596\n",
      "step:169 loss:0.32837918400764465\n",
      "step:170 loss:0.5140765309333801\n",
      "step:171 loss:0.2787128984928131\n",
      "step:172 loss:0.39886531233787537\n",
      "step:173 loss:0.24095739424228668\n",
      "step:174 loss:0.32621505856513977\n",
      "step:175 loss:0.32708045840263367\n",
      "step:176 loss:0.5769273638725281\n",
      "step:177 loss:0.22703701257705688\n",
      "step:178 loss:0.5209305882453918\n",
      "step:179 loss:0.42436453700065613\n",
      "step:180 loss:0.4003318250179291\n",
      "step:181 loss:0.2521117627620697\n",
      "step:182 loss:0.3922346830368042\n",
      "step:183 loss:0.38465869426727295\n",
      "step:184 loss:0.37766146659851074\n",
      "step:185 loss:0.4596468508243561\n",
      "step:186 loss:0.4434485137462616\n",
      "step:187 loss:0.28715673089027405\n",
      "step:188 loss:0.3477388322353363\n",
      "total_train_loss:0.3785242869498882, total_test_acc:0.5435262925320371\n",
      "------------------------------ \n",
      " epoch: 3\n",
      "step:0 loss:0.37434089183807373\n",
      "step:1 loss:0.5235853791236877\n",
      "step:2 loss:0.30036115646362305\n",
      "step:3 loss:0.4404946565628052\n",
      "step:4 loss:0.3623832166194916\n",
      "step:5 loss:0.3528934419155121\n",
      "step:6 loss:0.38452622294425964\n",
      "step:7 loss:0.5014523863792419\n",
      "step:8 loss:0.2357999086380005\n",
      "step:9 loss:0.35021424293518066\n",
      "step:10 loss:0.511644184589386\n",
      "step:11 loss:0.32204827666282654\n",
      "step:12 loss:0.35528913140296936\n",
      "step:13 loss:0.2595360279083252\n",
      "step:14 loss:0.4021109640598297\n",
      "step:15 loss:0.24464498460292816\n",
      "step:16 loss:0.3374573886394501\n",
      "step:17 loss:0.30386918783187866\n",
      "step:18 loss:0.5340341925621033\n",
      "step:19 loss:0.4554709494113922\n",
      "step:20 loss:0.35793253779411316\n",
      "step:21 loss:0.2814483344554901\n",
      "step:22 loss:0.26587846875190735\n",
      "step:23 loss:0.5582587122917175\n",
      "step:24 loss:0.7963075637817383\n",
      "step:25 loss:0.2936165928840637\n",
      "step:26 loss:0.19314254820346832\n",
      "step:27 loss:0.22481600940227509\n",
      "step:28 loss:0.3222658336162567\n",
      "step:29 loss:0.3523344099521637\n",
      "step:30 loss:0.4188331067562103\n",
      "step:31 loss:0.3791629374027252\n",
      "step:32 loss:0.584303081035614\n",
      "step:33 loss:0.3517323434352875\n",
      "step:34 loss:0.2950846254825592\n",
      "step:35 loss:0.2798879146575928\n",
      "step:36 loss:0.362935870885849\n",
      "step:37 loss:0.3243107497692108\n",
      "step:38 loss:0.3400352895259857\n",
      "step:39 loss:0.6092234253883362\n",
      "step:40 loss:0.506345808506012\n",
      "step:41 loss:0.3611952066421509\n",
      "step:42 loss:0.3416711986064911\n",
      "step:43 loss:0.41148534417152405\n",
      "step:44 loss:0.3676287829875946\n",
      "step:45 loss:0.33773741126060486\n",
      "step:46 loss:0.45484659075737\n",
      "step:47 loss:0.4668360650539398\n",
      "step:48 loss:0.4030574560165405\n",
      "step:49 loss:0.5877304673194885\n",
      "step:50 loss:0.21700668334960938\n",
      "step:51 loss:0.44401106238365173\n",
      "step:52 loss:0.2641325891017914\n",
      "step:53 loss:0.3498804569244385\n",
      "step:54 loss:0.2384212762117386\n",
      "step:55 loss:0.3763245642185211\n",
      "step:56 loss:0.3305048942565918\n",
      "step:57 loss:0.4678480923175812\n",
      "step:58 loss:0.32544460892677307\n",
      "step:59 loss:0.32768532633781433\n",
      "step:60 loss:0.4288252592086792\n",
      "step:61 loss:0.30553457140922546\n",
      "step:62 loss:0.38838812708854675\n",
      "step:63 loss:0.42527008056640625\n",
      "step:64 loss:0.3627467453479767\n",
      "step:65 loss:0.27337130904197693\n",
      "step:66 loss:0.22124910354614258\n",
      "step:67 loss:0.33460602164268494\n",
      "step:68 loss:0.26478034257888794\n",
      "step:69 loss:0.22324150800704956\n",
      "step:70 loss:0.4270336627960205\n",
      "step:71 loss:0.47081491351127625\n",
      "step:72 loss:0.29605749249458313\n",
      "step:73 loss:0.4335477650165558\n",
      "step:74 loss:0.3603406250476837\n",
      "step:75 loss:0.3721848726272583\n",
      "step:76 loss:0.3729616701602936\n",
      "step:77 loss:0.5191929340362549\n",
      "step:78 loss:0.3478565514087677\n",
      "step:79 loss:0.24344129860401154\n",
      "step:80 loss:0.4936412572860718\n",
      "step:81 loss:0.404386967420578\n",
      "step:82 loss:0.29619643092155457\n",
      "step:83 loss:0.3449397385120392\n",
      "step:84 loss:0.2303691953420639\n",
      "step:85 loss:0.3372868597507477\n",
      "step:86 loss:0.22623036801815033\n",
      "step:87 loss:0.5182422399520874\n",
      "step:88 loss:0.36955010890960693\n",
      "step:89 loss:0.31402280926704407\n",
      "step:90 loss:0.2617158889770508\n",
      "step:91 loss:0.2851545214653015\n",
      "step:92 loss:0.29909011721611023\n",
      "step:93 loss:0.3674471378326416\n",
      "step:94 loss:0.32798776030540466\n",
      "step:95 loss:0.49021950364112854\n",
      "step:96 loss:0.4846993386745453\n",
      "step:97 loss:0.2174520641565323\n",
      "step:98 loss:0.3468170464038849\n",
      "step:99 loss:0.5864287614822388\n",
      "step:100 loss:0.23801827430725098\n",
      "step:101 loss:0.38843199610710144\n",
      "step:102 loss:0.23635636270046234\n",
      "step:103 loss:0.4872649610042572\n",
      "step:104 loss:0.24237196147441864\n",
      "step:105 loss:0.26306602358818054\n",
      "step:106 loss:0.3854670524597168\n",
      "step:107 loss:0.40642014145851135\n",
      "step:108 loss:0.24240921437740326\n",
      "step:109 loss:0.2087370753288269\n",
      "step:110 loss:0.28777143359184265\n",
      "step:111 loss:0.3303793668746948\n",
      "step:112 loss:0.3020831048488617\n",
      "step:113 loss:0.3867844343185425\n",
      "step:114 loss:0.26484885811805725\n",
      "step:115 loss:0.3157767951488495\n",
      "step:116 loss:0.23327507078647614\n",
      "step:117 loss:0.2906579077243805\n",
      "step:118 loss:0.3409470319747925\n",
      "step:119 loss:0.42567387223243713\n",
      "step:120 loss:0.4594043791294098\n",
      "step:121 loss:0.23773038387298584\n",
      "step:122 loss:0.21959753334522247\n",
      "step:123 loss:0.31852999329566956\n",
      "step:124 loss:0.30896058678627014\n",
      "step:125 loss:0.1877869814634323\n",
      "step:126 loss:0.346311092376709\n",
      "step:127 loss:0.24102912843227386\n",
      "step:128 loss:0.3902825117111206\n",
      "step:129 loss:0.18556608259677887\n",
      "step:130 loss:0.5466057658195496\n",
      "step:131 loss:0.2371542602777481\n",
      "step:132 loss:0.33189207315444946\n",
      "step:133 loss:0.24050326645374298\n",
      "step:134 loss:0.3298099637031555\n",
      "step:135 loss:0.2586548924446106\n",
      "step:136 loss:0.567843496799469\n",
      "step:137 loss:0.2807845175266266\n",
      "step:138 loss:0.18521340191364288\n",
      "step:139 loss:0.25494304299354553\n",
      "step:140 loss:0.45967718958854675\n",
      "step:141 loss:0.24266000092029572\n",
      "step:142 loss:0.20420204102993011\n",
      "step:143 loss:0.28231796622276306\n",
      "step:144 loss:0.35190653800964355\n",
      "step:145 loss:0.22101402282714844\n",
      "step:146 loss:0.28245946764945984\n",
      "step:147 loss:0.3267746567726135\n",
      "step:148 loss:0.47000864148139954\n",
      "step:149 loss:0.311190664768219\n",
      "step:150 loss:0.308761328458786\n",
      "step:151 loss:0.43059229850769043\n",
      "step:152 loss:0.32592275738716125\n",
      "step:153 loss:0.37935248017311096\n",
      "step:154 loss:0.3043880760669708\n",
      "step:155 loss:0.3576813042163849\n",
      "step:156 loss:0.32152190804481506\n",
      "step:157 loss:0.21689659357070923\n",
      "step:158 loss:0.36925008893013\n",
      "step:159 loss:0.3469678461551666\n",
      "step:160 loss:0.4354289770126343\n",
      "step:161 loss:0.18108345568180084\n",
      "step:162 loss:0.4370534420013428\n",
      "step:163 loss:0.2998281419277191\n",
      "step:164 loss:0.373850017786026\n",
      "step:165 loss:0.2853906452655792\n",
      "step:166 loss:0.18053491413593292\n",
      "step:167 loss:0.3161676824092865\n",
      "step:168 loss:0.4116346836090088\n",
      "step:169 loss:0.3007955253124237\n",
      "step:170 loss:0.36427685618400574\n",
      "step:171 loss:0.27440258860588074\n",
      "step:172 loss:0.33451786637306213\n",
      "step:173 loss:0.37739887833595276\n",
      "step:174 loss:0.27116087079048157\n",
      "step:175 loss:0.3608340322971344\n",
      "step:176 loss:0.4241143763065338\n",
      "step:177 loss:0.37251555919647217\n",
      "step:178 loss:0.3218543827533722\n",
      "step:179 loss:0.34688258171081543\n",
      "step:180 loss:0.37878918647766113\n",
      "step:181 loss:0.38013043999671936\n",
      "step:182 loss:0.42441174387931824\n",
      "step:183 loss:0.1843634396791458\n",
      "step:184 loss:0.21976988017559052\n",
      "step:185 loss:0.2739354968070984\n",
      "step:186 loss:0.3172484338283539\n",
      "step:187 loss:0.41272374987602234\n",
      "step:188 loss:0.1726740151643753\n",
      "total_train_loss:0.3476034363216542, total_test_acc:0.8351745470614229\n",
      "------------------------------ \n",
      " epoch: 4\n",
      "step:0 loss:0.37289413809776306\n",
      "step:1 loss:0.6150258183479309\n",
      "step:2 loss:0.42781734466552734\n",
      "step:3 loss:0.3094855546951294\n",
      "step:4 loss:0.35963913798332214\n",
      "step:5 loss:0.4766724407672882\n",
      "step:6 loss:0.37525737285614014\n",
      "step:7 loss:0.3168112635612488\n",
      "step:8 loss:0.24358032643795013\n",
      "step:9 loss:0.3361034393310547\n",
      "step:10 loss:0.551659882068634\n",
      "step:11 loss:0.33275091648101807\n",
      "step:12 loss:0.42547371983528137\n",
      "step:13 loss:0.3236555755138397\n",
      "step:14 loss:0.499182790517807\n",
      "step:15 loss:0.2911040484905243\n",
      "step:16 loss:0.4990781247615814\n",
      "step:17 loss:0.254972368478775\n",
      "step:18 loss:0.3023125231266022\n",
      "step:19 loss:0.31752079725265503\n",
      "step:20 loss:0.2798644006252289\n",
      "step:21 loss:0.44825664162635803\n",
      "step:22 loss:0.4092973470687866\n",
      "step:23 loss:0.6776907444000244\n",
      "step:24 loss:0.30158671736717224\n",
      "step:25 loss:0.4786359965801239\n",
      "step:26 loss:0.38044485449790955\n",
      "step:27 loss:0.30967283248901367\n",
      "step:28 loss:0.20624373853206635\n",
      "step:29 loss:0.2863847017288208\n",
      "step:30 loss:0.16345340013504028\n",
      "step:31 loss:0.3356759548187256\n",
      "step:32 loss:0.33503246307373047\n",
      "step:33 loss:0.2590819299221039\n",
      "step:34 loss:0.3063751757144928\n",
      "step:35 loss:0.32670271396636963\n",
      "step:36 loss:0.3448528051376343\n",
      "step:37 loss:0.21767686307430267\n",
      "step:38 loss:0.2443658858537674\n",
      "step:39 loss:0.16198843717575073\n",
      "step:40 loss:0.3377816677093506\n",
      "step:41 loss:0.41205501556396484\n",
      "step:42 loss:0.3962676525115967\n",
      "step:43 loss:0.43701839447021484\n",
      "step:44 loss:0.32539981603622437\n",
      "step:45 loss:0.27254602313041687\n",
      "step:46 loss:0.3651371896266937\n",
      "step:47 loss:0.2464050054550171\n",
      "step:48 loss:0.3687765598297119\n",
      "step:49 loss:0.33189254999160767\n",
      "step:50 loss:0.28181013464927673\n",
      "step:51 loss:0.15246884524822235\n",
      "step:52 loss:0.425872802734375\n",
      "step:53 loss:0.4397369623184204\n",
      "step:54 loss:0.4523024559020996\n",
      "step:55 loss:0.2785510718822479\n",
      "step:56 loss:0.3470715582370758\n",
      "step:57 loss:0.3633560240268707\n",
      "step:58 loss:0.20199094712734222\n",
      "step:59 loss:0.2579560875892639\n",
      "step:60 loss:0.3268837332725525\n",
      "step:61 loss:0.3307209610939026\n",
      "step:62 loss:0.3167101740837097\n",
      "step:63 loss:0.4034707546234131\n",
      "step:64 loss:0.24400721490383148\n",
      "step:65 loss:0.3113155663013458\n",
      "step:66 loss:0.33636894822120667\n",
      "step:67 loss:0.36199823021888733\n",
      "step:68 loss:0.3238794803619385\n",
      "step:69 loss:0.3126017153263092\n",
      "step:70 loss:0.2961522042751312\n",
      "step:71 loss:0.18301403522491455\n",
      "step:72 loss:0.35042130947113037\n",
      "step:73 loss:0.37320971488952637\n",
      "step:74 loss:0.2164655476808548\n",
      "step:75 loss:0.513427734375\n",
      "step:76 loss:0.4373628795146942\n",
      "step:77 loss:0.32581594586372375\n",
      "step:78 loss:0.22255223989486694\n",
      "step:79 loss:0.40433958172798157\n",
      "step:80 loss:0.41152188181877136\n",
      "step:81 loss:0.30424389243125916\n",
      "step:82 loss:0.22905857861042023\n",
      "step:83 loss:0.30530649423599243\n",
      "step:84 loss:0.4027757942676544\n",
      "step:85 loss:0.47910526394844055\n",
      "step:86 loss:0.4036175012588501\n",
      "step:87 loss:0.247459277510643\n",
      "step:88 loss:0.4094984531402588\n",
      "step:89 loss:0.41945862770080566\n",
      "step:90 loss:0.38955965638160706\n",
      "step:91 loss:0.16917909681797028\n",
      "step:92 loss:0.14591702818870544\n",
      "step:93 loss:0.2516879737377167\n",
      "step:94 loss:0.37286198139190674\n",
      "step:95 loss:0.45534756779670715\n",
      "step:96 loss:0.351694792509079\n",
      "step:97 loss:0.40578708052635193\n",
      "step:98 loss:0.36193713545799255\n",
      "step:99 loss:0.29408949613571167\n",
      "step:100 loss:0.27074122428894043\n",
      "step:101 loss:0.22221867740154266\n",
      "step:102 loss:0.14573708176612854\n",
      "step:103 loss:0.35813799500465393\n",
      "step:104 loss:0.31492412090301514\n",
      "step:105 loss:0.1749352663755417\n",
      "step:106 loss:0.15161031484603882\n",
      "step:107 loss:0.3174501359462738\n",
      "step:108 loss:0.446932315826416\n",
      "step:109 loss:0.38649842143058777\n",
      "step:110 loss:0.19592992961406708\n",
      "step:111 loss:0.18869279325008392\n",
      "step:112 loss:0.1941152662038803\n",
      "step:113 loss:0.49551618099212646\n",
      "step:114 loss:0.4652588665485382\n",
      "step:115 loss:0.25173288583755493\n",
      "step:116 loss:0.5368513464927673\n",
      "step:117 loss:0.3812159597873688\n",
      "step:118 loss:0.26230672001838684\n",
      "step:119 loss:0.20771987736225128\n",
      "step:120 loss:0.3860710561275482\n",
      "step:121 loss:0.1423230916261673\n",
      "step:122 loss:0.3495609760284424\n",
      "step:123 loss:0.2775448262691498\n",
      "step:124 loss:0.22366978228092194\n",
      "step:125 loss:0.18086093664169312\n",
      "step:126 loss:0.38002482056617737\n",
      "step:127 loss:0.23435115814208984\n",
      "step:128 loss:0.337996244430542\n",
      "step:129 loss:0.28392502665519714\n",
      "step:130 loss:0.3354209363460541\n",
      "step:131 loss:0.37854206562042236\n",
      "step:132 loss:0.2888519763946533\n",
      "step:133 loss:0.2634800374507904\n",
      "step:134 loss:0.2963869571685791\n",
      "step:135 loss:0.27450892329216003\n",
      "step:136 loss:0.2936646044254303\n",
      "step:137 loss:0.38408029079437256\n",
      "step:138 loss:0.30693140625953674\n",
      "step:139 loss:0.14551503956317902\n",
      "step:140 loss:0.2545105516910553\n",
      "step:141 loss:0.47284236550331116\n",
      "step:142 loss:0.26105087995529175\n",
      "step:143 loss:0.25677379965782166\n",
      "step:144 loss:0.2667004466056824\n",
      "step:145 loss:0.28091952204704285\n",
      "step:146 loss:0.45392295718193054\n",
      "step:147 loss:0.2529207170009613\n",
      "step:148 loss:0.3119288980960846\n",
      "step:149 loss:0.4409639835357666\n",
      "step:150 loss:0.3852028548717499\n",
      "step:151 loss:0.20717363059520721\n",
      "step:152 loss:0.4023571014404297\n",
      "step:153 loss:0.2242804318666458\n",
      "step:154 loss:0.33872368931770325\n",
      "step:155 loss:0.303011029958725\n",
      "step:156 loss:0.3857964277267456\n",
      "step:157 loss:0.17609624564647675\n",
      "step:158 loss:0.4341493844985962\n",
      "step:159 loss:0.17639030516147614\n",
      "step:160 loss:0.24942363798618317\n",
      "step:161 loss:0.12760812044143677\n",
      "step:162 loss:0.24144364893436432\n",
      "step:163 loss:0.2123008817434311\n",
      "step:164 loss:0.3916700780391693\n",
      "step:165 loss:0.1930568963289261\n",
      "step:166 loss:0.4355282783508301\n",
      "step:167 loss:0.35775312781333923\n",
      "step:168 loss:0.43777307868003845\n",
      "step:169 loss:0.30689796805381775\n",
      "step:170 loss:0.29656168818473816\n",
      "step:171 loss:0.33054444193840027\n",
      "step:172 loss:0.4003450572490692\n",
      "step:173 loss:0.2852295935153961\n",
      "step:174 loss:0.14420749247074127\n",
      "step:175 loss:0.2794826328754425\n",
      "step:176 loss:0.19103515148162842\n",
      "step:177 loss:0.3004102408885956\n",
      "step:178 loss:0.28527626395225525\n",
      "step:179 loss:0.3975130617618561\n",
      "step:180 loss:0.22818724811077118\n",
      "step:181 loss:0.2169824242591858\n",
      "step:182 loss:0.15508103370666504\n",
      "step:183 loss:0.2998269200325012\n",
      "step:184 loss:0.319754958152771\n",
      "step:185 loss:0.26223158836364746\n",
      "step:186 loss:0.21208781003952026\n",
      "step:187 loss:0.4137057363986969\n",
      "step:188 loss:0.16192133724689484\n",
      "total_train_loss:0.3210175247426997, total_test_acc:0.7414935925762263\n",
      "------------------------------ \n",
      " epoch: 5\n",
      "step:0 loss:0.33884063363075256\n",
      "step:1 loss:0.26242712140083313\n",
      "step:2 loss:0.5385810136795044\n",
      "step:3 loss:0.21909628808498383\n",
      "step:4 loss:0.423580527305603\n",
      "step:5 loss:0.292154997587204\n",
      "step:6 loss:0.3110659718513489\n",
      "step:7 loss:0.36541709303855896\n",
      "step:8 loss:0.2153446227312088\n",
      "step:9 loss:0.5136727690696716\n",
      "step:10 loss:0.3549560010433197\n",
      "step:11 loss:0.38616979122161865\n",
      "step:12 loss:0.18859536945819855\n",
      "step:13 loss:0.32075902819633484\n",
      "step:14 loss:0.3024842441082001\n",
      "step:15 loss:0.28387466073036194\n",
      "step:16 loss:0.26817816495895386\n",
      "step:17 loss:0.24102288484573364\n",
      "step:18 loss:0.3037196695804596\n",
      "step:19 loss:0.36478838324546814\n",
      "step:20 loss:0.2977933883666992\n",
      "step:21 loss:0.3399888277053833\n",
      "step:22 loss:0.2584826648235321\n",
      "step:23 loss:0.338944673538208\n",
      "step:24 loss:0.25875124335289\n",
      "step:25 loss:0.2899439334869385\n",
      "step:26 loss:0.2880955636501312\n",
      "step:27 loss:0.25935542583465576\n",
      "step:28 loss:0.2641565203666687\n",
      "step:29 loss:0.2144605666399002\n",
      "step:30 loss:0.38621237874031067\n",
      "step:31 loss:0.19356320798397064\n",
      "step:32 loss:0.26460742950439453\n",
      "step:33 loss:0.2521078288555145\n",
      "step:34 loss:0.2677765190601349\n",
      "step:35 loss:0.28341910243034363\n",
      "step:36 loss:0.19195443391799927\n",
      "step:37 loss:0.269435852766037\n",
      "step:38 loss:0.31402334570884705\n",
      "step:39 loss:0.31318554282188416\n",
      "step:40 loss:0.25165149569511414\n",
      "step:41 loss:0.2273554652929306\n",
      "step:42 loss:0.45005735754966736\n",
      "step:43 loss:0.3081901967525482\n",
      "step:44 loss:0.38275668025016785\n",
      "step:45 loss:0.13675445318222046\n",
      "step:46 loss:0.22691142559051514\n",
      "step:47 loss:0.3228815197944641\n",
      "step:48 loss:0.446273535490036\n",
      "step:49 loss:0.40402162075042725\n",
      "step:50 loss:0.25639235973358154\n",
      "step:51 loss:0.142891064286232\n",
      "step:52 loss:0.46660473942756653\n",
      "step:53 loss:0.3793664276599884\n",
      "step:54 loss:0.27481311559677124\n",
      "step:55 loss:0.27674248814582825\n",
      "step:56 loss:0.3416038453578949\n",
      "step:57 loss:0.36445751786231995\n",
      "step:58 loss:0.4183765947818756\n",
      "step:59 loss:0.21123461425304413\n",
      "step:60 loss:0.29866907000541687\n",
      "step:61 loss:0.26144441962242126\n",
      "step:62 loss:0.2677823305130005\n",
      "step:63 loss:0.29666265845298767\n",
      "step:64 loss:0.31230834126472473\n",
      "step:65 loss:0.2845674455165863\n",
      "step:66 loss:0.39679011702537537\n",
      "step:67 loss:0.4365876615047455\n",
      "step:68 loss:0.43025267124176025\n",
      "step:69 loss:0.2935325503349304\n",
      "step:70 loss:0.23817385733127594\n",
      "step:71 loss:0.19192667305469513\n",
      "step:72 loss:0.2480260729789734\n",
      "step:73 loss:0.3764443099498749\n",
      "step:74 loss:0.4441428482532501\n",
      "step:75 loss:0.2826036512851715\n",
      "step:76 loss:0.34288349747657776\n",
      "step:77 loss:0.2630583345890045\n",
      "step:78 loss:0.22879916429519653\n",
      "step:79 loss:0.31448161602020264\n",
      "step:80 loss:0.3155910074710846\n",
      "step:81 loss:0.3071608245372772\n",
      "step:82 loss:0.17950744926929474\n",
      "step:83 loss:0.36912181973457336\n",
      "step:84 loss:0.40510067343711853\n",
      "step:85 loss:0.27933332324028015\n",
      "step:86 loss:0.18282759189605713\n",
      "step:87 loss:0.33557966351509094\n",
      "step:88 loss:0.3354797065258026\n",
      "step:89 loss:0.3363807201385498\n",
      "step:90 loss:0.311280220746994\n",
      "step:91 loss:0.4531804323196411\n",
      "step:92 loss:0.16081203520298004\n",
      "step:93 loss:0.18612785637378693\n",
      "step:94 loss:0.28571179509162903\n",
      "step:95 loss:0.3924446105957031\n",
      "step:96 loss:0.3347276747226715\n",
      "step:97 loss:0.31173115968704224\n",
      "step:98 loss:0.45725342631340027\n",
      "step:99 loss:0.3328098952770233\n",
      "step:100 loss:0.2978757321834564\n",
      "step:101 loss:0.35171258449554443\n",
      "step:102 loss:0.20934969186782837\n",
      "step:103 loss:0.3583212196826935\n",
      "step:104 loss:0.23059935867786407\n",
      "step:105 loss:0.2739204466342926\n",
      "step:106 loss:0.2675347924232483\n",
      "step:107 loss:0.3503305912017822\n",
      "step:108 loss:0.25239673256874084\n",
      "step:109 loss:0.23262085020542145\n",
      "step:110 loss:0.32641512155532837\n",
      "step:111 loss:0.3703349530696869\n",
      "step:112 loss:0.12696383893489838\n",
      "step:113 loss:0.11968213319778442\n",
      "step:114 loss:0.5194506049156189\n",
      "step:115 loss:0.29952624440193176\n",
      "step:116 loss:0.3729605972766876\n",
      "step:117 loss:0.2733779847621918\n",
      "step:118 loss:0.5396673083305359\n",
      "step:119 loss:0.34282025694847107\n",
      "step:120 loss:0.529900074005127\n",
      "step:121 loss:0.3999077379703522\n",
      "step:122 loss:0.14922593533992767\n",
      "step:123 loss:0.21420533955097198\n",
      "step:124 loss:0.41044914722442627\n",
      "step:125 loss:0.28135743737220764\n",
      "step:126 loss:0.31386396288871765\n",
      "step:127 loss:0.41694486141204834\n",
      "step:128 loss:0.3330838978290558\n",
      "step:129 loss:0.22895951569080353\n",
      "step:130 loss:0.20098024606704712\n",
      "step:131 loss:0.47575974464416504\n",
      "step:132 loss:0.4367266893386841\n",
      "step:133 loss:0.22777503728866577\n",
      "step:134 loss:0.3781925439834595\n",
      "step:135 loss:0.3210710287094116\n",
      "step:136 loss:0.38375768065452576\n",
      "step:137 loss:0.27102240920066833\n",
      "step:138 loss:0.32690587639808655\n",
      "step:139 loss:0.41142117977142334\n",
      "step:140 loss:0.29941827058792114\n",
      "step:141 loss:0.35168561339378357\n",
      "step:142 loss:0.265066534280777\n",
      "step:143 loss:0.3478921949863434\n",
      "step:144 loss:0.1894049048423767\n",
      "step:145 loss:0.3061438500881195\n",
      "step:146 loss:0.3574199378490448\n",
      "step:147 loss:0.2229909896850586\n",
      "step:148 loss:0.4256175756454468\n",
      "step:149 loss:0.26602867245674133\n",
      "step:150 loss:0.23432272672653198\n",
      "step:151 loss:0.34383976459503174\n",
      "step:152 loss:0.17619790136814117\n",
      "step:153 loss:0.3154146671295166\n",
      "step:154 loss:0.20560652017593384\n",
      "step:155 loss:0.30780500173568726\n",
      "step:156 loss:0.3648214340209961\n",
      "step:157 loss:0.21973080933094025\n",
      "step:158 loss:0.3216089606285095\n",
      "step:159 loss:0.25363820791244507\n",
      "step:160 loss:0.21418505907058716\n",
      "step:161 loss:0.2135780304670334\n",
      "step:162 loss:0.4518231153488159\n",
      "step:163 loss:0.24798254668712616\n",
      "step:164 loss:0.38439497351646423\n",
      "step:165 loss:0.40601155161857605\n",
      "step:166 loss:0.3459981679916382\n",
      "step:167 loss:0.3070976138114929\n",
      "step:168 loss:0.2977207601070404\n",
      "step:169 loss:0.28837665915489197\n",
      "step:170 loss:0.22842048108577728\n",
      "step:171 loss:0.2926661968231201\n",
      "step:172 loss:0.3201182186603546\n",
      "step:173 loss:0.21028804779052734\n",
      "step:174 loss:0.2432723492383957\n",
      "step:175 loss:0.28532931208610535\n",
      "step:176 loss:0.4559624493122101\n",
      "step:177 loss:0.27010324597358704\n",
      "step:178 loss:0.19858284294605255\n",
      "step:179 loss:0.35777488350868225\n",
      "step:180 loss:0.2824581265449524\n",
      "step:181 loss:0.48103973269462585\n",
      "step:182 loss:0.3706405460834503\n",
      "step:183 loss:0.398429274559021\n",
      "step:184 loss:0.2663562595844269\n",
      "step:185 loss:0.2610050141811371\n",
      "step:186 loss:0.39287546277046204\n",
      "step:187 loss:0.2609594762325287\n",
      "step:188 loss:0.2709623873233795\n",
      "total_train_loss:0.3107343728237964, total_test_acc:0.32832523199292973\n",
      "------------------------------ \n",
      " epoch: 6\n",
      "step:0 loss:0.2512587308883667\n",
      "step:1 loss:0.16169172525405884\n",
      "step:2 loss:0.1416207104921341\n",
      "step:3 loss:0.3820948898792267\n",
      "step:4 loss:0.36927035450935364\n",
      "step:5 loss:0.3403594195842743\n",
      "step:6 loss:0.2054072469472885\n",
      "step:7 loss:0.2982628345489502\n",
      "step:8 loss:0.2990168333053589\n",
      "step:9 loss:0.2689036428928375\n",
      "step:10 loss:0.26048174500465393\n",
      "step:11 loss:0.29359811544418335\n",
      "step:12 loss:0.12457598000764847\n",
      "step:13 loss:0.22953402996063232\n",
      "step:14 loss:0.3494549095630646\n",
      "step:15 loss:0.37495777010917664\n",
      "step:16 loss:0.22528953850269318\n",
      "step:17 loss:0.22526568174362183\n",
      "step:18 loss:0.5702471137046814\n",
      "step:19 loss:0.4308869540691376\n",
      "step:20 loss:0.22352690994739532\n",
      "step:21 loss:0.2196575552225113\n",
      "step:22 loss:0.29012802243232727\n",
      "step:23 loss:0.14827819168567657\n",
      "step:24 loss:0.15493546426296234\n",
      "step:25 loss:0.25458160042762756\n",
      "step:26 loss:0.27309736609458923\n",
      "step:27 loss:0.36260929703712463\n",
      "step:28 loss:0.31254905462265015\n",
      "step:29 loss:0.22321422398090363\n",
      "step:30 loss:0.3378477990627289\n",
      "step:31 loss:0.2971660792827606\n",
      "step:32 loss:0.32085683941841125\n",
      "step:33 loss:0.3037802278995514\n",
      "step:34 loss:0.5074383020401001\n",
      "step:35 loss:0.28632691502571106\n",
      "step:36 loss:0.22038429975509644\n",
      "step:37 loss:0.19618196785449982\n",
      "step:38 loss:0.4671384394168854\n",
      "step:39 loss:0.33187076449394226\n",
      "step:40 loss:0.2756177484989166\n",
      "step:41 loss:0.2790595293045044\n",
      "step:42 loss:0.359084814786911\n",
      "step:43 loss:0.22691814601421356\n",
      "step:44 loss:0.24910135567188263\n",
      "step:45 loss:0.28150275349617004\n",
      "step:46 loss:0.27815431356430054\n",
      "step:47 loss:0.4312870502471924\n",
      "step:48 loss:0.3842151463031769\n",
      "step:49 loss:0.25376060605049133\n",
      "step:50 loss:0.2604282796382904\n",
      "step:51 loss:0.39747825264930725\n",
      "step:52 loss:0.22822876274585724\n",
      "step:53 loss:0.2211841493844986\n",
      "step:54 loss:0.4069004952907562\n",
      "step:55 loss:0.1860881894826889\n",
      "step:56 loss:0.41967177391052246\n",
      "step:57 loss:0.3071156442165375\n",
      "step:58 loss:0.1773311346769333\n",
      "step:59 loss:0.30097782611846924\n",
      "step:60 loss:0.38304564356803894\n",
      "step:61 loss:0.3260873258113861\n",
      "step:62 loss:0.29517364501953125\n",
      "step:63 loss:0.16247263550758362\n",
      "step:64 loss:0.3259095847606659\n",
      "step:65 loss:0.31029435992240906\n",
      "step:66 loss:0.30755898356437683\n",
      "step:67 loss:0.2988317012786865\n",
      "step:68 loss:0.1282004415988922\n",
      "step:69 loss:0.3914245367050171\n",
      "step:70 loss:0.2645686864852905\n",
      "step:71 loss:0.14283524453639984\n",
      "step:72 loss:0.28799524903297424\n",
      "step:73 loss:0.42056500911712646\n",
      "step:74 loss:0.35767778754234314\n",
      "step:75 loss:0.24677367508411407\n",
      "step:76 loss:0.1530916839838028\n",
      "step:77 loss:0.24698413908481598\n",
      "step:78 loss:0.31350424885749817\n",
      "step:79 loss:0.1955004334449768\n",
      "step:80 loss:0.32687288522720337\n",
      "step:81 loss:0.4744006097316742\n",
      "step:82 loss:0.33012527227401733\n",
      "step:83 loss:0.2753271758556366\n",
      "step:84 loss:0.49853432178497314\n",
      "step:85 loss:0.1767568737268448\n",
      "step:86 loss:0.5494850277900696\n",
      "step:87 loss:0.2506283223628998\n",
      "step:88 loss:0.7761027216911316\n",
      "step:89 loss:0.21468766033649445\n",
      "step:90 loss:0.2761172354221344\n",
      "step:91 loss:0.4938249886035919\n",
      "step:92 loss:0.3627721965312958\n",
      "step:93 loss:0.1932590752840042\n",
      "step:94 loss:0.2801014482975006\n",
      "step:95 loss:0.3031369745731354\n",
      "step:96 loss:0.38175252079963684\n",
      "step:97 loss:0.1897561103105545\n",
      "step:98 loss:0.26803532242774963\n",
      "step:99 loss:0.2745084762573242\n",
      "step:100 loss:0.1241888627409935\n",
      "step:101 loss:0.232710599899292\n",
      "step:102 loss:0.18807677924633026\n",
      "step:103 loss:0.2786247730255127\n",
      "step:104 loss:0.13114149868488312\n",
      "step:105 loss:0.3295290172100067\n",
      "step:106 loss:0.31143760681152344\n",
      "step:107 loss:0.23652620613574982\n",
      "step:108 loss:0.16551360487937927\n",
      "step:109 loss:0.19201509654521942\n",
      "step:110 loss:0.5217297673225403\n",
      "step:111 loss:0.24084192514419556\n",
      "step:112 loss:0.28683462738990784\n",
      "step:113 loss:0.3578980267047882\n",
      "step:114 loss:0.1454598754644394\n",
      "step:115 loss:0.15467539429664612\n",
      "step:116 loss:0.2545417845249176\n",
      "step:117 loss:0.2554210126399994\n",
      "step:118 loss:0.2559381127357483\n",
      "step:119 loss:0.2741304934024811\n",
      "step:120 loss:0.406253457069397\n",
      "step:121 loss:0.2588688135147095\n",
      "step:122 loss:0.24350585043430328\n",
      "step:123 loss:0.23190419375896454\n",
      "step:124 loss:0.25751107931137085\n",
      "step:125 loss:0.3203757107257843\n",
      "step:126 loss:0.26822349429130554\n",
      "step:127 loss:0.48965349793434143\n",
      "step:128 loss:0.37238582968711853\n",
      "step:129 loss:0.2586025595664978\n",
      "step:130 loss:0.1518668383359909\n",
      "step:131 loss:0.3207080662250519\n",
      "step:132 loss:0.3121049702167511\n",
      "step:133 loss:0.2275247722864151\n",
      "step:134 loss:0.47640228271484375\n",
      "step:135 loss:0.21701699495315552\n",
      "step:136 loss:0.40523502230644226\n",
      "step:137 loss:0.3373279571533203\n",
      "step:138 loss:0.2979033589363098\n",
      "step:139 loss:0.16762042045593262\n",
      "step:140 loss:0.22313310205936432\n",
      "step:141 loss:0.2997906804084778\n",
      "step:142 loss:0.25580787658691406\n",
      "step:143 loss:0.3239709734916687\n",
      "step:144 loss:0.37935325503349304\n",
      "step:145 loss:0.17474471032619476\n",
      "step:146 loss:0.33269432187080383\n",
      "step:147 loss:0.42828893661499023\n",
      "step:148 loss:0.36497700214385986\n",
      "step:149 loss:0.4274744689464569\n",
      "step:150 loss:0.24192334711551666\n",
      "step:151 loss:0.18967242538928986\n",
      "step:152 loss:0.34224697947502136\n",
      "step:153 loss:0.24076807498931885\n",
      "step:154 loss:0.2321883887052536\n",
      "step:155 loss:0.3726397752761841\n",
      "step:156 loss:0.5376811623573303\n",
      "step:157 loss:0.1756969541311264\n",
      "step:158 loss:0.4199651777744293\n",
      "step:159 loss:0.20900999009609222\n",
      "step:160 loss:0.28572461009025574\n",
      "step:161 loss:0.23142342269420624\n",
      "step:162 loss:0.26990190148353577\n",
      "step:163 loss:0.1820802092552185\n",
      "step:164 loss:0.3823275566101074\n",
      "step:165 loss:0.19298426806926727\n",
      "step:166 loss:0.15913312137126923\n",
      "step:167 loss:0.22654320299625397\n",
      "step:168 loss:0.26628929376602173\n",
      "step:169 loss:0.19226284325122833\n",
      "step:170 loss:0.2650981843471527\n",
      "step:171 loss:0.439605712890625\n",
      "step:172 loss:0.23055078089237213\n",
      "step:173 loss:0.324652761220932\n",
      "step:174 loss:0.3711739480495453\n",
      "step:175 loss:0.3254025876522064\n",
      "step:176 loss:0.19112181663513184\n",
      "step:177 loss:0.417694091796875\n",
      "step:178 loss:0.48988357186317444\n",
      "step:179 loss:0.34593260288238525\n",
      "step:180 loss:0.2674314081668854\n",
      "step:181 loss:0.3908124268054962\n",
      "step:182 loss:0.2007189840078354\n",
      "step:183 loss:0.36170271039009094\n",
      "step:184 loss:0.25657927989959717\n",
      "step:185 loss:0.23579470813274384\n",
      "step:186 loss:0.2196149080991745\n",
      "step:187 loss:0.3498327434062958\n",
      "step:188 loss:0.23632825911045074\n",
      "total_train_loss:0.29442230588261115, total_test_acc:0.7490057445868317\n",
      "------------------------------ \n",
      " epoch: 7\n",
      "step:0 loss:0.3033505976200104\n",
      "step:1 loss:0.3858107030391693\n",
      "step:2 loss:0.3591080605983734\n",
      "step:3 loss:0.18425041437149048\n",
      "step:4 loss:0.36249473690986633\n",
      "step:5 loss:0.18009072542190552\n",
      "step:6 loss:0.1886511594057083\n",
      "step:7 loss:0.1536516696214676\n",
      "step:8 loss:0.43552014231681824\n",
      "step:9 loss:0.23346243798732758\n",
      "step:10 loss:0.32772520184516907\n",
      "step:11 loss:0.397612065076828\n",
      "step:12 loss:0.339258074760437\n",
      "step:13 loss:0.3483641445636749\n",
      "step:14 loss:0.15470927953720093\n",
      "step:15 loss:0.1974399834871292\n",
      "step:16 loss:0.3326742351055145\n",
      "step:17 loss:0.298490434885025\n",
      "step:18 loss:0.47481611371040344\n",
      "step:19 loss:0.24025113880634308\n",
      "step:20 loss:0.274351567029953\n",
      "step:21 loss:0.18591226637363434\n",
      "step:22 loss:0.46342992782592773\n",
      "step:23 loss:0.3495992124080658\n",
      "step:24 loss:0.224315345287323\n",
      "step:25 loss:0.2521175742149353\n",
      "step:26 loss:0.30048468708992004\n",
      "step:27 loss:0.17234401404857635\n",
      "step:28 loss:0.20443832874298096\n",
      "step:29 loss:0.17551358044147491\n",
      "step:30 loss:0.19197791814804077\n",
      "step:31 loss:0.3087279200553894\n",
      "step:32 loss:0.207956001162529\n",
      "step:33 loss:0.19196496903896332\n",
      "step:34 loss:0.17290370166301727\n",
      "step:35 loss:0.4408569037914276\n",
      "step:36 loss:0.1933443397283554\n",
      "step:37 loss:0.38885700702667236\n",
      "step:38 loss:0.18083345890045166\n",
      "step:39 loss:0.15823717415332794\n",
      "step:40 loss:0.4562528431415558\n",
      "step:41 loss:0.3540448844432831\n",
      "step:42 loss:0.28022730350494385\n",
      "step:43 loss:0.28601500391960144\n",
      "step:44 loss:0.1926424354314804\n",
      "step:45 loss:0.2030128836631775\n",
      "step:46 loss:0.3444921672344208\n",
      "step:47 loss:0.287032812833786\n",
      "step:48 loss:0.3475940525531769\n",
      "step:49 loss:0.4332200288772583\n",
      "step:50 loss:0.231424018740654\n",
      "step:51 loss:0.20564579963684082\n",
      "step:52 loss:0.43845799565315247\n",
      "step:53 loss:0.17082317173480988\n",
      "step:54 loss:0.27902111411094666\n",
      "step:55 loss:0.31173738837242126\n",
      "step:56 loss:0.18712396919727325\n",
      "step:57 loss:0.3139968514442444\n",
      "step:58 loss:0.19358164072036743\n",
      "step:59 loss:0.45267078280448914\n",
      "step:60 loss:0.2627306878566742\n",
      "step:61 loss:0.47059011459350586\n",
      "step:62 loss:0.2694965600967407\n",
      "step:63 loss:0.19011829793453217\n",
      "step:64 loss:0.19956375658512115\n",
      "step:65 loss:0.2903992831707001\n",
      "step:66 loss:0.22899217903614044\n",
      "step:67 loss:0.22884683310985565\n",
      "step:68 loss:0.44768810272216797\n",
      "step:69 loss:0.23430365324020386\n",
      "step:70 loss:0.1981366127729416\n",
      "step:71 loss:0.22197146713733673\n",
      "step:72 loss:0.31724241375923157\n",
      "step:73 loss:0.22834379971027374\n",
      "step:74 loss:0.26757997274398804\n",
      "step:75 loss:0.29315730929374695\n",
      "step:76 loss:0.2342788130044937\n",
      "step:77 loss:0.2507196366786957\n",
      "step:78 loss:0.19629286229610443\n",
      "step:79 loss:0.3012489378452301\n",
      "step:80 loss:0.30463364720344543\n",
      "step:81 loss:0.28356510400772095\n",
      "step:82 loss:0.20306766033172607\n",
      "step:83 loss:0.18729306757450104\n",
      "step:84 loss:0.36617085337638855\n",
      "step:85 loss:0.24418866634368896\n",
      "step:86 loss:0.2828110158443451\n",
      "step:87 loss:0.25801795721054077\n",
      "step:88 loss:0.182292640209198\n",
      "step:89 loss:0.40820708870887756\n",
      "step:90 loss:0.28993234038352966\n",
      "step:91 loss:0.33269935846328735\n",
      "step:92 loss:0.21618640422821045\n",
      "step:93 loss:0.17711801826953888\n",
      "step:94 loss:0.4187929332256317\n",
      "step:95 loss:0.17791135609149933\n",
      "step:96 loss:0.2871258556842804\n",
      "step:97 loss:0.42863723635673523\n",
      "step:98 loss:0.1787470579147339\n",
      "step:99 loss:0.22432588040828705\n",
      "step:100 loss:0.22788357734680176\n",
      "step:101 loss:0.18676503002643585\n",
      "step:102 loss:0.24521778523921967\n",
      "step:103 loss:0.24553696811199188\n",
      "step:104 loss:0.3674921989440918\n",
      "step:105 loss:0.2693498730659485\n",
      "step:106 loss:0.3099655508995056\n",
      "step:107 loss:0.2752741873264313\n",
      "step:108 loss:0.28027722239494324\n",
      "step:109 loss:0.31238213181495667\n",
      "step:110 loss:0.27532514929771423\n",
      "step:111 loss:0.21158663928508759\n",
      "step:112 loss:0.4834609925746918\n",
      "step:113 loss:0.23967210948467255\n",
      "step:114 loss:0.1962975114583969\n",
      "step:115 loss:0.18803977966308594\n",
      "step:116 loss:0.1815883368253708\n",
      "step:117 loss:0.31612175703048706\n",
      "step:118 loss:0.21868832409381866\n",
      "step:119 loss:0.23911194503307343\n",
      "step:120 loss:0.20289604365825653\n",
      "step:121 loss:0.266638845205307\n",
      "step:122 loss:0.19993972778320312\n",
      "step:123 loss:0.4006631076335907\n",
      "step:124 loss:0.1447358876466751\n",
      "step:125 loss:0.0813542827963829\n",
      "step:126 loss:0.15747858583927155\n",
      "step:127 loss:0.29707571864128113\n",
      "step:128 loss:0.2713572382926941\n",
      "step:129 loss:0.3278842866420746\n",
      "step:130 loss:0.3499540388584137\n",
      "step:131 loss:0.18711185455322266\n",
      "step:132 loss:0.29911336302757263\n",
      "step:133 loss:0.25429603457450867\n",
      "step:134 loss:0.4380995035171509\n",
      "step:135 loss:0.4165765941143036\n",
      "step:136 loss:0.1620437502861023\n",
      "step:137 loss:0.41335880756378174\n",
      "step:138 loss:0.21003608405590057\n",
      "step:139 loss:0.3394729793071747\n",
      "step:140 loss:0.22569924592971802\n",
      "step:141 loss:0.262029767036438\n",
      "step:142 loss:0.4653107821941376\n",
      "step:143 loss:0.28582432866096497\n",
      "step:144 loss:0.3163374364376068\n",
      "step:145 loss:0.24385009706020355\n",
      "step:146 loss:0.25489920377731323\n",
      "step:147 loss:0.2234492152929306\n",
      "step:148 loss:0.33612117171287537\n",
      "step:149 loss:0.20790547132492065\n",
      "step:150 loss:0.1357429176568985\n",
      "step:151 loss:0.247413769364357\n",
      "step:152 loss:0.2304106503725052\n",
      "step:153 loss:0.28714463114738464\n",
      "step:154 loss:0.2028414011001587\n",
      "step:155 loss:0.30673274397850037\n",
      "step:156 loss:0.23393313586711884\n",
      "step:157 loss:0.3675652742385864\n",
      "step:158 loss:0.28858575224876404\n",
      "step:159 loss:0.1838034987449646\n",
      "step:160 loss:0.2499374896287918\n",
      "step:161 loss:0.23282767832279205\n",
      "step:162 loss:0.2903115749359131\n",
      "step:163 loss:0.23883174359798431\n",
      "step:164 loss:0.32628384232521057\n",
      "step:165 loss:0.21092182397842407\n",
      "step:166 loss:0.27420535683631897\n",
      "step:167 loss:0.3010503947734833\n",
      "step:168 loss:0.2205817848443985\n",
      "step:169 loss:0.3662289083003998\n",
      "step:170 loss:0.1993846297264099\n",
      "step:171 loss:0.34883955121040344\n",
      "step:172 loss:0.45165348052978516\n",
      "step:173 loss:0.25544002652168274\n",
      "step:174 loss:0.21739943325519562\n",
      "step:175 loss:0.2573001980781555\n",
      "step:176 loss:0.19970522820949554\n",
      "step:177 loss:0.16834114491939545\n",
      "step:178 loss:0.389913409948349\n",
      "step:179 loss:0.1263599693775177\n",
      "step:180 loss:0.1378875970840454\n",
      "step:181 loss:0.22184140980243683\n",
      "step:182 loss:0.0752156600356102\n",
      "step:183 loss:0.20973433554172516\n",
      "step:184 loss:0.31272202730178833\n",
      "step:185 loss:0.19821768999099731\n",
      "step:186 loss:0.24359796941280365\n",
      "step:187 loss:0.3201633393764496\n",
      "step:188 loss:0.19320808351039886\n",
      "total_train_loss:0.2708626057397812, total_test_acc:0.7901016349977905\n",
      "------------------------------ \n",
      " epoch: 8\n",
      "step:0 loss:0.18912853300571442\n",
      "step:1 loss:0.2780134379863739\n",
      "step:2 loss:0.32163745164871216\n",
      "step:3 loss:0.0836077407002449\n",
      "step:4 loss:0.14906249940395355\n",
      "step:5 loss:0.1595289558172226\n",
      "step:6 loss:0.2087675780057907\n",
      "step:7 loss:0.41056200861930847\n",
      "step:8 loss:0.2699601948261261\n",
      "step:9 loss:0.45233604311943054\n",
      "step:10 loss:0.20207738876342773\n",
      "step:11 loss:0.4590977132320404\n",
      "step:12 loss:0.12297423928976059\n",
      "step:13 loss:0.27210789918899536\n",
      "step:14 loss:0.34121859073638916\n",
      "step:15 loss:0.3327154815196991\n",
      "step:16 loss:0.3629927337169647\n",
      "step:17 loss:0.1968827247619629\n",
      "step:18 loss:0.5077902674674988\n",
      "step:19 loss:0.27444079518318176\n",
      "step:20 loss:0.4932279884815216\n",
      "step:21 loss:0.2917996942996979\n",
      "step:22 loss:0.4547542333602905\n",
      "step:23 loss:0.26628127694129944\n",
      "step:24 loss:0.25781556963920593\n",
      "step:25 loss:0.5124288201332092\n",
      "step:26 loss:0.2977857291698456\n",
      "step:27 loss:0.22932405769824982\n",
      "step:28 loss:0.2626345157623291\n",
      "step:29 loss:0.2660232186317444\n",
      "step:30 loss:0.19331882894039154\n",
      "step:31 loss:0.22910819947719574\n",
      "step:32 loss:0.4460580348968506\n",
      "step:33 loss:0.3801317512989044\n",
      "step:34 loss:0.21739192306995392\n",
      "step:35 loss:0.20567594468593597\n",
      "step:36 loss:0.26869726181030273\n",
      "step:37 loss:0.2307383418083191\n",
      "step:38 loss:0.29174092411994934\n",
      "step:39 loss:0.3514728546142578\n",
      "step:40 loss:0.2987419366836548\n",
      "step:41 loss:0.2603182792663574\n",
      "step:42 loss:0.36930540204048157\n",
      "step:43 loss:0.3636169731616974\n",
      "step:44 loss:0.11616126447916031\n",
      "step:45 loss:0.2968690097332001\n",
      "step:46 loss:0.3357095420360565\n",
      "step:47 loss:0.39509132504463196\n",
      "step:48 loss:0.2590971291065216\n",
      "step:49 loss:0.31025996804237366\n",
      "step:50 loss:0.22040317952632904\n",
      "step:51 loss:0.36747682094573975\n",
      "step:52 loss:0.23179686069488525\n",
      "step:53 loss:0.26848480105400085\n",
      "step:54 loss:0.2604556083679199\n",
      "step:55 loss:0.277749627828598\n",
      "step:56 loss:0.24243973195552826\n",
      "step:57 loss:0.13525526225566864\n",
      "step:58 loss:0.2541404664516449\n",
      "step:59 loss:0.2740206718444824\n",
      "step:60 loss:0.16921961307525635\n",
      "step:61 loss:0.27890822291374207\n",
      "step:62 loss:0.29530638456344604\n",
      "step:63 loss:0.23365557193756104\n",
      "step:64 loss:0.1943221092224121\n",
      "step:65 loss:0.20018649101257324\n",
      "step:66 loss:0.2297118902206421\n",
      "step:67 loss:0.17430251836776733\n",
      "step:68 loss:0.3676941692829132\n",
      "step:69 loss:0.3140051066875458\n",
      "step:70 loss:0.2719482481479645\n",
      "step:71 loss:0.25418606400489807\n",
      "step:72 loss:0.10573259741067886\n",
      "step:73 loss:0.21798165142536163\n",
      "step:74 loss:0.2530965209007263\n",
      "step:75 loss:0.15636910498142242\n",
      "step:76 loss:0.23316580057144165\n",
      "step:77 loss:0.3292379081249237\n",
      "step:78 loss:0.19349515438079834\n",
      "step:79 loss:0.16976921260356903\n",
      "step:80 loss:0.23306451737880707\n",
      "step:81 loss:0.19378912448883057\n",
      "step:82 loss:0.19848136603832245\n",
      "step:83 loss:0.3788241147994995\n",
      "step:84 loss:0.24134057760238647\n",
      "step:85 loss:0.19772998988628387\n",
      "step:86 loss:0.2536366581916809\n",
      "step:87 loss:0.26805242896080017\n",
      "step:88 loss:0.1756775975227356\n",
      "step:89 loss:0.4864940941333771\n",
      "step:90 loss:0.22725866734981537\n",
      "step:91 loss:0.3336848020553589\n",
      "step:92 loss:0.3262163996696472\n",
      "step:93 loss:0.14072079956531525\n",
      "step:94 loss:0.19170348346233368\n",
      "step:95 loss:0.3360961377620697\n",
      "step:96 loss:0.24495674669742584\n",
      "step:97 loss:0.16952882707118988\n",
      "step:98 loss:0.2093767523765564\n",
      "step:99 loss:0.34626248478889465\n",
      "step:100 loss:0.23855799436569214\n",
      "step:101 loss:0.2616495192050934\n",
      "step:102 loss:0.16120995581150055\n",
      "step:103 loss:0.26647311449050903\n",
      "step:104 loss:0.1662091314792633\n",
      "step:105 loss:0.1581517904996872\n",
      "step:106 loss:0.31009456515312195\n",
      "step:107 loss:0.4375061094760895\n",
      "step:108 loss:0.1860780119895935\n",
      "step:109 loss:0.0892692506313324\n",
      "step:110 loss:0.1435968577861786\n",
      "step:111 loss:0.26182007789611816\n",
      "step:112 loss:0.2512385845184326\n",
      "step:113 loss:0.24321843683719635\n",
      "step:114 loss:0.3019649386405945\n",
      "step:115 loss:0.13001687824726105\n",
      "step:116 loss:0.21152059733867645\n",
      "step:117 loss:0.16280098259449005\n",
      "step:118 loss:0.19465886056423187\n",
      "step:119 loss:0.299791544675827\n",
      "step:120 loss:0.21929995715618134\n",
      "step:121 loss:0.3040344715118408\n",
      "step:122 loss:0.08351653814315796\n",
      "step:123 loss:0.28760355710983276\n",
      "step:124 loss:0.32551950216293335\n",
      "step:125 loss:0.38360628485679626\n",
      "step:126 loss:0.2076491117477417\n",
      "step:127 loss:0.1694580465555191\n",
      "step:128 loss:0.3196850121021271\n",
      "step:129 loss:0.4184352159500122\n",
      "step:130 loss:0.24637667834758759\n",
      "step:131 loss:0.23138092458248138\n",
      "step:132 loss:0.14942364394664764\n",
      "step:133 loss:0.264104425907135\n",
      "step:134 loss:0.27966877818107605\n",
      "step:135 loss:0.2584800720214844\n",
      "step:136 loss:0.2517819106578827\n",
      "step:137 loss:0.25863784551620483\n",
      "step:138 loss:0.4051321744918823\n",
      "step:139 loss:0.23384064435958862\n",
      "step:140 loss:0.19111134111881256\n",
      "step:141 loss:0.2423871010541916\n",
      "step:142 loss:0.2526808977127075\n",
      "step:143 loss:0.2462834119796753\n",
      "step:144 loss:0.24882008135318756\n",
      "step:145 loss:0.3535894453525543\n",
      "step:146 loss:0.14575384557247162\n",
      "step:147 loss:0.2563503086566925\n",
      "step:148 loss:0.25874748826026917\n",
      "step:149 loss:0.20225435495376587\n",
      "step:150 loss:0.28395378589630127\n",
      "step:151 loss:0.17006660997867584\n",
      "step:152 loss:0.21074305474758148\n",
      "step:153 loss:0.16993118822574615\n",
      "step:154 loss:0.18235529959201813\n",
      "step:155 loss:0.33571651577949524\n",
      "step:156 loss:0.1978714019060135\n",
      "step:157 loss:0.3256596028804779\n",
      "step:158 loss:0.28257283568382263\n",
      "step:159 loss:0.20666027069091797\n",
      "step:160 loss:0.15404659509658813\n",
      "step:161 loss:0.24752390384674072\n",
      "step:162 loss:0.21090686321258545\n",
      "step:163 loss:0.3642900884151459\n",
      "step:164 loss:0.42242053151130676\n",
      "step:165 loss:0.20634685456752777\n",
      "step:166 loss:0.31526681780815125\n",
      "step:167 loss:0.1783708930015564\n",
      "step:168 loss:0.29538044333457947\n",
      "step:169 loss:0.21837718784809113\n",
      "step:170 loss:0.15074510872364044\n",
      "step:171 loss:0.1259423941373825\n",
      "step:172 loss:0.2693682014942169\n",
      "step:173 loss:0.1978331357240677\n",
      "step:174 loss:0.3255503475666046\n",
      "step:175 loss:0.3157656490802765\n",
      "step:176 loss:0.31126150488853455\n",
      "step:177 loss:0.29376906156539917\n",
      "step:178 loss:0.2644270956516266\n",
      "step:179 loss:0.2030525803565979\n",
      "step:180 loss:0.31739720702171326\n",
      "step:181 loss:0.24668389558792114\n",
      "step:182 loss:0.3204539120197296\n",
      "step:183 loss:0.25462576746940613\n",
      "step:184 loss:0.1692945957183838\n",
      "step:185 loss:0.2894521951675415\n",
      "step:186 loss:0.19659942388534546\n",
      "step:187 loss:0.3531552255153656\n",
      "step:188 loss:0.12925101816654205\n",
      "total_train_loss:0.2604171518791229, total_test_acc:0.8444542642509942\n",
      "------------------------------ \n",
      " epoch: 9\n",
      "step:0 loss:0.26061752438545227\n",
      "step:1 loss:0.2641904354095459\n",
      "step:2 loss:0.31051018834114075\n",
      "step:3 loss:0.1829293817281723\n",
      "step:4 loss:0.32292208075523376\n",
      "step:5 loss:0.185637429356575\n",
      "step:6 loss:0.07986663281917572\n",
      "step:7 loss:0.2150614708662033\n",
      "step:8 loss:0.2264242172241211\n",
      "step:9 loss:0.21011759340763092\n",
      "step:10 loss:0.2644793689250946\n",
      "step:11 loss:0.27512678503990173\n",
      "step:12 loss:0.21144747734069824\n",
      "step:13 loss:0.24060045182704926\n",
      "step:14 loss:0.44676482677459717\n",
      "step:15 loss:0.3878653347492218\n",
      "step:16 loss:0.43572115898132324\n",
      "step:17 loss:0.12743434309959412\n",
      "step:18 loss:0.20273680984973907\n",
      "step:19 loss:0.34391143918037415\n",
      "step:20 loss:0.1553238332271576\n",
      "step:21 loss:0.3512798845767975\n",
      "step:22 loss:0.21150760352611542\n",
      "step:23 loss:0.20332564413547516\n",
      "step:24 loss:0.3450024127960205\n",
      "step:25 loss:0.2560400068759918\n",
      "step:26 loss:0.21040339767932892\n",
      "step:27 loss:0.30091992020606995\n",
      "step:28 loss:0.2262060046195984\n",
      "step:29 loss:0.2910930812358856\n",
      "step:30 loss:0.22828245162963867\n",
      "step:31 loss:0.2617427110671997\n",
      "step:32 loss:0.09829529374837875\n",
      "step:33 loss:0.30225488543510437\n",
      "step:34 loss:0.3162185847759247\n",
      "step:35 loss:0.24725347757339478\n",
      "step:36 loss:0.36112692952156067\n",
      "step:37 loss:0.2030211091041565\n",
      "step:38 loss:0.24533943831920624\n",
      "step:39 loss:0.1805158257484436\n",
      "step:40 loss:0.27317890524864197\n",
      "step:41 loss:0.18737171590328217\n",
      "step:42 loss:0.14083455502986908\n",
      "step:43 loss:0.31265753507614136\n",
      "step:44 loss:0.18347607553005219\n",
      "step:45 loss:0.3372180759906769\n",
      "step:46 loss:0.2283545583486557\n",
      "step:47 loss:0.31288689374923706\n",
      "step:48 loss:0.21715106070041656\n",
      "step:49 loss:0.1603146642446518\n",
      "step:50 loss:0.2418491393327713\n",
      "step:51 loss:0.11631402373313904\n",
      "step:52 loss:0.3784317672252655\n",
      "step:53 loss:0.2535921633243561\n",
      "step:54 loss:0.21018581092357635\n",
      "step:55 loss:0.3683553636074066\n",
      "step:56 loss:0.33042389154434204\n",
      "step:57 loss:0.21121467649936676\n",
      "step:58 loss:0.3197881281375885\n",
      "step:59 loss:0.3040517270565033\n",
      "step:60 loss:0.2067628651857376\n",
      "step:61 loss:0.2079647332429886\n",
      "step:62 loss:0.42470625042915344\n",
      "step:63 loss:0.2278369814157486\n",
      "step:64 loss:0.43879783153533936\n",
      "step:65 loss:0.34458795189857483\n",
      "step:66 loss:0.43725910782814026\n",
      "step:67 loss:0.35889139771461487\n",
      "step:68 loss:0.43013548851013184\n",
      "step:69 loss:0.290762335062027\n",
      "step:70 loss:0.14302997291088104\n",
      "step:71 loss:0.4001947343349457\n",
      "step:72 loss:0.18341992795467377\n",
      "step:73 loss:0.2700359523296356\n",
      "step:74 loss:0.28452637791633606\n",
      "step:75 loss:0.2884658873081207\n",
      "step:76 loss:0.11842840164899826\n",
      "step:77 loss:0.3516346216201782\n",
      "step:78 loss:0.18713968992233276\n",
      "step:79 loss:0.28618043661117554\n",
      "step:80 loss:0.09053697437047958\n",
      "step:81 loss:0.31105226278305054\n",
      "step:82 loss:0.08074011653661728\n",
      "step:83 loss:0.3301030099391937\n",
      "step:84 loss:0.38780856132507324\n",
      "step:85 loss:0.22225451469421387\n",
      "step:86 loss:0.375969260931015\n",
      "step:87 loss:0.24989451467990875\n",
      "step:88 loss:0.2297673225402832\n",
      "step:89 loss:0.2930419147014618\n",
      "step:90 loss:0.19448818266391754\n",
      "step:91 loss:0.25500503182411194\n",
      "step:92 loss:0.21872560679912567\n",
      "step:93 loss:0.1071055456995964\n",
      "step:94 loss:0.14827705919742584\n",
      "step:95 loss:0.3353954255580902\n",
      "step:96 loss:0.1623832732439041\n",
      "step:97 loss:0.23393206298351288\n",
      "step:98 loss:0.07626944780349731\n",
      "step:99 loss:0.22842736542224884\n",
      "step:100 loss:0.3110681176185608\n",
      "step:101 loss:0.21744222939014435\n",
      "step:102 loss:0.29630157351493835\n",
      "step:103 loss:0.4270986020565033\n",
      "step:104 loss:0.19291746616363525\n",
      "step:105 loss:0.2004687637090683\n",
      "step:106 loss:0.5150265097618103\n",
      "step:107 loss:0.1752445250749588\n",
      "step:108 loss:0.2937938868999481\n",
      "step:109 loss:0.34832778573036194\n",
      "step:110 loss:0.24848973751068115\n",
      "step:111 loss:0.2082410603761673\n",
      "step:112 loss:0.31759700179100037\n",
      "step:113 loss:0.19922566413879395\n",
      "step:114 loss:0.28809988498687744\n",
      "step:115 loss:0.3329133689403534\n",
      "step:116 loss:0.4225812256336212\n",
      "step:117 loss:0.22280649840831757\n",
      "step:118 loss:0.3660496771335602\n",
      "step:119 loss:0.15987607836723328\n",
      "step:120 loss:0.16136324405670166\n",
      "step:121 loss:0.34775784611701965\n",
      "step:122 loss:0.14813478291034698\n",
      "step:123 loss:0.1876392960548401\n",
      "step:124 loss:0.29958003759384155\n",
      "step:125 loss:0.3401828110218048\n",
      "step:126 loss:0.23025810718536377\n",
      "step:127 loss:0.2326069474220276\n",
      "step:128 loss:0.18452966213226318\n",
      "step:129 loss:0.13888247311115265\n",
      "step:130 loss:0.17434220016002655\n",
      "step:131 loss:0.16095668077468872\n",
      "step:132 loss:0.09040691703557968\n",
      "step:133 loss:0.2722465693950653\n",
      "step:134 loss:0.20839367806911469\n",
      "step:135 loss:0.1418105661869049\n",
      "step:136 loss:0.5127306580543518\n",
      "step:137 loss:0.3178690969944\n",
      "step:138 loss:0.26455947756767273\n",
      "step:139 loss:0.13328278064727783\n",
      "step:140 loss:0.15319333970546722\n",
      "step:141 loss:0.14437603950500488\n",
      "step:142 loss:0.2809155285358429\n",
      "step:143 loss:0.2514147460460663\n",
      "step:144 loss:0.14333152770996094\n",
      "step:145 loss:0.15450602769851685\n",
      "step:146 loss:0.2333429902791977\n",
      "step:147 loss:0.23673571646213531\n",
      "step:148 loss:0.20297648012638092\n",
      "step:149 loss:0.09824404120445251\n",
      "step:150 loss:0.3425394594669342\n",
      "step:151 loss:0.2302437275648117\n",
      "step:152 loss:0.1931539922952652\n",
      "step:153 loss:0.19464850425720215\n",
      "step:154 loss:0.14742255210876465\n",
      "step:155 loss:0.12851682305335999\n",
      "step:156 loss:0.21389132738113403\n",
      "step:157 loss:0.20630569756031036\n",
      "step:158 loss:0.4160667955875397\n",
      "step:159 loss:0.2739894688129425\n",
      "step:160 loss:0.2569074332714081\n",
      "step:161 loss:0.2215633988380432\n",
      "step:162 loss:0.21450085937976837\n",
      "step:163 loss:0.20196403563022614\n",
      "step:164 loss:0.2905865013599396\n",
      "step:165 loss:0.25538313388824463\n",
      "step:166 loss:0.1870189756155014\n",
      "step:167 loss:0.19048869609832764\n",
      "step:168 loss:0.2537321150302887\n",
      "step:169 loss:0.25927338004112244\n",
      "step:170 loss:0.1510300189256668\n",
      "step:171 loss:0.3596964180469513\n",
      "step:172 loss:0.1944190263748169\n",
      "step:173 loss:0.22328020632266998\n",
      "step:174 loss:0.23836994171142578\n",
      "step:175 loss:0.33999815583229065\n",
      "step:176 loss:0.25091981887817383\n",
      "step:177 loss:0.12354723364114761\n",
      "step:178 loss:0.18883903324604034\n",
      "step:179 loss:0.25932732224464417\n",
      "step:180 loss:0.14736640453338623\n",
      "step:181 loss:0.4830726385116577\n",
      "step:182 loss:0.361374169588089\n",
      "step:183 loss:0.0703224241733551\n",
      "step:184 loss:0.26813456416130066\n",
      "step:185 loss:0.12881337106227875\n",
      "step:186 loss:0.2249269038438797\n",
      "step:187 loss:0.3491112291812897\n",
      "step:188 loss:0.3888663649559021\n",
      "total_train_loss:0.2516428650297383, total_test_acc:0.8448961555457357\n",
      "------------------------------ \n",
      " epoch: 10\n",
      "step:0 loss:0.39249542355537415\n",
      "step:1 loss:0.16648809611797333\n",
      "step:2 loss:0.2259926199913025\n",
      "step:3 loss:0.4282362461090088\n",
      "step:4 loss:0.09177767485380173\n",
      "step:5 loss:0.33913108706474304\n",
      "step:6 loss:0.24562841653823853\n",
      "step:7 loss:0.31382328271865845\n",
      "step:8 loss:0.229638934135437\n",
      "step:9 loss:0.22841376066207886\n",
      "step:10 loss:0.26324304938316345\n",
      "step:11 loss:0.2533731758594513\n",
      "step:12 loss:0.24871492385864258\n",
      "step:13 loss:0.2347281128168106\n",
      "step:14 loss:0.22566886246204376\n",
      "step:15 loss:0.2870941460132599\n",
      "step:16 loss:0.17647622525691986\n",
      "step:17 loss:0.2831116020679474\n",
      "step:18 loss:0.23489058017730713\n",
      "step:19 loss:0.2027721256017685\n",
      "step:20 loss:0.2737271189689636\n",
      "step:21 loss:0.33297446370124817\n",
      "step:22 loss:0.3598879277706146\n",
      "step:23 loss:0.18439631164073944\n",
      "step:24 loss:0.13370192050933838\n",
      "step:25 loss:0.29461798071861267\n",
      "step:26 loss:0.3343855142593384\n",
      "step:27 loss:0.2641315162181854\n",
      "step:28 loss:0.19301994144916534\n",
      "step:29 loss:0.328306645154953\n",
      "step:30 loss:0.10193068534135818\n",
      "step:31 loss:0.12484702467918396\n",
      "step:32 loss:0.27729663252830505\n",
      "step:33 loss:0.27904409170150757\n",
      "step:34 loss:0.3124484121799469\n",
      "step:35 loss:0.16727523505687714\n",
      "step:36 loss:0.13060900568962097\n",
      "step:37 loss:0.18974828720092773\n",
      "step:38 loss:0.11895085126161575\n",
      "step:39 loss:0.26509013772010803\n",
      "step:40 loss:0.1939753293991089\n",
      "step:41 loss:0.15420889854431152\n",
      "step:42 loss:0.208576500415802\n",
      "step:43 loss:0.23696954548358917\n",
      "step:44 loss:0.27673500776290894\n",
      "step:45 loss:0.28812822699546814\n",
      "step:46 loss:0.35617795586586\n",
      "step:47 loss:0.40882501006126404\n",
      "step:48 loss:0.32540854811668396\n",
      "step:49 loss:0.1959899514913559\n",
      "step:50 loss:0.24950164556503296\n",
      "step:51 loss:0.1814255565404892\n",
      "step:52 loss:0.3083874583244324\n",
      "step:53 loss:0.14326082170009613\n",
      "step:54 loss:0.22356809675693512\n",
      "step:55 loss:0.2605213522911072\n",
      "step:56 loss:0.2256689816713333\n",
      "step:57 loss:0.268553227186203\n",
      "step:58 loss:0.13608650863170624\n",
      "step:59 loss:0.166317418217659\n",
      "step:60 loss:0.22515016794204712\n",
      "step:61 loss:0.20212094485759735\n",
      "step:62 loss:0.2474381923675537\n",
      "step:63 loss:0.23043549060821533\n",
      "step:64 loss:0.26100024580955505\n",
      "step:65 loss:0.3426922559738159\n",
      "step:66 loss:0.19464750587940216\n",
      "step:67 loss:0.21973668038845062\n",
      "step:68 loss:0.27036044001579285\n",
      "step:69 loss:0.2767474353313446\n",
      "step:70 loss:0.2377871423959732\n",
      "step:71 loss:0.2421526312828064\n",
      "step:72 loss:0.3307723104953766\n",
      "step:73 loss:0.23828275501728058\n",
      "step:74 loss:0.22871606051921844\n",
      "step:75 loss:0.2607690691947937\n",
      "step:76 loss:0.2680269777774811\n",
      "step:77 loss:0.33635592460632324\n",
      "step:78 loss:0.15699949860572815\n",
      "step:79 loss:0.28740212321281433\n",
      "step:80 loss:0.20563997328281403\n",
      "step:81 loss:0.28077220916748047\n",
      "step:82 loss:0.20322544872760773\n",
      "step:83 loss:0.31938549876213074\n",
      "step:84 loss:0.3419921398162842\n",
      "step:85 loss:0.16084276139736176\n",
      "step:86 loss:0.1728571206331253\n",
      "step:87 loss:0.21415920555591583\n",
      "step:88 loss:0.14961960911750793\n",
      "step:89 loss:0.357406347990036\n",
      "step:90 loss:0.12116417288780212\n",
      "step:91 loss:0.31053486466407776\n",
      "step:92 loss:0.4081040620803833\n",
      "step:93 loss:0.4756554067134857\n",
      "step:94 loss:0.2657511532306671\n",
      "step:95 loss:0.4147392213344574\n",
      "step:96 loss:0.18726342916488647\n",
      "step:97 loss:0.3192514181137085\n",
      "step:98 loss:0.3717151880264282\n",
      "step:99 loss:0.19887100160121918\n",
      "step:100 loss:0.21844317018985748\n",
      "step:101 loss:0.21657569706439972\n",
      "step:102 loss:0.18629305064678192\n",
      "step:103 loss:0.3407694399356842\n",
      "step:104 loss:0.24887120723724365\n",
      "step:105 loss:0.23657678067684174\n",
      "step:106 loss:0.17385518550872803\n",
      "step:107 loss:0.22336554527282715\n",
      "step:108 loss:0.17256122827529907\n",
      "step:109 loss:0.30459854006767273\n",
      "step:110 loss:0.1521216779947281\n",
      "step:111 loss:0.13594640791416168\n",
      "step:112 loss:0.1532929241657257\n",
      "step:113 loss:0.1961159110069275\n",
      "step:114 loss:0.1985858678817749\n",
      "step:115 loss:0.15638911724090576\n",
      "step:116 loss:0.1698111742734909\n",
      "step:117 loss:0.20998793840408325\n",
      "step:118 loss:0.49571141600608826\n",
      "step:119 loss:0.2826259434223175\n",
      "step:120 loss:0.43501099944114685\n",
      "step:121 loss:0.08218172937631607\n",
      "step:122 loss:0.24654258787631989\n",
      "step:123 loss:0.3070358633995056\n",
      "step:124 loss:0.1632363349199295\n",
      "step:125 loss:0.25075989961624146\n",
      "step:126 loss:0.22598741948604584\n",
      "step:127 loss:0.5158095359802246\n",
      "step:128 loss:0.20326973497867584\n",
      "step:129 loss:0.5925244688987732\n",
      "step:130 loss:0.22455249726772308\n",
      "step:131 loss:0.3428688943386078\n",
      "step:132 loss:0.3841499388217926\n",
      "step:133 loss:0.3747815191745758\n",
      "step:134 loss:0.5358503460884094\n",
      "step:135 loss:0.2621212303638458\n",
      "step:136 loss:0.1928996592760086\n",
      "step:137 loss:0.3577977418899536\n",
      "step:138 loss:0.4139954745769501\n",
      "step:139 loss:0.22720634937286377\n",
      "step:140 loss:0.24603332579135895\n",
      "step:141 loss:0.26782122254371643\n",
      "step:142 loss:0.212566077709198\n",
      "step:143 loss:0.2521008253097534\n",
      "step:144 loss:0.1313697248697281\n",
      "step:145 loss:0.2528413236141205\n",
      "step:146 loss:0.24859140813350677\n",
      "step:147 loss:0.2906315326690674\n",
      "step:148 loss:0.21451176702976227\n",
      "step:149 loss:0.20280681550502777\n",
      "step:150 loss:0.349310040473938\n",
      "step:151 loss:0.1915498524904251\n",
      "step:152 loss:0.2126316875219345\n",
      "step:153 loss:0.23789586126804352\n",
      "step:154 loss:0.38473978638648987\n",
      "step:155 loss:0.2544154226779938\n",
      "step:156 loss:0.3199935257434845\n",
      "step:157 loss:0.21583010256290436\n",
      "step:158 loss:0.26144352555274963\n",
      "step:159 loss:0.1988593488931656\n",
      "step:160 loss:0.3717060983181\n",
      "step:161 loss:0.13261522352695465\n",
      "step:162 loss:0.4434605538845062\n",
      "step:163 loss:0.2181376963853836\n",
      "step:164 loss:0.3027231693267822\n",
      "step:165 loss:0.10458889603614807\n",
      "step:166 loss:0.22892604768276215\n",
      "step:167 loss:0.1476045548915863\n",
      "step:168 loss:0.2547973692417145\n",
      "step:169 loss:0.10654902458190918\n",
      "step:170 loss:0.15263672173023224\n",
      "step:171 loss:0.23447275161743164\n",
      "step:172 loss:0.2980705499649048\n",
      "step:173 loss:0.2960861027240753\n",
      "step:174 loss:0.1657133847475052\n",
      "step:175 loss:0.23911167681217194\n",
      "step:176 loss:0.22793395817279816\n",
      "step:177 loss:0.25321218371391296\n",
      "step:178 loss:0.1138763427734375\n",
      "step:179 loss:0.21032649278640747\n",
      "step:180 loss:0.3512183427810669\n",
      "step:181 loss:0.2553749978542328\n",
      "step:182 loss:0.1700764149427414\n",
      "step:183 loss:0.13696078956127167\n",
      "step:184 loss:0.3363610804080963\n",
      "step:185 loss:0.14545248448848724\n",
      "step:186 loss:0.2668816149234772\n",
      "step:187 loss:0.32257935404777527\n",
      "step:188 loss:0.2895447015762329\n",
      "total_train_loss:0.2532815977613977, total_test_acc:0.8833406981882457\n",
      "------------------------------ \n",
      " epoch: 11\n",
      "step:0 loss:0.2604276239871979\n",
      "step:1 loss:0.23261702060699463\n",
      "step:2 loss:0.17790067195892334\n",
      "step:3 loss:0.1774400919675827\n",
      "step:4 loss:0.22926878929138184\n",
      "step:5 loss:0.23124754428863525\n",
      "step:6 loss:0.3318342864513397\n",
      "step:7 loss:0.2362595647573471\n",
      "step:8 loss:0.22920803725719452\n",
      "step:9 loss:0.1580793261528015\n",
      "step:10 loss:0.28860944509506226\n",
      "step:11 loss:0.21841007471084595\n",
      "step:12 loss:0.2186136394739151\n",
      "step:13 loss:0.2521543800830841\n",
      "step:14 loss:0.47465780377388\n",
      "step:15 loss:0.46422532200813293\n",
      "step:16 loss:0.30614525079727173\n",
      "step:17 loss:0.339221715927124\n",
      "step:18 loss:0.3666013181209564\n",
      "step:19 loss:0.6482435464859009\n",
      "step:20 loss:0.2462795227766037\n",
      "step:21 loss:0.32416197657585144\n",
      "step:22 loss:0.21720731258392334\n",
      "step:23 loss:0.16706673800945282\n",
      "step:24 loss:0.2615239918231964\n",
      "step:25 loss:0.28459432721138\n",
      "step:26 loss:0.230296790599823\n",
      "step:27 loss:0.40070948004722595\n",
      "step:28 loss:0.21342170238494873\n",
      "step:29 loss:0.12129814177751541\n",
      "step:30 loss:0.1262180358171463\n",
      "step:31 loss:0.37255191802978516\n",
      "step:32 loss:0.4026571214199066\n",
      "step:33 loss:0.17350399494171143\n",
      "step:34 loss:0.33913543820381165\n",
      "step:35 loss:0.3951296806335449\n",
      "step:36 loss:0.18833839893341064\n",
      "step:37 loss:0.17427057027816772\n",
      "step:38 loss:0.2881090044975281\n",
      "step:39 loss:0.15287892520427704\n",
      "step:40 loss:0.367569237947464\n",
      "step:41 loss:0.16682951152324677\n",
      "step:42 loss:0.2914513051509857\n",
      "step:43 loss:0.27036938071250916\n",
      "step:44 loss:0.17784416675567627\n",
      "step:45 loss:0.2540203630924225\n",
      "step:46 loss:0.13679327070713043\n",
      "step:47 loss:0.22793792188167572\n",
      "step:48 loss:0.28060612082481384\n",
      "step:49 loss:0.2780814468860626\n",
      "step:50 loss:0.28741976618766785\n",
      "step:51 loss:0.16854971647262573\n",
      "step:52 loss:0.1242050752043724\n",
      "step:53 loss:0.10169204324483871\n",
      "step:54 loss:0.16217570006847382\n",
      "step:55 loss:0.20454560220241547\n",
      "step:56 loss:0.3401084244251251\n",
      "step:57 loss:0.1444481909275055\n",
      "step:58 loss:0.22057493031024933\n",
      "step:59 loss:0.2364659309387207\n",
      "step:60 loss:0.2717689871788025\n",
      "step:61 loss:0.32249829173088074\n",
      "step:62 loss:0.28124794363975525\n",
      "step:63 loss:0.24969126284122467\n",
      "step:64 loss:0.12353241443634033\n",
      "step:65 loss:0.17427845299243927\n",
      "step:66 loss:0.23664581775665283\n",
      "step:67 loss:0.23474621772766113\n",
      "step:68 loss:0.11641716212034225\n",
      "step:69 loss:0.14605651795864105\n",
      "step:70 loss:0.1988588571548462\n",
      "step:71 loss:0.2279655784368515\n",
      "step:72 loss:0.44931259751319885\n",
      "step:73 loss:0.18061484396457672\n",
      "step:74 loss:0.2544398605823517\n",
      "step:75 loss:0.17957662045955658\n",
      "step:76 loss:0.2811304032802582\n",
      "step:77 loss:0.1998075246810913\n",
      "step:78 loss:0.1310037225484848\n",
      "step:79 loss:0.29750099778175354\n",
      "step:80 loss:0.16679207980632782\n",
      "step:81 loss:0.3165421187877655\n",
      "step:82 loss:0.15467289090156555\n",
      "step:83 loss:0.10855597257614136\n",
      "step:84 loss:0.25646087527275085\n",
      "step:85 loss:0.2987194359302521\n",
      "step:86 loss:0.2106190323829651\n",
      "step:87 loss:0.15956903994083405\n",
      "step:88 loss:0.34535741806030273\n",
      "step:89 loss:0.1585037112236023\n",
      "step:90 loss:0.2910887897014618\n",
      "step:91 loss:0.21541239321231842\n",
      "step:92 loss:0.10955295711755753\n",
      "step:93 loss:0.2922939956188202\n",
      "step:94 loss:0.3997054994106293\n",
      "step:95 loss:0.2589034140110016\n",
      "step:96 loss:0.1838604211807251\n",
      "step:97 loss:0.37473559379577637\n",
      "step:98 loss:0.22099240124225616\n",
      "step:99 loss:0.27741867303848267\n",
      "step:100 loss:0.1801782250404358\n",
      "step:101 loss:0.3353910744190216\n",
      "step:102 loss:0.22324572503566742\n",
      "step:103 loss:0.2236223667860031\n",
      "step:104 loss:0.21409912407398224\n",
      "step:105 loss:0.23697371780872345\n",
      "step:106 loss:0.2179528921842575\n",
      "step:107 loss:0.2504746615886688\n",
      "step:108 loss:0.22287382185459137\n",
      "step:109 loss:0.1848202794790268\n",
      "step:110 loss:0.16584770381450653\n",
      "step:111 loss:0.11486083269119263\n",
      "step:112 loss:0.19646471738815308\n",
      "step:113 loss:0.06654459983110428\n",
      "step:114 loss:0.3199373781681061\n",
      "step:115 loss:0.26887428760528564\n",
      "step:116 loss:0.16568104922771454\n",
      "step:117 loss:0.626879096031189\n",
      "step:118 loss:0.21165359020233154\n",
      "step:119 loss:0.4035312235355377\n",
      "step:120 loss:0.30519312620162964\n",
      "step:121 loss:0.37818586826324463\n",
      "step:122 loss:0.23418200016021729\n",
      "step:123 loss:0.1579601913690567\n",
      "step:124 loss:0.28069406747817993\n",
      "step:125 loss:0.1698077917098999\n",
      "step:126 loss:0.21339593827724457\n",
      "step:127 loss:0.22444967925548553\n",
      "step:128 loss:0.2502249479293823\n",
      "step:129 loss:0.11929766088724136\n",
      "step:130 loss:0.34404197335243225\n",
      "step:131 loss:0.29317328333854675\n",
      "step:132 loss:0.211893692612648\n",
      "step:133 loss:0.2331743985414505\n",
      "step:134 loss:0.19171977043151855\n",
      "step:135 loss:0.4205350875854492\n",
      "step:136 loss:0.19931089878082275\n",
      "step:137 loss:0.16172386705875397\n",
      "step:138 loss:0.17732100188732147\n",
      "step:139 loss:0.3666914701461792\n",
      "step:140 loss:0.3271779716014862\n",
      "step:141 loss:0.2920656204223633\n",
      "step:142 loss:0.17824788391590118\n",
      "step:143 loss:0.24953313171863556\n",
      "step:144 loss:0.19658470153808594\n",
      "step:145 loss:0.20591022074222565\n",
      "step:146 loss:0.2925977408885956\n",
      "step:147 loss:0.20688152313232422\n",
      "step:148 loss:0.2146102786064148\n",
      "step:149 loss:0.1866011619567871\n",
      "step:150 loss:0.15702565014362335\n",
      "step:151 loss:0.24069799482822418\n",
      "step:152 loss:0.20281805098056793\n",
      "step:153 loss:0.15272410213947296\n",
      "step:154 loss:0.29607418179512024\n",
      "step:155 loss:0.1859811693429947\n",
      "step:156 loss:0.1310328096151352\n",
      "step:157 loss:0.28264135122299194\n",
      "step:158 loss:0.12055298686027527\n",
      "step:159 loss:0.20338977873325348\n",
      "step:160 loss:0.4321606159210205\n",
      "step:161 loss:0.24260644614696503\n",
      "step:162 loss:0.18614983558654785\n",
      "step:163 loss:0.13035914301872253\n",
      "step:164 loss:0.4277292788028717\n",
      "step:165 loss:0.2162654846906662\n",
      "step:166 loss:0.2890694737434387\n",
      "step:167 loss:0.19017677009105682\n",
      "step:168 loss:0.3285517990589142\n",
      "step:169 loss:0.3301449120044708\n",
      "step:170 loss:0.10964921861886978\n",
      "step:171 loss:0.19764088094234467\n",
      "step:172 loss:0.19767802953720093\n",
      "step:173 loss:0.10674538463354111\n",
      "step:174 loss:0.20631127059459686\n",
      "step:175 loss:0.2760261595249176\n",
      "step:176 loss:0.18829530477523804\n",
      "step:177 loss:0.31489241123199463\n",
      "step:178 loss:0.13598795235157013\n",
      "step:179 loss:0.3369882106781006\n",
      "step:180 loss:0.20087279379367828\n",
      "step:181 loss:0.3157017230987549\n",
      "step:182 loss:0.3919367790222168\n",
      "step:183 loss:0.20328842103481293\n",
      "step:184 loss:0.2305305004119873\n",
      "step:185 loss:0.15735632181167603\n",
      "step:186 loss:0.12633003294467926\n",
      "step:187 loss:0.29415372014045715\n",
      "step:188 loss:0.07494110614061356\n",
      "total_train_loss:0.24278525952646074, total_test_acc:0.8179407865665046\n",
      "------------------------------ \n",
      " epoch: 12\n",
      "step:0 loss:0.12988020479679108\n",
      "step:1 loss:0.17433620989322662\n",
      "step:2 loss:0.2831476628780365\n",
      "step:3 loss:0.24504341185092926\n",
      "step:4 loss:0.2873663306236267\n",
      "step:5 loss:0.1993134468793869\n",
      "step:6 loss:0.21278715133666992\n",
      "step:7 loss:0.36372315883636475\n",
      "step:8 loss:0.12630940973758698\n",
      "step:9 loss:0.09776299446821213\n",
      "step:10 loss:0.14888454973697662\n",
      "step:11 loss:0.22319965064525604\n",
      "step:12 loss:0.21465462446212769\n",
      "step:13 loss:0.2616945505142212\n",
      "step:14 loss:0.2895374596118927\n",
      "step:15 loss:0.11754199117422104\n",
      "step:16 loss:0.31451231241226196\n",
      "step:17 loss:0.17325426638126373\n",
      "step:18 loss:0.17694981396198273\n",
      "step:19 loss:0.1486249417066574\n",
      "step:20 loss:0.25819528102874756\n",
      "step:21 loss:0.13335953652858734\n",
      "step:22 loss:0.2784745395183563\n",
      "step:23 loss:0.35515204071998596\n",
      "step:24 loss:0.17086319625377655\n",
      "step:25 loss:0.09708362072706223\n",
      "step:26 loss:0.27193689346313477\n",
      "step:27 loss:0.26583540439605713\n",
      "step:28 loss:0.2778765559196472\n",
      "step:29 loss:0.22501181066036224\n",
      "step:30 loss:0.2177005261182785\n",
      "step:31 loss:0.2190060019493103\n",
      "step:32 loss:0.11723344773054123\n",
      "step:33 loss:0.4619276225566864\n",
      "step:34 loss:0.3926492929458618\n",
      "step:35 loss:0.1435132771730423\n",
      "step:36 loss:0.23571892082691193\n",
      "step:37 loss:0.17898313701152802\n",
      "step:38 loss:0.12459317594766617\n",
      "step:39 loss:0.11052917689085007\n",
      "step:40 loss:0.3646444082260132\n",
      "step:41 loss:0.14893202483654022\n",
      "step:42 loss:0.14445817470550537\n",
      "step:43 loss:0.19060367345809937\n",
      "step:44 loss:0.22937710583209991\n",
      "step:45 loss:0.17544664442539215\n",
      "step:46 loss:0.15575845539569855\n",
      "step:47 loss:0.2607887387275696\n",
      "step:48 loss:0.3000961244106293\n",
      "step:49 loss:0.20042067766189575\n",
      "step:50 loss:0.10420253127813339\n",
      "step:51 loss:0.24834729731082916\n",
      "step:52 loss:0.1477152556180954\n",
      "step:53 loss:0.19007046520709991\n",
      "step:54 loss:0.25281986594200134\n",
      "step:55 loss:0.5375791788101196\n",
      "step:56 loss:0.19476564228534698\n",
      "step:57 loss:0.25630173087120056\n",
      "step:58 loss:0.5788022875785828\n",
      "step:59 loss:0.38279989361763\n",
      "step:60 loss:0.16053177416324615\n",
      "step:61 loss:0.24623680114746094\n",
      "step:62 loss:0.10077723115682602\n",
      "step:63 loss:0.3364078998565674\n",
      "step:64 loss:0.30227091908454895\n",
      "step:65 loss:0.28392115235328674\n",
      "step:66 loss:0.20128148794174194\n",
      "step:67 loss:0.23006735742092133\n",
      "step:68 loss:0.15759830176830292\n",
      "step:69 loss:0.2743971347808838\n",
      "step:70 loss:0.17368273437023163\n",
      "step:71 loss:0.2979554533958435\n",
      "step:72 loss:0.30466794967651367\n",
      "step:73 loss:0.25551077723503113\n",
      "step:74 loss:0.24989216029644012\n",
      "step:75 loss:0.21476604044437408\n",
      "step:76 loss:0.2292681336402893\n",
      "step:77 loss:0.17574180662631989\n",
      "step:78 loss:0.22269929945468903\n",
      "step:79 loss:0.16902802884578705\n",
      "step:80 loss:0.26157212257385254\n",
      "step:81 loss:0.30158403515815735\n",
      "step:82 loss:0.2837093770503998\n",
      "step:83 loss:0.19961126148700714\n",
      "step:84 loss:0.23455850780010223\n",
      "step:85 loss:0.25698187947273254\n",
      "step:86 loss:0.40139853954315186\n",
      "step:87 loss:0.33106961846351624\n",
      "step:88 loss:0.2567112445831299\n",
      "step:89 loss:0.2706739008426666\n",
      "step:90 loss:0.2322014719247818\n",
      "step:91 loss:0.19520144164562225\n",
      "step:92 loss:0.20435410737991333\n",
      "step:93 loss:0.48419880867004395\n",
      "step:94 loss:0.3340616524219513\n",
      "step:95 loss:0.24743247032165527\n",
      "step:96 loss:0.2619217336177826\n",
      "step:97 loss:0.21107812225818634\n",
      "step:98 loss:0.13545139133930206\n",
      "step:99 loss:0.17838788032531738\n",
      "step:100 loss:0.19494391977787018\n",
      "step:101 loss:0.19904597103595734\n",
      "step:102 loss:0.42930468916893005\n",
      "step:103 loss:0.21148301661014557\n",
      "step:104 loss:0.22904826700687408\n",
      "step:105 loss:0.19469964504241943\n",
      "step:106 loss:0.20545445382595062\n",
      "step:107 loss:0.24858194589614868\n",
      "step:108 loss:0.31843462586402893\n",
      "step:109 loss:0.42097601294517517\n",
      "step:110 loss:0.2108304500579834\n",
      "step:111 loss:0.2083224654197693\n",
      "step:112 loss:0.21859407424926758\n",
      "step:113 loss:0.22051434218883514\n",
      "step:114 loss:0.1608259379863739\n",
      "step:115 loss:0.14507846534252167\n",
      "step:116 loss:0.09890863299369812\n",
      "step:117 loss:0.2703755795955658\n",
      "step:118 loss:0.19249413907527924\n",
      "step:119 loss:0.15034374594688416\n",
      "step:120 loss:0.07869715988636017\n",
      "step:121 loss:0.1459367275238037\n",
      "step:122 loss:0.135872483253479\n",
      "step:123 loss:0.3343425691127777\n",
      "step:124 loss:0.24470704793930054\n",
      "step:125 loss:0.210862398147583\n",
      "step:126 loss:0.3342437446117401\n",
      "step:127 loss:0.16400118172168732\n",
      "step:128 loss:0.2578704059123993\n",
      "step:129 loss:0.18422269821166992\n",
      "step:130 loss:0.09773758798837662\n",
      "step:131 loss:0.3222012221813202\n",
      "step:132 loss:0.14318852126598358\n",
      "step:133 loss:0.17702017724514008\n",
      "step:134 loss:0.13766716420650482\n",
      "step:135 loss:0.20322243869304657\n",
      "step:136 loss:0.20700496435165405\n",
      "step:137 loss:0.16653788089752197\n",
      "step:138 loss:0.29314032196998596\n",
      "step:139 loss:0.14350713789463043\n",
      "step:140 loss:0.2684406638145447\n",
      "step:141 loss:0.22145773470401764\n",
      "step:142 loss:0.2414776235818863\n",
      "step:143 loss:0.3092130720615387\n",
      "step:144 loss:0.2561394274234772\n",
      "step:145 loss:0.14285175502300262\n",
      "step:146 loss:0.2897934019565582\n",
      "step:147 loss:0.25204935669898987\n",
      "step:148 loss:0.18027354776859283\n",
      "step:149 loss:0.2518630623817444\n",
      "step:150 loss:0.31592828035354614\n",
      "step:151 loss:0.20407246053218842\n",
      "step:152 loss:0.20965969562530518\n",
      "step:153 loss:0.26736506819725037\n",
      "step:154 loss:0.22142668068408966\n",
      "step:155 loss:0.23335571587085724\n",
      "step:156 loss:0.19895751774311066\n",
      "step:157 loss:0.21065358817577362\n",
      "step:158 loss:0.20391930639743805\n",
      "step:159 loss:0.22688424587249756\n",
      "step:160 loss:0.17436593770980835\n",
      "step:161 loss:0.30652499198913574\n",
      "step:162 loss:0.24800850450992584\n",
      "step:163 loss:0.1317453533411026\n",
      "step:164 loss:0.16930006444454193\n",
      "step:165 loss:0.3009008467197418\n",
      "step:166 loss:0.1903628557920456\n",
      "step:167 loss:0.19738568365573883\n",
      "step:168 loss:0.15599210560321808\n",
      "step:169 loss:0.32867130637168884\n",
      "step:170 loss:0.24194079637527466\n",
      "step:171 loss:0.21413367986679077\n",
      "step:172 loss:0.17711307108402252\n",
      "step:173 loss:0.16356691718101501\n",
      "step:174 loss:0.222712442278862\n",
      "step:175 loss:0.1306498795747757\n",
      "step:176 loss:0.27450433373451233\n",
      "step:177 loss:0.2994956076145172\n",
      "step:178 loss:0.28975051641464233\n",
      "step:179 loss:0.37835443019866943\n",
      "step:180 loss:0.17795129120349884\n",
      "step:181 loss:0.29081991314888\n",
      "step:182 loss:0.37696516513824463\n",
      "step:183 loss:0.18237288296222687\n",
      "step:184 loss:0.11617275327444077\n",
      "step:185 loss:0.13670752942562103\n",
      "step:186 loss:0.13265360891819\n",
      "step:187 loss:0.32104793190956116\n",
      "step:188 loss:0.06082212179899216\n",
      "total_train_loss:0.23012754642107386, total_test_acc:0.9155987627043747\n",
      "------------------------------ \n",
      " epoch: 13\n",
      "step:0 loss:0.20866094529628754\n",
      "step:1 loss:0.24927030503749847\n",
      "step:2 loss:0.21692480146884918\n",
      "step:3 loss:0.25777170062065125\n",
      "step:4 loss:0.13596515357494354\n",
      "step:5 loss:0.16096748411655426\n",
      "step:6 loss:0.19169063866138458\n",
      "step:7 loss:0.20138327777385712\n",
      "step:8 loss:0.20809507369995117\n",
      "step:9 loss:0.0980938971042633\n",
      "step:10 loss:0.2627682685852051\n",
      "step:11 loss:0.1590316891670227\n",
      "step:12 loss:0.24238522350788116\n",
      "step:13 loss:0.16988994181156158\n",
      "step:14 loss:0.2804538905620575\n",
      "step:15 loss:0.41968321800231934\n",
      "step:16 loss:0.08895379304885864\n",
      "step:17 loss:0.2276010364294052\n",
      "step:18 loss:0.26022636890411377\n",
      "step:19 loss:0.1779228001832962\n",
      "step:20 loss:0.17963479459285736\n",
      "step:21 loss:0.16129250824451447\n",
      "step:22 loss:0.14435411989688873\n",
      "step:23 loss:0.31713294982910156\n",
      "step:24 loss:0.13861025869846344\n",
      "step:25 loss:0.20302116870880127\n",
      "step:26 loss:0.25656306743621826\n",
      "step:27 loss:0.1817084103822708\n",
      "step:28 loss:0.1744542121887207\n",
      "step:29 loss:0.18964195251464844\n",
      "step:30 loss:0.318387895822525\n",
      "step:31 loss:0.19188356399536133\n",
      "step:32 loss:0.17667414247989655\n",
      "step:33 loss:0.35650572180747986\n",
      "step:34 loss:0.3574288785457611\n",
      "step:35 loss:0.1817636489868164\n",
      "step:36 loss:0.3881644904613495\n",
      "step:37 loss:0.2395140528678894\n",
      "step:38 loss:0.3327288329601288\n",
      "step:39 loss:0.18426305055618286\n",
      "step:40 loss:0.4487255811691284\n",
      "step:41 loss:0.2285795360803604\n",
      "step:42 loss:0.22167205810546875\n",
      "step:43 loss:0.10275939851999283\n",
      "step:44 loss:0.3220408856868744\n",
      "step:45 loss:0.32387396693229675\n",
      "step:46 loss:0.237260639667511\n",
      "step:47 loss:0.18325436115264893\n",
      "step:48 loss:0.49918439984321594\n",
      "step:49 loss:0.20144939422607422\n",
      "step:50 loss:0.32950153946876526\n",
      "step:51 loss:0.21501658856868744\n",
      "step:52 loss:0.1692689210176468\n",
      "step:53 loss:0.27077046036720276\n",
      "step:54 loss:0.18933242559432983\n",
      "step:55 loss:0.34275761246681213\n",
      "step:56 loss:0.15628069639205933\n",
      "step:57 loss:0.28454700112342834\n",
      "step:58 loss:0.1891016960144043\n",
      "step:59 loss:0.28652337193489075\n",
      "step:60 loss:0.13384263217449188\n",
      "step:61 loss:0.06868110597133636\n",
      "step:62 loss:0.2260655164718628\n",
      "step:63 loss:0.15940798819065094\n",
      "step:64 loss:0.4066828191280365\n",
      "step:65 loss:0.16672785580158234\n",
      "step:66 loss:0.09285885840654373\n",
      "step:67 loss:0.11937115341424942\n",
      "step:68 loss:0.12296131253242493\n",
      "step:69 loss:0.21600651741027832\n",
      "step:70 loss:0.14051131904125214\n",
      "step:71 loss:0.2752005159854889\n",
      "step:72 loss:0.4272984266281128\n",
      "step:73 loss:0.44115543365478516\n",
      "step:74 loss:0.2482854574918747\n",
      "step:75 loss:0.10710084438323975\n",
      "step:76 loss:0.23060263693332672\n",
      "step:77 loss:0.1432497799396515\n",
      "step:78 loss:0.1784178614616394\n",
      "step:79 loss:0.16736961901187897\n",
      "step:80 loss:0.16849078238010406\n",
      "step:81 loss:0.20368681848049164\n",
      "step:82 loss:0.12586742639541626\n",
      "step:83 loss:0.16792647540569305\n",
      "step:84 loss:0.277199923992157\n",
      "step:85 loss:0.2256428450345993\n",
      "step:86 loss:0.19142965972423553\n",
      "step:87 loss:0.23594744503498077\n",
      "step:88 loss:0.19744114577770233\n",
      "step:89 loss:0.23624078929424286\n",
      "step:90 loss:0.37543821334838867\n",
      "step:91 loss:0.3431239128112793\n",
      "step:92 loss:0.1645076721906662\n",
      "step:93 loss:0.22486837208271027\n",
      "step:94 loss:0.2509962022304535\n",
      "step:95 loss:0.17097775638103485\n",
      "step:96 loss:0.2117462307214737\n",
      "step:97 loss:0.2327038198709488\n",
      "step:98 loss:0.198247492313385\n",
      "step:99 loss:0.2295502871274948\n",
      "step:100 loss:0.11856444925069809\n",
      "step:101 loss:0.2757182717323303\n",
      "step:102 loss:0.20778532326221466\n",
      "step:103 loss:0.25723567605018616\n",
      "step:104 loss:0.15972445905208588\n",
      "step:105 loss:0.20930282771587372\n",
      "step:106 loss:0.2550037205219269\n",
      "step:107 loss:0.29949507117271423\n",
      "step:108 loss:0.26827844977378845\n",
      "step:109 loss:0.25798872113227844\n",
      "step:110 loss:0.14636492729187012\n",
      "step:111 loss:0.24750465154647827\n",
      "step:112 loss:0.224176287651062\n",
      "step:113 loss:0.4009913206100464\n",
      "step:114 loss:0.1501753032207489\n",
      "step:115 loss:0.20637305080890656\n",
      "step:116 loss:0.18690568208694458\n",
      "step:117 loss:0.3704115152359009\n",
      "step:118 loss:0.17696499824523926\n",
      "step:119 loss:0.17090274393558502\n",
      "step:120 loss:0.28676918148994446\n",
      "step:121 loss:0.23521478474140167\n",
      "step:122 loss:0.19555269181728363\n",
      "step:123 loss:0.1553948074579239\n",
      "step:124 loss:0.32822081446647644\n",
      "step:125 loss:0.10588505119085312\n",
      "step:126 loss:0.29962438344955444\n",
      "step:127 loss:0.20650671422481537\n",
      "step:128 loss:0.16336458921432495\n",
      "step:129 loss:0.1815968006849289\n",
      "step:130 loss:0.2551821768283844\n",
      "step:131 loss:0.25068607926368713\n",
      "step:132 loss:0.11626020073890686\n",
      "step:133 loss:0.20333611965179443\n",
      "step:134 loss:0.36805400252342224\n",
      "step:135 loss:0.1827031821012497\n",
      "step:136 loss:0.14990876615047455\n",
      "step:137 loss:0.2050897479057312\n",
      "step:138 loss:0.13055990636348724\n",
      "step:139 loss:0.17984507977962494\n",
      "step:140 loss:0.3390982151031494\n",
      "step:141 loss:0.07730724662542343\n",
      "step:142 loss:0.10702408105134964\n",
      "step:143 loss:0.20708179473876953\n",
      "step:144 loss:0.21665769815444946\n",
      "step:145 loss:0.27556368708610535\n",
      "step:146 loss:0.17477844655513763\n",
      "step:147 loss:0.1985696703195572\n",
      "step:148 loss:0.2664099335670471\n",
      "step:149 loss:0.1954287439584732\n",
      "step:150 loss:0.14814621210098267\n",
      "step:151 loss:0.23132885992527008\n",
      "step:152 loss:0.26907414197921753\n",
      "step:153 loss:0.16332532465457916\n",
      "step:154 loss:0.34648558497428894\n",
      "step:155 loss:0.20175664126873016\n",
      "step:156 loss:0.17933113873004913\n",
      "step:157 loss:0.17044048011302948\n",
      "step:158 loss:0.12759144604206085\n",
      "step:159 loss:0.3170708417892456\n",
      "step:160 loss:0.2432870715856552\n",
      "step:161 loss:0.2221875935792923\n",
      "step:162 loss:0.2047910839319229\n",
      "step:163 loss:0.15632976591587067\n",
      "step:164 loss:0.16428963840007782\n",
      "step:165 loss:0.37573301792144775\n",
      "step:166 loss:0.2890223562717438\n",
      "step:167 loss:0.13892216980457306\n",
      "step:168 loss:0.275833398103714\n",
      "step:169 loss:0.23030094802379608\n",
      "step:170 loss:0.27448514103889465\n",
      "step:171 loss:0.2226308137178421\n",
      "step:172 loss:0.1569550484418869\n",
      "step:173 loss:0.2841898798942566\n",
      "step:174 loss:0.3210853040218353\n",
      "step:175 loss:0.06488201767206192\n",
      "step:176 loss:0.4531213045120239\n",
      "step:177 loss:0.18612362444400787\n",
      "step:178 loss:0.2406427264213562\n",
      "step:179 loss:0.376792311668396\n",
      "step:180 loss:0.3557959496974945\n",
      "step:181 loss:0.18357402086257935\n",
      "step:182 loss:0.28792259097099304\n",
      "step:183 loss:0.13220901787281036\n",
      "step:184 loss:0.20659488439559937\n",
      "step:185 loss:0.24254097044467926\n",
      "step:186 loss:0.2669507563114166\n",
      "step:187 loss:0.42197170853614807\n",
      "step:188 loss:0.5142414569854736\n",
      "total_train_loss:0.22921647860648783, total_test_acc:0.6133451171011931\n",
      "------------------------------ \n",
      " epoch: 14\n",
      "step:0 loss:0.1792738437652588\n",
      "step:1 loss:0.18769890069961548\n",
      "step:2 loss:0.3013354539871216\n",
      "step:3 loss:0.41934919357299805\n",
      "step:4 loss:0.2271350473165512\n",
      "step:5 loss:0.34518635272979736\n",
      "step:6 loss:0.10849740356206894\n",
      "step:7 loss:0.29044532775878906\n",
      "step:8 loss:0.21553604304790497\n",
      "step:9 loss:0.1725420355796814\n",
      "step:10 loss:0.20762015879154205\n",
      "step:11 loss:0.11313905566930771\n",
      "step:12 loss:0.36166083812713623\n",
      "step:13 loss:0.34409448504447937\n",
      "step:14 loss:0.2070525884628296\n",
      "step:15 loss:0.31928232312202454\n",
      "step:16 loss:0.21650882065296173\n",
      "step:17 loss:0.27951040863990784\n",
      "step:18 loss:0.17607344686985016\n",
      "step:19 loss:0.0895472839474678\n",
      "step:20 loss:0.15187104046344757\n",
      "step:21 loss:0.1899300217628479\n",
      "step:22 loss:0.29630741477012634\n",
      "step:23 loss:0.25535181164741516\n",
      "step:24 loss:0.17452795803546906\n",
      "step:25 loss:0.25921592116355896\n",
      "step:26 loss:0.24375921487808228\n",
      "step:27 loss:0.23695510625839233\n",
      "step:28 loss:0.24501828849315643\n",
      "step:29 loss:0.09963355213403702\n",
      "step:30 loss:0.12362539768218994\n",
      "step:31 loss:0.09736856818199158\n",
      "step:32 loss:0.20035777986049652\n",
      "step:33 loss:0.1074373722076416\n",
      "step:34 loss:0.3509856164455414\n",
      "step:35 loss:0.18354128301143646\n",
      "step:36 loss:0.2284257411956787\n",
      "step:37 loss:0.1929997354745865\n",
      "step:38 loss:0.1737259477376938\n",
      "step:39 loss:0.27539971470832825\n",
      "step:40 loss:0.23176686465740204\n",
      "step:41 loss:0.23051147162914276\n",
      "step:42 loss:0.23096896708011627\n",
      "step:43 loss:0.16496384143829346\n",
      "step:44 loss:0.0894472524523735\n",
      "step:45 loss:0.21509726345539093\n",
      "step:46 loss:0.2681616246700287\n",
      "step:47 loss:0.337839812040329\n",
      "step:48 loss:0.2711668610572815\n",
      "step:49 loss:0.18138273060321808\n",
      "step:50 loss:0.15461480617523193\n",
      "step:51 loss:0.16451527178287506\n",
      "step:52 loss:0.27416661381721497\n",
      "step:53 loss:0.23606614768505096\n",
      "step:54 loss:0.14985181391239166\n",
      "step:55 loss:0.3443739116191864\n",
      "step:56 loss:0.30444273352622986\n",
      "step:57 loss:0.2267046570777893\n",
      "step:58 loss:0.36963796615600586\n",
      "step:59 loss:0.2228323370218277\n",
      "step:60 loss:0.16123606264591217\n",
      "step:61 loss:0.18392176926136017\n",
      "step:62 loss:0.3564929664134979\n",
      "step:63 loss:0.31566402316093445\n",
      "step:64 loss:0.4153706729412079\n",
      "step:65 loss:0.2230304628610611\n",
      "step:66 loss:0.2345830351114273\n",
      "step:67 loss:0.2562887668609619\n",
      "step:68 loss:0.1956319957971573\n",
      "step:69 loss:0.1959800273180008\n",
      "step:70 loss:0.165289044380188\n",
      "step:71 loss:0.16663384437561035\n",
      "step:72 loss:0.19662195444107056\n",
      "step:73 loss:0.17389704287052155\n",
      "step:74 loss:0.21950960159301758\n",
      "step:75 loss:0.2175436168909073\n",
      "step:76 loss:0.29877012968063354\n",
      "step:77 loss:0.2888047993183136\n",
      "step:78 loss:0.35134831070899963\n",
      "step:79 loss:0.08748892694711685\n",
      "step:80 loss:0.2835126221179962\n",
      "step:81 loss:0.18686766922473907\n",
      "step:82 loss:0.18468062579631805\n",
      "step:83 loss:0.22750324010849\n",
      "step:84 loss:0.26871463656425476\n",
      "step:85 loss:0.2599426805973053\n",
      "step:86 loss:0.18193946778774261\n",
      "step:87 loss:0.2485022097826004\n",
      "step:88 loss:0.30899885296821594\n",
      "step:89 loss:0.15992172062397003\n",
      "step:90 loss:0.26567909121513367\n",
      "step:91 loss:0.15872500836849213\n",
      "step:92 loss:0.225632444024086\n",
      "step:93 loss:0.1621832400560379\n",
      "step:94 loss:0.18150804936885834\n",
      "step:95 loss:0.0802285447716713\n",
      "step:96 loss:0.30707988142967224\n",
      "step:97 loss:0.421660453081131\n",
      "step:98 loss:0.05522221326828003\n",
      "step:99 loss:0.16497410833835602\n",
      "step:100 loss:0.30260226130485535\n",
      "step:101 loss:0.125112846493721\n",
      "step:102 loss:0.23752598464488983\n",
      "step:103 loss:0.29404416680336\n",
      "step:104 loss:0.3090296983718872\n",
      "step:105 loss:0.2335386723279953\n",
      "step:106 loss:0.2672029435634613\n",
      "step:107 loss:0.07544942200183868\n",
      "step:108 loss:0.07599888741970062\n",
      "step:109 loss:0.20746779441833496\n",
      "step:110 loss:0.41004785895347595\n",
      "step:111 loss:0.15970589220523834\n",
      "step:112 loss:0.17403149604797363\n",
      "step:113 loss:0.2489510029554367\n",
      "step:114 loss:0.16493763029575348\n",
      "step:115 loss:0.1107720360159874\n",
      "step:116 loss:0.17385812103748322\n",
      "step:117 loss:0.2849274277687073\n",
      "step:118 loss:0.3280284106731415\n",
      "step:119 loss:0.1083318218588829\n",
      "step:120 loss:0.2847449779510498\n",
      "step:121 loss:0.17287661135196686\n",
      "step:122 loss:0.1445908397436142\n",
      "step:123 loss:0.12897555530071259\n",
      "step:124 loss:0.07295693457126617\n",
      "step:125 loss:0.2037423998117447\n",
      "step:126 loss:0.1342635601758957\n",
      "step:127 loss:0.3124299943447113\n",
      "step:128 loss:0.08076971024274826\n",
      "step:129 loss:0.3684726655483246\n",
      "step:130 loss:0.15835212171077728\n",
      "step:131 loss:0.2182394415140152\n",
      "step:132 loss:0.20654238760471344\n",
      "step:133 loss:0.13601623475551605\n",
      "step:134 loss:0.34319818019866943\n",
      "step:135 loss:0.13725312054157257\n",
      "step:136 loss:0.12355556339025497\n",
      "step:137 loss:0.20630459487438202\n",
      "step:138 loss:0.22338396310806274\n",
      "step:139 loss:0.37322798371315\n",
      "step:140 loss:0.18127329647541046\n",
      "step:141 loss:0.23287345468997955\n",
      "step:142 loss:0.2842429578304291\n",
      "step:143 loss:0.26120662689208984\n",
      "step:144 loss:0.25524112582206726\n",
      "step:145 loss:0.2240281105041504\n",
      "step:146 loss:0.1774846911430359\n",
      "step:147 loss:0.1008809432387352\n",
      "step:148 loss:0.13899892568588257\n",
      "step:149 loss:0.31721338629722595\n",
      "step:150 loss:0.21484971046447754\n",
      "step:151 loss:0.1587335616350174\n",
      "step:152 loss:0.21780072152614594\n",
      "step:153 loss:0.18469099700450897\n",
      "step:154 loss:0.22978772222995758\n",
      "step:155 loss:0.2260388880968094\n",
      "step:156 loss:0.12679806351661682\n",
      "step:157 loss:0.173631489276886\n",
      "step:158 loss:0.18909978866577148\n",
      "step:159 loss:0.18331478536128998\n",
      "step:160 loss:0.27990004420280457\n",
      "step:161 loss:0.2128574103116989\n",
      "step:162 loss:0.24079211056232452\n",
      "step:163 loss:0.205313041806221\n",
      "step:164 loss:0.2513750195503235\n",
      "step:165 loss:0.17815768718719482\n",
      "step:166 loss:0.2453157752752304\n",
      "step:167 loss:0.146802619099617\n",
      "step:168 loss:0.23213838040828705\n",
      "step:169 loss:0.2166658490896225\n",
      "step:170 loss:0.23167355358600616\n",
      "step:171 loss:0.060689330101013184\n",
      "step:172 loss:0.13996990025043488\n",
      "step:173 loss:0.13544608652591705\n",
      "step:174 loss:0.22736583650112152\n",
      "step:175 loss:0.201455757021904\n",
      "step:176 loss:0.17406092584133148\n",
      "step:177 loss:0.09175964444875717\n",
      "step:178 loss:0.11036958545446396\n",
      "step:179 loss:0.2819925546646118\n",
      "step:180 loss:0.24676835536956787\n",
      "step:181 loss:0.27006715536117554\n",
      "step:182 loss:0.251249760389328\n",
      "step:183 loss:0.14533104002475739\n",
      "step:184 loss:0.21737854182720184\n",
      "step:185 loss:0.19749192893505096\n",
      "step:186 loss:0.21049541234970093\n",
      "step:187 loss:0.1973826289176941\n",
      "step:188 loss:0.28065335750579834\n",
      "total_train_loss:0.2172052819836647, total_test_acc:0.8068935041979673\n",
      "------------------------------ \n",
      " epoch: 15\n",
      "step:0 loss:0.13232900202274323\n",
      "step:1 loss:0.14661595225334167\n",
      "step:2 loss:0.31663045287132263\n",
      "step:3 loss:0.15172305703163147\n",
      "step:4 loss:0.13048681616783142\n",
      "step:5 loss:0.23816238343715668\n",
      "step:6 loss:0.15425705909729004\n",
      "step:7 loss:0.17047102749347687\n",
      "step:8 loss:0.11229970306158066\n",
      "step:9 loss:0.141122967004776\n",
      "step:10 loss:0.1707848310470581\n",
      "step:11 loss:0.21585655212402344\n",
      "step:12 loss:0.2462330460548401\n",
      "step:13 loss:0.0722755491733551\n",
      "step:14 loss:0.11895547062158585\n",
      "step:15 loss:0.19664441049098969\n",
      "step:16 loss:0.17585688829421997\n",
      "step:17 loss:0.2795359790325165\n",
      "step:18 loss:0.1401834636926651\n",
      "step:19 loss:0.2405523657798767\n",
      "step:20 loss:0.19361919164657593\n",
      "step:21 loss:0.22675073146820068\n",
      "step:22 loss:0.19890230894088745\n",
      "step:23 loss:0.13811734318733215\n",
      "step:24 loss:0.4438141882419586\n",
      "step:25 loss:0.1776040941476822\n",
      "step:26 loss:0.12050136923789978\n",
      "step:27 loss:0.27174457907676697\n",
      "step:28 loss:0.1117437407374382\n",
      "step:29 loss:0.18448591232299805\n",
      "step:30 loss:0.18574897944927216\n",
      "step:31 loss:0.17437809705734253\n",
      "step:32 loss:0.30616748332977295\n",
      "step:33 loss:0.22768981754779816\n",
      "step:34 loss:0.2033340483903885\n",
      "step:35 loss:0.1673920899629593\n",
      "step:36 loss:0.2235937863588333\n",
      "step:37 loss:0.25732600688934326\n",
      "step:38 loss:0.25624218583106995\n",
      "step:39 loss:0.15029732882976532\n",
      "step:40 loss:0.18439804017543793\n",
      "step:41 loss:0.22132791578769684\n",
      "step:42 loss:0.2507186233997345\n",
      "step:43 loss:0.35232746601104736\n",
      "step:44 loss:0.20298248529434204\n",
      "step:45 loss:0.09460707753896713\n",
      "step:46 loss:0.056757986545562744\n",
      "step:47 loss:0.11876193434000015\n",
      "step:48 loss:0.17067748308181763\n",
      "step:49 loss:0.3811267614364624\n",
      "step:50 loss:0.18499206006526947\n",
      "step:51 loss:0.25828444957733154\n",
      "step:52 loss:0.25249356031417847\n",
      "step:53 loss:0.34475550055503845\n",
      "step:54 loss:0.2461194545030594\n",
      "step:55 loss:0.4391094148159027\n",
      "step:56 loss:0.15071840584278107\n",
      "step:57 loss:0.20399653911590576\n",
      "step:58 loss:0.20593155920505524\n",
      "step:59 loss:0.26183632016181946\n",
      "step:60 loss:0.19854332506656647\n",
      "step:61 loss:0.16446934640407562\n",
      "step:62 loss:0.18593846261501312\n",
      "step:63 loss:0.12494689971208572\n",
      "step:64 loss:0.34520983695983887\n",
      "step:65 loss:0.16127991676330566\n",
      "step:66 loss:0.31138479709625244\n",
      "step:67 loss:0.22229500114917755\n",
      "step:68 loss:0.20356197655200958\n",
      "step:69 loss:0.21862412989139557\n",
      "step:70 loss:0.21464359760284424\n",
      "step:71 loss:0.09181619435548782\n",
      "step:72 loss:0.2761039137840271\n",
      "step:73 loss:0.18350790441036224\n",
      "step:74 loss:0.19406594336032867\n",
      "step:75 loss:0.16656555235385895\n",
      "step:76 loss:0.28307607769966125\n",
      "step:77 loss:0.28689390420913696\n",
      "step:78 loss:0.29789337515830994\n",
      "step:79 loss:0.09240800887346268\n",
      "step:80 loss:0.11994686722755432\n",
      "step:81 loss:0.2399732917547226\n",
      "step:82 loss:0.11120444536209106\n",
      "step:83 loss:0.3161543905735016\n",
      "step:84 loss:0.26011115312576294\n",
      "step:85 loss:0.1937105804681778\n",
      "step:86 loss:0.2421656996011734\n",
      "step:87 loss:0.13366900384426117\n",
      "step:88 loss:0.10148719698190689\n",
      "step:89 loss:0.05989097058773041\n",
      "step:90 loss:0.16169176995754242\n",
      "step:91 loss:0.11600231379270554\n",
      "step:92 loss:0.23758091032505035\n",
      "step:93 loss:0.22952145338058472\n",
      "step:94 loss:0.27036210894584656\n",
      "step:95 loss:0.14917327463626862\n",
      "step:96 loss:0.16535498201847076\n",
      "step:97 loss:0.2966425120830536\n",
      "step:98 loss:0.23830753564834595\n",
      "step:99 loss:0.22111661732196808\n",
      "step:100 loss:0.16344806551933289\n",
      "step:101 loss:0.11838185787200928\n",
      "step:102 loss:0.27088671922683716\n",
      "step:103 loss:0.06516581773757935\n",
      "step:104 loss:0.26639536023139954\n",
      "step:105 loss:0.27469077706336975\n",
      "step:106 loss:0.23133164644241333\n",
      "step:107 loss:0.30573728680610657\n",
      "step:108 loss:0.15873105823993683\n",
      "step:109 loss:0.12406858801841736\n",
      "step:110 loss:0.21174192428588867\n",
      "step:111 loss:0.19576793909072876\n",
      "step:112 loss:0.1408979594707489\n",
      "step:113 loss:0.16245894134044647\n",
      "step:114 loss:0.1696522980928421\n",
      "step:115 loss:0.2651033401489258\n",
      "step:116 loss:0.14885550737380981\n",
      "step:117 loss:0.2761323153972626\n",
      "step:118 loss:0.2284703403711319\n",
      "step:119 loss:0.18318744003772736\n",
      "step:120 loss:0.2019905298948288\n",
      "step:121 loss:0.1843165159225464\n",
      "step:122 loss:0.17137672007083893\n",
      "step:123 loss:0.13678933680057526\n",
      "step:124 loss:0.18731115758419037\n",
      "step:125 loss:0.22959645092487335\n",
      "step:126 loss:0.16040129959583282\n",
      "step:127 loss:0.17951111495494843\n",
      "step:128 loss:0.326305627822876\n",
      "step:129 loss:0.21238793432712555\n",
      "step:130 loss:0.1307499259710312\n",
      "step:131 loss:0.18815374374389648\n",
      "step:132 loss:0.2588336765766144\n",
      "step:133 loss:0.15658652782440186\n",
      "step:134 loss:0.27251186966896057\n",
      "step:135 loss:0.29013437032699585\n",
      "step:136 loss:0.2032533437013626\n",
      "step:137 loss:0.1582237035036087\n",
      "step:138 loss:0.09021207690238953\n",
      "step:139 loss:0.18498684465885162\n",
      "step:140 loss:0.3599769175052643\n",
      "step:141 loss:0.17542177438735962\n",
      "step:142 loss:0.22261105477809906\n",
      "step:143 loss:0.19679181277751923\n",
      "step:144 loss:0.29666921496391296\n",
      "step:145 loss:0.16506166756153107\n",
      "step:146 loss:0.30674979090690613\n",
      "step:147 loss:0.21038345992565155\n",
      "step:148 loss:0.17038382589817047\n",
      "step:149 loss:0.2312670797109604\n",
      "step:150 loss:0.30784139037132263\n",
      "step:151 loss:0.3601055145263672\n",
      "step:152 loss:0.20810270309448242\n",
      "step:153 loss:0.22124171257019043\n",
      "step:154 loss:0.3269639015197754\n",
      "step:155 loss:0.1284855306148529\n",
      "step:156 loss:0.2333511859178543\n",
      "step:157 loss:0.18911480903625488\n",
      "step:158 loss:0.2635081112384796\n",
      "step:159 loss:0.1904146522283554\n",
      "step:160 loss:0.2861805260181427\n",
      "step:161 loss:0.3054521977901459\n",
      "step:162 loss:0.1931471824645996\n",
      "step:163 loss:0.21177834272384644\n",
      "step:164 loss:0.16117450594902039\n",
      "step:165 loss:0.222239151597023\n",
      "step:166 loss:0.10871212929487228\n",
      "step:167 loss:0.10552401095628738\n",
      "step:168 loss:0.23629146814346313\n",
      "step:169 loss:0.24059931933879852\n",
      "step:170 loss:0.17452800273895264\n",
      "step:171 loss:0.25830724835395813\n",
      "step:172 loss:0.41818392276763916\n",
      "step:173 loss:0.38995590806007385\n",
      "step:174 loss:0.23872269690036774\n",
      "step:175 loss:0.1696716994047165\n",
      "step:176 loss:0.14912335574626923\n",
      "step:177 loss:0.13176079094409943\n",
      "step:178 loss:0.2834024429321289\n",
      "step:179 loss:0.13295771181583405\n",
      "step:180 loss:0.2554432451725006\n",
      "step:181 loss:0.2086334228515625\n",
      "step:182 loss:0.2019285410642624\n",
      "step:183 loss:0.18347598612308502\n",
      "step:184 loss:0.11556944251060486\n",
      "step:185 loss:0.27469146251678467\n",
      "step:186 loss:0.3760794401168823\n",
      "step:187 loss:0.18753033876419067\n",
      "step:188 loss:0.12584809958934784\n",
      "total_train_loss:0.20976460209869324, total_test_acc:0.929297392841361\n",
      "------------------------------ \n",
      " epoch: 16\n",
      "step:0 loss:0.1390097290277481\n",
      "step:1 loss:0.04163515195250511\n",
      "step:2 loss:0.10255411267280579\n",
      "step:3 loss:0.3351851999759674\n",
      "step:4 loss:0.3485772907733917\n",
      "step:5 loss:0.0787813737988472\n",
      "step:6 loss:0.16590599715709686\n",
      "step:7 loss:0.16603203117847443\n",
      "step:8 loss:0.28691715002059937\n",
      "step:9 loss:0.2835778295993805\n",
      "step:10 loss:0.2008875161409378\n",
      "step:11 loss:0.15060456097126007\n",
      "step:12 loss:0.0797729641199112\n",
      "step:13 loss:0.14707089960575104\n",
      "step:14 loss:0.26524147391319275\n",
      "step:15 loss:0.22246016561985016\n",
      "step:16 loss:0.3202563226222992\n",
      "step:17 loss:0.29254332184791565\n",
      "step:18 loss:0.38075029850006104\n",
      "step:19 loss:0.2513505518436432\n",
      "step:20 loss:0.2602950930595398\n",
      "step:21 loss:0.35462328791618347\n",
      "step:22 loss:0.14319871366024017\n",
      "step:23 loss:0.18612627685070038\n",
      "step:24 loss:0.23367361724376678\n",
      "step:25 loss:0.16864867508411407\n",
      "step:26 loss:0.2705174386501312\n",
      "step:27 loss:0.2497616559267044\n",
      "step:28 loss:0.3115294575691223\n",
      "step:29 loss:0.09977396577596664\n",
      "step:30 loss:0.36789488792419434\n",
      "step:31 loss:0.1383456587791443\n",
      "step:32 loss:0.22716571390628815\n",
      "step:33 loss:0.20528127253055573\n",
      "step:34 loss:0.10693114250898361\n",
      "step:35 loss:0.18906860053539276\n",
      "step:36 loss:0.18735413253307343\n",
      "step:37 loss:0.24158506095409393\n",
      "step:38 loss:0.22020763158798218\n",
      "step:39 loss:0.2064073234796524\n",
      "step:40 loss:0.18089498579502106\n",
      "step:41 loss:0.26538875699043274\n",
      "step:42 loss:0.1521766632795334\n",
      "step:43 loss:0.2376956194639206\n",
      "step:44 loss:0.1629800647497177\n",
      "step:45 loss:0.18974502384662628\n",
      "step:46 loss:0.3442500829696655\n",
      "step:47 loss:0.20255516469478607\n",
      "step:48 loss:0.06629012525081635\n",
      "step:49 loss:0.1821235567331314\n",
      "step:50 loss:0.22089451551437378\n",
      "step:51 loss:0.2123686671257019\n",
      "step:52 loss:0.27848929166793823\n",
      "step:53 loss:0.056662607938051224\n",
      "step:54 loss:0.11661138385534286\n",
      "step:55 loss:0.14674784243106842\n",
      "step:56 loss:0.21208274364471436\n",
      "step:57 loss:0.3858717381954193\n",
      "step:58 loss:0.33342835307121277\n",
      "step:59 loss:0.23127473890781403\n",
      "step:60 loss:0.2992309629917145\n",
      "step:61 loss:0.16159681975841522\n",
      "step:62 loss:0.2364264875650406\n",
      "step:63 loss:0.16798608005046844\n",
      "step:64 loss:0.11960089206695557\n",
      "step:65 loss:0.18567043542861938\n",
      "step:66 loss:0.09399067610502243\n",
      "step:67 loss:0.2829236090183258\n",
      "step:68 loss:0.21354837715625763\n",
      "step:69 loss:0.07832150906324387\n",
      "step:70 loss:0.1492307335138321\n",
      "step:71 loss:0.15962980687618256\n",
      "step:72 loss:0.1776779443025589\n",
      "step:73 loss:0.12870894372463226\n",
      "step:74 loss:0.17978216707706451\n",
      "step:75 loss:0.1179574728012085\n",
      "step:76 loss:0.14438776671886444\n",
      "step:77 loss:0.12175199389457703\n",
      "step:78 loss:0.15805275738239288\n",
      "step:79 loss:0.20577116310596466\n",
      "step:80 loss:0.13332265615463257\n",
      "step:81 loss:0.08940564841032028\n",
      "step:82 loss:0.277053564786911\n",
      "step:83 loss:0.3835723400115967\n",
      "step:84 loss:0.28818145394325256\n",
      "step:85 loss:0.24401630461215973\n",
      "step:86 loss:0.15543599426746368\n",
      "step:87 loss:0.11111816763877869\n",
      "step:88 loss:0.14343909919261932\n",
      "step:89 loss:0.10587119311094284\n",
      "step:90 loss:0.13499760627746582\n",
      "step:91 loss:0.13342954218387604\n",
      "step:92 loss:0.13972705602645874\n",
      "step:93 loss:0.15970033407211304\n",
      "step:94 loss:0.19269388914108276\n",
      "step:95 loss:0.3954629898071289\n",
      "step:96 loss:0.33415189385414124\n",
      "step:97 loss:0.10406208038330078\n",
      "step:98 loss:0.17792676389217377\n",
      "step:99 loss:0.1525387167930603\n",
      "step:100 loss:0.18229927122592926\n",
      "step:101 loss:0.16225241124629974\n",
      "step:102 loss:0.2564271092414856\n",
      "step:103 loss:0.08140530437231064\n",
      "step:104 loss:0.37109220027923584\n",
      "step:105 loss:0.07099542766809464\n",
      "step:106 loss:0.20722214877605438\n",
      "step:107 loss:0.15775205194950104\n",
      "step:108 loss:0.3186591863632202\n",
      "step:109 loss:0.23207396268844604\n",
      "step:110 loss:0.28637003898620605\n",
      "step:111 loss:0.21934843063354492\n",
      "step:112 loss:0.30175766348838806\n",
      "step:113 loss:0.22325946390628815\n",
      "step:114 loss:0.26109108328819275\n",
      "step:115 loss:0.0869607999920845\n",
      "step:116 loss:0.20408986508846283\n",
      "step:117 loss:0.19026625156402588\n",
      "step:118 loss:0.38402247428894043\n",
      "step:119 loss:0.16226688027381897\n",
      "step:120 loss:0.13526591658592224\n",
      "step:121 loss:0.14191529154777527\n",
      "step:122 loss:0.1407560110092163\n",
      "step:123 loss:0.3005449175834656\n",
      "step:124 loss:0.06633079797029495\n",
      "step:125 loss:0.13535083830356598\n",
      "step:126 loss:0.11190535873174667\n",
      "step:127 loss:0.07836443185806274\n",
      "step:128 loss:0.16860538721084595\n",
      "step:129 loss:0.1164819598197937\n",
      "step:130 loss:0.12353209406137466\n",
      "step:131 loss:0.13730916380882263\n",
      "step:132 loss:0.21395058929920197\n",
      "step:133 loss:0.11383039504289627\n",
      "step:134 loss:0.2541992962360382\n",
      "step:135 loss:0.16036073863506317\n",
      "step:136 loss:0.27661266922950745\n",
      "step:137 loss:0.21885786950588226\n",
      "step:138 loss:0.12062433362007141\n",
      "step:139 loss:0.4545474350452423\n",
      "step:140 loss:0.22098064422607422\n",
      "step:141 loss:0.16593821346759796\n",
      "step:142 loss:0.10336795449256897\n",
      "step:143 loss:0.0570969395339489\n",
      "step:144 loss:0.1303638219833374\n",
      "step:145 loss:0.10662395507097244\n",
      "step:146 loss:0.25876662135124207\n",
      "step:147 loss:0.19116760790348053\n",
      "step:148 loss:0.27574026584625244\n",
      "step:149 loss:0.18572603166103363\n",
      "step:150 loss:0.16565845906734467\n",
      "step:151 loss:0.24047493934631348\n",
      "step:152 loss:0.2702343761920929\n",
      "step:153 loss:0.2018623799085617\n",
      "step:154 loss:0.19950245320796967\n",
      "step:155 loss:0.2204541712999344\n",
      "step:156 loss:0.20157641172409058\n",
      "step:157 loss:0.3014139235019684\n",
      "step:158 loss:0.3014082908630371\n",
      "step:159 loss:0.13642287254333496\n",
      "step:160 loss:0.1128602996468544\n",
      "step:161 loss:0.184475839138031\n",
      "step:162 loss:0.2793613076210022\n",
      "step:163 loss:0.2020704597234726\n",
      "step:164 loss:0.210591658949852\n",
      "step:165 loss:0.0965212807059288\n",
      "step:166 loss:0.10199233144521713\n",
      "step:167 loss:0.26254042983055115\n",
      "step:168 loss:0.24608926475048065\n",
      "step:169 loss:0.12698403000831604\n",
      "step:170 loss:0.2516351044178009\n",
      "step:171 loss:0.15590399503707886\n",
      "step:172 loss:0.15269239246845245\n",
      "step:173 loss:0.16009899973869324\n",
      "step:174 loss:0.26005634665489197\n",
      "step:175 loss:0.2135343998670578\n",
      "step:176 loss:0.09695461392402649\n",
      "step:177 loss:0.18505997955799103\n",
      "step:178 loss:0.11778596043586731\n",
      "step:179 loss:0.14683227241039276\n",
      "step:180 loss:0.12217667698860168\n",
      "step:181 loss:0.12151649594306946\n",
      "step:182 loss:0.13060666620731354\n",
      "step:183 loss:0.12559866905212402\n",
      "step:184 loss:0.2549727261066437\n",
      "step:185 loss:0.2500690221786499\n",
      "step:186 loss:0.14995263516902924\n",
      "step:187 loss:0.12019050121307373\n",
      "step:188 loss:0.21242669224739075\n",
      "total_train_loss:0.19606211846892505, total_test_acc:0.9186920017675652\n",
      "------------------------------ \n",
      " epoch: 17\n",
      "step:0 loss:0.3381178677082062\n",
      "step:1 loss:0.0392594151198864\n",
      "step:2 loss:0.21006439626216888\n",
      "step:3 loss:0.1915733814239502\n",
      "step:4 loss:0.18893593549728394\n",
      "step:5 loss:0.10241568088531494\n",
      "step:6 loss:0.22524791955947876\n",
      "step:7 loss:0.19805465638637543\n",
      "step:8 loss:0.18541936576366425\n",
      "step:9 loss:0.24794387817382812\n",
      "step:10 loss:0.0741405263543129\n",
      "step:11 loss:0.2856319844722748\n",
      "step:12 loss:0.4517211616039276\n",
      "step:13 loss:0.2520901560783386\n",
      "step:14 loss:0.15433168411254883\n",
      "step:15 loss:0.11881759762763977\n",
      "step:16 loss:0.07930011302232742\n",
      "step:17 loss:0.14065396785736084\n",
      "step:18 loss:0.3082190752029419\n",
      "step:19 loss:0.09437409788370132\n",
      "step:20 loss:0.17027795314788818\n",
      "step:21 loss:0.27246490120887756\n",
      "step:22 loss:0.21604835987091064\n",
      "step:23 loss:0.16233685612678528\n",
      "step:24 loss:0.22713477909564972\n",
      "step:25 loss:0.1644018143415451\n",
      "step:26 loss:0.43176665902137756\n",
      "step:27 loss:0.11070948839187622\n",
      "step:28 loss:0.2599230706691742\n",
      "step:29 loss:0.2664482295513153\n",
      "step:30 loss:0.3180111348628998\n",
      "step:31 loss:0.1322510689496994\n",
      "step:32 loss:0.23075681924819946\n",
      "step:33 loss:0.1566966325044632\n",
      "step:34 loss:0.38345181941986084\n",
      "step:35 loss:0.14502041041851044\n",
      "step:36 loss:0.21939140558242798\n",
      "step:37 loss:0.25299492478370667\n",
      "step:38 loss:0.14821894466876984\n",
      "step:39 loss:0.25348061323165894\n",
      "step:40 loss:0.11740172654390335\n",
      "step:41 loss:0.3336792290210724\n",
      "step:42 loss:0.12376489490270615\n",
      "step:43 loss:0.15263433754444122\n",
      "step:44 loss:0.4642122685909271\n",
      "step:45 loss:0.23580403625965118\n",
      "step:46 loss:0.31049564480781555\n",
      "step:47 loss:0.22721953690052032\n",
      "step:48 loss:0.09149844199419022\n",
      "step:49 loss:0.21862947940826416\n",
      "step:50 loss:0.27783551812171936\n",
      "step:51 loss:0.19241325557231903\n",
      "step:52 loss:0.2476779669523239\n",
      "step:53 loss:0.1402575820684433\n",
      "step:54 loss:0.08861730247735977\n",
      "step:55 loss:0.07907431572675705\n",
      "step:56 loss:0.18250185251235962\n",
      "step:57 loss:0.10579878836870193\n",
      "step:58 loss:0.15563228726387024\n",
      "step:59 loss:0.09120815992355347\n",
      "step:60 loss:0.2288927435874939\n",
      "step:61 loss:0.13624833524227142\n",
      "step:62 loss:0.2816965579986572\n",
      "step:63 loss:0.15107150375843048\n",
      "step:64 loss:0.13837380707263947\n",
      "step:65 loss:0.18095624446868896\n",
      "step:66 loss:0.21757221221923828\n",
      "step:67 loss:0.24478328227996826\n",
      "step:68 loss:0.17228472232818604\n",
      "step:69 loss:0.2525918781757355\n",
      "step:70 loss:0.1252184510231018\n",
      "step:71 loss:0.22125057876110077\n",
      "step:72 loss:0.08404079079627991\n",
      "step:73 loss:0.1662837713956833\n",
      "step:74 loss:0.17527015507221222\n",
      "step:75 loss:0.15784548223018646\n",
      "step:76 loss:0.16613470017910004\n",
      "step:77 loss:0.23220272362232208\n",
      "step:78 loss:0.16286268830299377\n",
      "step:79 loss:0.2401682287454605\n",
      "step:80 loss:0.26344987750053406\n",
      "step:81 loss:0.4602018892765045\n",
      "step:82 loss:0.1223452165722847\n",
      "step:83 loss:0.09671785682439804\n",
      "step:84 loss:0.22975169122219086\n",
      "step:85 loss:0.19270968437194824\n",
      "step:86 loss:0.2735646367073059\n",
      "step:87 loss:0.3034937381744385\n",
      "step:88 loss:0.12607386708259583\n",
      "step:89 loss:0.09996244311332703\n",
      "step:90 loss:0.23376993834972382\n",
      "step:91 loss:0.13689164817333221\n",
      "step:92 loss:0.19280602037906647\n",
      "step:93 loss:0.1987195760011673\n",
      "step:94 loss:0.1947694569826126\n",
      "step:95 loss:0.10869506001472473\n",
      "step:96 loss:0.3832167088985443\n",
      "step:97 loss:0.14107511937618256\n",
      "step:98 loss:0.30607572197914124\n",
      "step:99 loss:0.23429684340953827\n",
      "step:100 loss:0.13620664179325104\n",
      "step:101 loss:0.28469642996788025\n",
      "step:102 loss:0.13590307533740997\n",
      "step:103 loss:0.1009034737944603\n",
      "step:104 loss:0.23350591957569122\n",
      "step:105 loss:0.29994937777519226\n",
      "step:106 loss:0.06902105361223221\n",
      "step:107 loss:0.24377381801605225\n",
      "step:108 loss:0.23870408535003662\n",
      "step:109 loss:0.11609885096549988\n",
      "step:110 loss:0.2892880141735077\n",
      "step:111 loss:0.22622458636760712\n",
      "step:112 loss:0.30431750416755676\n",
      "step:113 loss:0.2179238200187683\n",
      "step:114 loss:0.1892908364534378\n",
      "step:115 loss:0.17696519196033478\n",
      "step:116 loss:0.19601909816265106\n",
      "step:117 loss:0.08991160988807678\n",
      "step:118 loss:0.19096451997756958\n",
      "step:119 loss:0.20697350800037384\n",
      "step:120 loss:0.13760820031166077\n",
      "step:121 loss:0.0968492329120636\n",
      "step:122 loss:0.2874777615070343\n",
      "step:123 loss:0.24505208432674408\n",
      "step:124 loss:0.0908975824713707\n",
      "step:125 loss:0.09317681938409805\n",
      "step:126 loss:0.10305697470903397\n",
      "step:127 loss:0.28352561593055725\n",
      "step:128 loss:0.16059918701648712\n",
      "step:129 loss:0.25667670369148254\n",
      "step:130 loss:0.12811948359012604\n",
      "step:131 loss:0.17667411267757416\n",
      "step:132 loss:0.11647825688123703\n",
      "step:133 loss:0.20128673315048218\n",
      "step:134 loss:0.21022796630859375\n",
      "step:135 loss:0.17179834842681885\n",
      "step:136 loss:0.23619568347930908\n",
      "step:137 loss:0.27532491087913513\n",
      "step:138 loss:0.42274460196495056\n",
      "step:139 loss:0.08635526895523071\n",
      "step:140 loss:0.23384881019592285\n",
      "step:141 loss:0.2090584635734558\n",
      "step:142 loss:0.09713166207075119\n",
      "step:143 loss:0.25163307785987854\n",
      "step:144 loss:0.20854467153549194\n",
      "step:145 loss:0.10785654187202454\n",
      "step:146 loss:0.2843281328678131\n",
      "step:147 loss:0.2504836320877075\n",
      "step:148 loss:0.11989178508520126\n",
      "step:149 loss:0.08812073618173599\n",
      "step:150 loss:0.11696439981460571\n",
      "step:151 loss:0.2340955138206482\n",
      "step:152 loss:0.09440866112709045\n",
      "step:153 loss:0.3002835214138031\n",
      "step:154 loss:0.23577792942523956\n",
      "step:155 loss:0.08634445071220398\n",
      "step:156 loss:0.1207105740904808\n",
      "step:157 loss:0.24319905042648315\n",
      "step:158 loss:0.40850046277046204\n",
      "step:159 loss:0.2992987632751465\n",
      "step:160 loss:0.20418059825897217\n",
      "step:161 loss:0.1320682168006897\n",
      "step:162 loss:0.17177999019622803\n",
      "step:163 loss:0.17765553295612335\n",
      "step:164 loss:0.15946802496910095\n",
      "step:165 loss:0.24932892620563507\n",
      "step:166 loss:0.11483266204595566\n",
      "step:167 loss:0.11337766796350479\n",
      "step:168 loss:0.11655846983194351\n",
      "step:169 loss:0.13318632543087006\n",
      "step:170 loss:0.08129862695932388\n",
      "step:171 loss:0.21082477271556854\n",
      "step:172 loss:0.1086273118853569\n",
      "step:173 loss:0.19224511086940765\n",
      "step:174 loss:0.31627970933914185\n",
      "step:175 loss:0.17150135338306427\n",
      "step:176 loss:0.18194735050201416\n",
      "step:177 loss:0.18705518543720245\n",
      "step:178 loss:0.14098717272281647\n",
      "step:179 loss:0.3051806390285492\n",
      "step:180 loss:0.11578639596700668\n",
      "step:181 loss:0.21076564490795135\n",
      "step:182 loss:0.11941968649625778\n",
      "step:183 loss:0.09502292424440384\n",
      "step:184 loss:0.12596800923347473\n",
      "step:185 loss:0.1817195564508438\n",
      "step:186 loss:0.07474343478679657\n",
      "step:187 loss:0.2507700026035309\n",
      "step:188 loss:0.1325702667236328\n",
      "total_train_loss:0.19559142859454484, total_test_acc:0.8753866548828988\n",
      "------------------------------ \n",
      " epoch: 18\n",
      "step:0 loss:0.19384710490703583\n",
      "step:1 loss:0.1037527397274971\n",
      "step:2 loss:0.15681277215480804\n",
      "step:3 loss:0.23850871622562408\n",
      "step:4 loss:0.19513516128063202\n",
      "step:5 loss:0.12165132164955139\n",
      "step:6 loss:0.15602083504199982\n",
      "step:7 loss:0.05020417645573616\n",
      "step:8 loss:0.14307542145252228\n",
      "step:9 loss:0.0918322503566742\n",
      "step:10 loss:0.1708129644393921\n",
      "step:11 loss:0.09990760684013367\n",
      "step:12 loss:0.1399637907743454\n",
      "step:13 loss:0.19647948443889618\n",
      "step:14 loss:0.12632541358470917\n",
      "step:15 loss:0.15534017980098724\n",
      "step:16 loss:0.2605462968349457\n",
      "step:17 loss:0.2516465485095978\n",
      "step:18 loss:0.2665645182132721\n",
      "step:19 loss:0.11122632026672363\n",
      "step:20 loss:0.10572940856218338\n",
      "step:21 loss:0.0941494032740593\n",
      "step:22 loss:0.07224196940660477\n",
      "step:23 loss:0.2359648495912552\n",
      "step:24 loss:0.3652716875076294\n",
      "step:25 loss:0.07038285583257675\n",
      "step:26 loss:0.11023744195699692\n",
      "step:27 loss:0.046120867133140564\n",
      "step:28 loss:0.1839727908372879\n",
      "step:29 loss:0.12691335380077362\n",
      "step:30 loss:0.19622425734996796\n",
      "step:31 loss:0.14877253770828247\n",
      "step:32 loss:0.2582360506057739\n",
      "step:33 loss:0.06678106635808945\n",
      "step:34 loss:0.11701679974794388\n",
      "step:35 loss:0.26164522767066956\n",
      "step:36 loss:0.5270706415176392\n",
      "step:37 loss:0.08416534215211868\n",
      "step:38 loss:0.13241954147815704\n",
      "step:39 loss:0.11877819895744324\n",
      "step:40 loss:0.2094435840845108\n",
      "step:41 loss:0.2286679893732071\n",
      "step:42 loss:0.2347778081893921\n",
      "step:43 loss:0.20134246349334717\n",
      "step:44 loss:0.20547740161418915\n",
      "step:45 loss:0.29073742032051086\n",
      "step:46 loss:0.27052628993988037\n",
      "step:47 loss:0.22343307733535767\n",
      "step:48 loss:0.22302423417568207\n",
      "step:49 loss:0.23111213743686676\n",
      "step:50 loss:0.14979426562786102\n",
      "step:51 loss:0.22758470475673676\n",
      "step:52 loss:0.22278422117233276\n",
      "step:53 loss:0.19873744249343872\n",
      "step:54 loss:0.10523274540901184\n",
      "step:55 loss:0.20334775745868683\n",
      "step:56 loss:0.2503407895565033\n",
      "step:57 loss:0.20484532415866852\n",
      "step:58 loss:0.18866370618343353\n",
      "step:59 loss:0.1525464951992035\n",
      "step:60 loss:0.1793959140777588\n",
      "step:61 loss:0.08479159325361252\n",
      "step:62 loss:0.17385976016521454\n",
      "step:63 loss:0.1661468893289566\n",
      "step:64 loss:0.12796251475811005\n",
      "step:65 loss:0.26394370198249817\n",
      "step:66 loss:0.24577146768569946\n",
      "step:67 loss:0.13278810679912567\n",
      "step:68 loss:0.24079568684101105\n",
      "step:69 loss:0.15238328278064728\n",
      "step:70 loss:0.09577608853578568\n",
      "step:71 loss:0.15554304420948029\n",
      "step:72 loss:0.17827807366847992\n",
      "step:73 loss:0.3507011830806732\n",
      "step:74 loss:0.10136982798576355\n",
      "step:75 loss:0.16169781982898712\n",
      "step:76 loss:0.08759516477584839\n",
      "step:77 loss:0.08485325425863266\n",
      "step:78 loss:0.1947600394487381\n",
      "step:79 loss:0.19444376230239868\n",
      "step:80 loss:0.11551260948181152\n",
      "step:81 loss:0.30476298928260803\n",
      "step:82 loss:0.2364806979894638\n",
      "step:83 loss:0.28986456990242004\n",
      "step:84 loss:0.1952301263809204\n",
      "step:85 loss:0.2366185188293457\n",
      "step:86 loss:0.14101721346378326\n",
      "step:87 loss:0.10856496542692184\n",
      "step:88 loss:0.14814327657222748\n",
      "step:89 loss:0.1387404054403305\n",
      "step:90 loss:0.30637839436531067\n",
      "step:91 loss:0.21014411747455597\n",
      "step:92 loss:0.33431199193000793\n",
      "step:93 loss:0.08422520756721497\n",
      "step:94 loss:0.18776418268680573\n",
      "step:95 loss:0.14392998814582825\n",
      "step:96 loss:0.1267237663269043\n",
      "step:97 loss:0.18696357309818268\n",
      "step:98 loss:0.13051582872867584\n",
      "step:99 loss:0.35777974128723145\n",
      "step:100 loss:0.17781104147434235\n",
      "step:101 loss:0.1301395148038864\n",
      "step:102 loss:0.17317260801792145\n",
      "step:103 loss:0.19390220940113068\n",
      "step:104 loss:0.22390402853488922\n",
      "step:105 loss:0.13235758244991302\n",
      "step:106 loss:0.20971821248531342\n",
      "step:107 loss:0.06685802340507507\n",
      "step:108 loss:0.3521043360233307\n",
      "step:109 loss:0.18135780096054077\n",
      "step:110 loss:0.3375553786754608\n",
      "step:111 loss:0.2208305150270462\n",
      "step:112 loss:0.1952207088470459\n",
      "step:113 loss:0.08527946472167969\n",
      "step:114 loss:0.1518622487783432\n",
      "step:115 loss:0.2416893094778061\n",
      "step:116 loss:0.12490668147802353\n",
      "step:117 loss:0.08332698792219162\n",
      "step:118 loss:0.09689845889806747\n",
      "step:119 loss:0.216202512383461\n",
      "step:120 loss:0.2590794265270233\n",
      "step:121 loss:0.0761416107416153\n",
      "step:122 loss:0.14915607869625092\n",
      "step:123 loss:0.1915295571088791\n",
      "step:124 loss:0.14972352981567383\n",
      "step:125 loss:0.20127002894878387\n",
      "step:126 loss:0.18479834496974945\n",
      "step:127 loss:0.27816101908683777\n",
      "step:128 loss:0.35506772994995117\n",
      "step:129 loss:0.27340152859687805\n",
      "step:130 loss:0.2457510232925415\n",
      "step:131 loss:0.14575466513633728\n",
      "step:132 loss:0.12893430888652802\n",
      "step:133 loss:0.06398386508226395\n",
      "step:134 loss:0.16951526701450348\n",
      "step:135 loss:0.0906948447227478\n",
      "step:136 loss:0.21544790267944336\n",
      "step:137 loss:0.23008637130260468\n",
      "step:138 loss:0.10977059602737427\n",
      "step:139 loss:0.2718040943145752\n",
      "step:140 loss:0.27649086713790894\n",
      "step:141 loss:0.29225966334342957\n",
      "step:142 loss:0.13544006645679474\n",
      "step:143 loss:0.20344184339046478\n",
      "step:144 loss:0.3058263659477234\n",
      "step:145 loss:0.18593460321426392\n",
      "step:146 loss:0.28498730063438416\n",
      "step:147 loss:0.2735990583896637\n",
      "step:148 loss:0.1443982571363449\n",
      "step:149 loss:0.14570759236812592\n",
      "step:150 loss:0.13352397084236145\n",
      "step:151 loss:0.2763286530971527\n",
      "step:152 loss:0.18516872823238373\n",
      "step:153 loss:0.1000383198261261\n",
      "step:154 loss:0.25767436623573303\n",
      "step:155 loss:0.15090520679950714\n",
      "step:156 loss:0.1438894122838974\n",
      "step:157 loss:0.25253620743751526\n",
      "step:158 loss:0.12206080555915833\n",
      "step:159 loss:0.10421816259622574\n",
      "step:160 loss:0.1913180947303772\n",
      "step:161 loss:0.07488150894641876\n",
      "step:162 loss:0.1622992306947708\n",
      "step:163 loss:0.11712267249822617\n",
      "step:164 loss:0.17297159135341644\n",
      "step:165 loss:0.28439465165138245\n",
      "step:166 loss:0.2564155161380768\n",
      "step:167 loss:0.1469465047121048\n",
      "step:168 loss:0.37068602442741394\n",
      "step:169 loss:0.15532232820987701\n",
      "step:170 loss:0.1885298490524292\n",
      "step:171 loss:0.06867405027151108\n",
      "step:172 loss:0.1958167403936386\n",
      "step:173 loss:0.15988324582576752\n",
      "step:174 loss:0.17144131660461426\n",
      "step:175 loss:0.09385057538747787\n",
      "step:176 loss:0.39519014954566956\n",
      "step:177 loss:0.18154317140579224\n",
      "step:178 loss:0.3050268292427063\n",
      "step:179 loss:0.14965792000293732\n",
      "step:180 loss:0.16283075511455536\n",
      "step:181 loss:0.2564699649810791\n",
      "step:182 loss:0.19466497004032135\n",
      "step:183 loss:0.31627556681632996\n",
      "step:184 loss:0.11999908089637756\n",
      "step:185 loss:0.25971847772598267\n",
      "step:186 loss:0.27729499340057373\n",
      "step:187 loss:0.19561916589736938\n",
      "step:188 loss:0.2937743067741394\n",
      "total_train_loss:0.18834529379184575, total_test_acc:0.9133893062306673\n",
      "------------------------------ \n",
      " epoch: 19\n",
      "step:0 loss:0.12277791649103165\n",
      "step:1 loss:0.32518211007118225\n",
      "step:2 loss:0.08462164551019669\n",
      "step:3 loss:0.07985759526491165\n",
      "step:4 loss:0.08555827289819717\n",
      "step:5 loss:0.1854981631040573\n",
      "step:6 loss:0.16528119146823883\n",
      "step:7 loss:0.15600772202014923\n",
      "step:8 loss:0.1655452996492386\n",
      "step:9 loss:0.150065079331398\n",
      "step:10 loss:0.18559981882572174\n",
      "step:11 loss:0.14657944440841675\n",
      "step:12 loss:0.11714751273393631\n",
      "step:13 loss:0.2146514505147934\n",
      "step:14 loss:0.15297557413578033\n",
      "step:15 loss:0.11183752864599228\n",
      "step:16 loss:0.08189120143651962\n",
      "step:17 loss:0.17280280590057373\n",
      "step:18 loss:0.15535016357898712\n",
      "step:19 loss:0.15262050926685333\n",
      "step:20 loss:0.2443394511938095\n",
      "step:21 loss:0.09731090068817139\n",
      "step:22 loss:0.14071878790855408\n",
      "step:23 loss:0.09736103564500809\n",
      "step:24 loss:0.2453649789094925\n",
      "step:25 loss:0.12056341022253036\n",
      "step:26 loss:0.17867052555084229\n",
      "step:27 loss:0.15941427648067474\n",
      "step:28 loss:0.16105736792087555\n",
      "step:29 loss:0.1559571474790573\n",
      "step:30 loss:0.10110556334257126\n",
      "step:31 loss:0.07403165847063065\n",
      "step:32 loss:0.13170377910137177\n",
      "step:33 loss:0.24589870870113373\n",
      "step:34 loss:0.15924100577831268\n",
      "step:35 loss:0.2514667809009552\n",
      "step:36 loss:0.15404851734638214\n",
      "step:37 loss:0.06004796549677849\n",
      "step:38 loss:0.14604492485523224\n",
      "step:39 loss:0.09580951184034348\n",
      "step:40 loss:0.12864410877227783\n",
      "step:41 loss:0.23833729326725006\n",
      "step:42 loss:0.1675456315279007\n",
      "step:43 loss:0.0928112342953682\n",
      "step:44 loss:0.07378710061311722\n",
      "step:45 loss:0.27366718649864197\n",
      "step:46 loss:0.24126237630844116\n",
      "step:47 loss:0.1850234866142273\n",
      "step:48 loss:0.1818172186613083\n",
      "step:49 loss:0.4761926829814911\n",
      "step:50 loss:0.4576472043991089\n",
      "step:51 loss:0.10368935018777847\n",
      "step:52 loss:0.29640594124794006\n",
      "step:53 loss:0.16952352225780487\n",
      "step:54 loss:0.0981091856956482\n",
      "step:55 loss:0.27265310287475586\n",
      "step:56 loss:0.12727980315685272\n",
      "step:57 loss:0.2413809448480606\n",
      "step:58 loss:0.10946936160326004\n",
      "step:59 loss:0.10320112109184265\n",
      "step:60 loss:0.23247496783733368\n",
      "step:61 loss:0.19692067801952362\n",
      "step:62 loss:0.05864439904689789\n",
      "step:63 loss:0.17890016734600067\n",
      "step:64 loss:0.09764691442251205\n",
      "step:65 loss:0.18925952911376953\n",
      "step:66 loss:0.09285636991262436\n",
      "step:67 loss:0.09065619111061096\n",
      "step:68 loss:0.20313771069049835\n",
      "step:69 loss:0.34494078159332275\n",
      "step:70 loss:0.20006485283374786\n",
      "step:71 loss:0.13080216944217682\n",
      "step:72 loss:0.21492888033390045\n",
      "step:73 loss:0.16388604044914246\n",
      "step:74 loss:0.18020860850811005\n",
      "step:75 loss:0.30099382996559143\n",
      "step:76 loss:0.08882563561201096\n",
      "step:77 loss:0.16288518905639648\n",
      "step:78 loss:0.1739240139722824\n",
      "step:79 loss:0.12192946672439575\n",
      "step:80 loss:0.09502595663070679\n",
      "step:81 loss:0.09120434522628784\n",
      "step:82 loss:0.16401898860931396\n",
      "step:83 loss:0.18825273215770721\n",
      "step:84 loss:0.1630154401063919\n",
      "step:85 loss:0.1736934334039688\n",
      "step:86 loss:0.1610250473022461\n",
      "step:87 loss:0.19623470306396484\n",
      "step:88 loss:0.1384574919939041\n",
      "step:89 loss:0.09085612744092941\n",
      "step:90 loss:0.3721868097782135\n",
      "step:91 loss:0.0953608825802803\n",
      "step:92 loss:0.16522996127605438\n",
      "step:93 loss:0.2981750965118408\n",
      "step:94 loss:0.19580096006393433\n",
      "step:95 loss:0.22651688754558563\n",
      "step:96 loss:0.376155823469162\n",
      "step:97 loss:0.1106342300772667\n",
      "step:98 loss:0.21202127635478973\n",
      "step:99 loss:0.11309007555246353\n",
      "step:100 loss:0.17471353709697723\n",
      "step:101 loss:0.15817509591579437\n",
      "step:102 loss:0.16186325252056122\n",
      "step:103 loss:0.28755733370780945\n",
      "step:104 loss:0.1647292822599411\n",
      "step:105 loss:0.16147661209106445\n",
      "step:106 loss:0.1884360909461975\n",
      "step:107 loss:0.1830471009016037\n",
      "step:108 loss:0.3012034595012665\n",
      "step:109 loss:0.0444391705095768\n",
      "step:110 loss:0.14049382507801056\n",
      "step:111 loss:0.22743640840053558\n",
      "step:112 loss:0.16597498953342438\n",
      "step:113 loss:0.20837831497192383\n",
      "step:114 loss:0.27772560715675354\n",
      "step:115 loss:0.07028333097696304\n",
      "step:116 loss:0.22774837911128998\n",
      "step:117 loss:0.11851803213357925\n",
      "step:118 loss:0.07314418256282806\n",
      "step:119 loss:0.22295700013637543\n",
      "step:120 loss:0.15139608085155487\n",
      "step:121 loss:0.2177347093820572\n",
      "step:122 loss:0.18806076049804688\n",
      "step:123 loss:0.11843248456716537\n",
      "step:124 loss:0.11792877316474915\n",
      "step:125 loss:0.1512608677148819\n",
      "step:126 loss:0.25720229744911194\n",
      "step:127 loss:0.3091485798358917\n",
      "step:128 loss:0.2318638414144516\n",
      "step:129 loss:0.26359936594963074\n",
      "step:130 loss:0.17770804464817047\n",
      "step:131 loss:0.11995480209589005\n",
      "step:132 loss:0.16627566516399384\n",
      "step:133 loss:0.13035805523395538\n",
      "step:134 loss:0.2774970829486847\n",
      "step:135 loss:0.15193338692188263\n",
      "step:136 loss:0.1265219748020172\n",
      "step:137 loss:0.06362438946962357\n",
      "step:138 loss:0.1490851789712906\n",
      "step:139 loss:0.05338912084698677\n",
      "step:140 loss:0.15802954137325287\n",
      "step:141 loss:0.18390880525112152\n",
      "step:142 loss:0.10854645818471909\n",
      "step:143 loss:0.24884504079818726\n",
      "step:144 loss:0.15631237626075745\n",
      "step:145 loss:0.22632233798503876\n",
      "step:146 loss:0.16712959110736847\n",
      "step:147 loss:0.23203442990779877\n",
      "step:148 loss:0.153879776597023\n",
      "step:149 loss:0.314181923866272\n",
      "step:150 loss:0.1248466968536377\n",
      "step:151 loss:0.25582489371299744\n",
      "step:152 loss:0.17726540565490723\n",
      "step:153 loss:0.1930686980485916\n",
      "step:154 loss:0.24357867240905762\n",
      "step:155 loss:0.18141420185565948\n",
      "step:156 loss:0.2970481812953949\n",
      "step:157 loss:0.06604737043380737\n",
      "step:158 loss:0.1210717260837555\n",
      "step:159 loss:0.09104833006858826\n",
      "step:160 loss:0.2414901703596115\n",
      "step:161 loss:0.14598190784454346\n",
      "step:162 loss:0.22656749188899994\n",
      "step:163 loss:0.15158025920391083\n",
      "step:164 loss:0.128001868724823\n",
      "step:165 loss:0.16457420587539673\n",
      "step:166 loss:0.09088364243507385\n",
      "step:167 loss:0.1574491709470749\n",
      "step:168 loss:0.14119115471839905\n",
      "step:169 loss:0.09988173097372055\n",
      "step:170 loss:0.45800313353538513\n",
      "step:171 loss:0.2156401425600052\n",
      "step:172 loss:0.08266637474298477\n",
      "step:173 loss:0.09118451923131943\n",
      "step:174 loss:0.24073149263858795\n",
      "step:175 loss:0.232258602976799\n",
      "step:176 loss:0.21215008199214935\n",
      "step:177 loss:0.06083935499191284\n",
      "step:178 loss:0.07305910438299179\n",
      "step:179 loss:0.1932194083929062\n",
      "step:180 loss:0.1687137931585312\n",
      "step:181 loss:0.30860525369644165\n",
      "step:182 loss:0.1464993804693222\n",
      "step:183 loss:0.20938701927661896\n",
      "step:184 loss:0.14696843922138214\n",
      "step:185 loss:0.29449957609176636\n",
      "step:186 loss:0.17308151721954346\n",
      "step:187 loss:0.1088683232665062\n",
      "step:188 loss:0.20285028219223022\n",
      "total_train_loss:0.17537130932620865, total_test_acc:0.794962439239947\n",
      "------------------------------ \n",
      " epoch: 20\n",
      "step:0 loss:0.3776041269302368\n",
      "step:1 loss:0.3321976959705353\n",
      "step:2 loss:0.1948614865541458\n",
      "step:3 loss:0.12485242635011673\n",
      "step:4 loss:0.09346506744623184\n",
      "step:5 loss:0.26998522877693176\n",
      "step:6 loss:0.14973637461662292\n",
      "step:7 loss:0.1840553879737854\n",
      "step:8 loss:0.17633505165576935\n",
      "step:9 loss:0.36485496163368225\n",
      "step:10 loss:0.15598072111606598\n",
      "step:11 loss:0.18280695378780365\n",
      "step:12 loss:0.1606406420469284\n",
      "step:13 loss:0.11264718323945999\n",
      "step:14 loss:0.2600409984588623\n",
      "step:15 loss:0.209706649184227\n",
      "step:16 loss:0.11459655314683914\n",
      "step:17 loss:0.13930334150791168\n",
      "step:18 loss:0.18647558987140656\n",
      "step:19 loss:0.21794898808002472\n",
      "step:20 loss:0.11274521797895432\n",
      "step:21 loss:0.1898043304681778\n",
      "step:22 loss:0.2337445616722107\n",
      "step:23 loss:0.15954554080963135\n",
      "step:24 loss:0.20498912036418915\n",
      "step:25 loss:0.20685666799545288\n",
      "step:26 loss:0.1536589115858078\n",
      "step:27 loss:0.41397330164909363\n",
      "step:28 loss:0.0582892931997776\n",
      "step:29 loss:0.2611055076122284\n",
      "step:30 loss:0.11744276434183121\n",
      "step:31 loss:0.2132786065340042\n",
      "step:32 loss:0.23646263778209686\n",
      "step:33 loss:0.0952497124671936\n",
      "step:34 loss:0.22191333770751953\n",
      "step:35 loss:0.16148342192173004\n",
      "step:36 loss:0.20829735696315765\n",
      "step:37 loss:0.37864241003990173\n",
      "step:38 loss:0.13260488212108612\n",
      "step:39 loss:0.17904500663280487\n",
      "step:40 loss:0.12089499831199646\n",
      "step:41 loss:0.21472994983196259\n",
      "step:42 loss:0.36613526940345764\n",
      "step:43 loss:0.19259454309940338\n",
      "step:44 loss:0.09611377120018005\n",
      "step:45 loss:0.09790552407503128\n",
      "step:46 loss:0.07661990076303482\n",
      "step:47 loss:0.1469039022922516\n",
      "step:48 loss:0.12835992872714996\n",
      "step:49 loss:0.27382272481918335\n",
      "step:50 loss:0.14157551527023315\n",
      "step:51 loss:0.42015862464904785\n",
      "step:52 loss:0.13079161942005157\n",
      "step:53 loss:0.1460963934659958\n",
      "step:54 loss:0.2198546677827835\n",
      "step:55 loss:0.1239088699221611\n",
      "step:56 loss:0.29841646552085876\n",
      "step:57 loss:0.25507816672325134\n",
      "step:58 loss:0.15034310519695282\n",
      "step:59 loss:0.2063264101743698\n",
      "step:60 loss:0.10624683648347855\n",
      "step:61 loss:0.19685028493404388\n",
      "step:62 loss:0.3223992884159088\n",
      "step:63 loss:0.18394897878170013\n",
      "step:64 loss:0.2514890730381012\n",
      "step:65 loss:0.3346434533596039\n",
      "step:66 loss:0.11994659900665283\n",
      "step:67 loss:0.24839740991592407\n",
      "step:68 loss:0.29550692439079285\n",
      "step:69 loss:0.2561139762401581\n",
      "step:70 loss:0.2304397076368332\n",
      "step:71 loss:0.18878977000713348\n",
      "step:72 loss:0.16048604249954224\n",
      "step:73 loss:0.35548797249794006\n",
      "step:74 loss:0.059831663966178894\n",
      "step:75 loss:0.17317421734333038\n",
      "step:76 loss:0.11401107907295227\n",
      "step:77 loss:0.16787904500961304\n",
      "step:78 loss:0.29619577527046204\n",
      "step:79 loss:0.12357494235038757\n",
      "step:80 loss:0.27302804589271545\n",
      "step:81 loss:0.32603487372398376\n",
      "step:82 loss:0.16690556704998016\n",
      "step:83 loss:0.13253037631511688\n",
      "step:84 loss:0.07203275710344315\n",
      "step:85 loss:0.135173961520195\n",
      "step:86 loss:0.18287009000778198\n",
      "step:87 loss:0.15206298232078552\n",
      "step:88 loss:0.2967667281627655\n",
      "step:89 loss:0.18435709178447723\n",
      "step:90 loss:0.11003890633583069\n",
      "step:91 loss:0.35604849457740784\n",
      "step:92 loss:0.10305788367986679\n",
      "step:93 loss:0.17804962396621704\n",
      "step:94 loss:0.27622172236442566\n",
      "step:95 loss:0.2360057681798935\n",
      "step:96 loss:0.23432230949401855\n",
      "step:97 loss:0.19790607690811157\n",
      "step:98 loss:0.25724801421165466\n",
      "step:99 loss:0.19518357515335083\n",
      "step:100 loss:0.1991724967956543\n",
      "step:101 loss:0.12061621993780136\n",
      "step:102 loss:0.17018957436084747\n",
      "step:103 loss:0.18954111635684967\n",
      "step:104 loss:0.29191267490386963\n",
      "step:105 loss:0.1705344170331955\n",
      "step:106 loss:0.2691333591938019\n",
      "step:107 loss:0.0897643193602562\n",
      "step:108 loss:0.13524822890758514\n",
      "step:109 loss:0.18552954494953156\n",
      "step:110 loss:0.17177776992321014\n",
      "step:111 loss:0.22564882040023804\n",
      "step:112 loss:0.4309045970439911\n",
      "step:113 loss:0.19833904504776\n",
      "step:114 loss:0.24475856125354767\n",
      "step:115 loss:0.15453235805034637\n",
      "step:116 loss:0.19876109063625336\n",
      "step:117 loss:0.215490460395813\n",
      "step:118 loss:0.19207598268985748\n",
      "step:119 loss:0.20797963440418243\n",
      "step:120 loss:0.13126426935195923\n",
      "step:121 loss:0.13856276869773865\n",
      "step:122 loss:0.11696778982877731\n",
      "step:123 loss:0.15629930794239044\n",
      "step:124 loss:0.21890747547149658\n",
      "step:125 loss:0.10125702619552612\n",
      "step:126 loss:0.12018492817878723\n",
      "step:127 loss:0.2762864828109741\n",
      "step:128 loss:0.28421750664711\n",
      "step:129 loss:0.14957022666931152\n",
      "step:130 loss:0.2465125173330307\n",
      "step:131 loss:0.22244270145893097\n",
      "step:132 loss:0.12959273159503937\n",
      "step:133 loss:0.19537222385406494\n",
      "step:134 loss:0.20530785620212555\n",
      "step:135 loss:0.3620205819606781\n",
      "step:136 loss:0.13840602338314056\n",
      "step:137 loss:0.12957154214382172\n",
      "step:138 loss:0.09362810850143433\n",
      "step:139 loss:0.17585574090480804\n",
      "step:140 loss:0.14424698054790497\n",
      "step:141 loss:0.06309447437524796\n",
      "step:142 loss:0.14362861216068268\n",
      "step:143 loss:0.13954895734786987\n",
      "step:144 loss:0.13980339467525482\n",
      "step:145 loss:0.27568289637565613\n",
      "step:146 loss:0.13539786636829376\n",
      "step:147 loss:0.15708062052726746\n",
      "step:148 loss:0.13993191719055176\n",
      "step:149 loss:0.11298561841249466\n",
      "step:150 loss:0.11655402928590775\n",
      "step:151 loss:0.23480068147182465\n",
      "step:152 loss:0.1092417910695076\n",
      "step:153 loss:0.1709883064031601\n",
      "step:154 loss:0.14793969690799713\n",
      "step:155 loss:0.14681987464427948\n",
      "step:156 loss:0.04576572775840759\n",
      "step:157 loss:0.08204038441181183\n",
      "step:158 loss:0.11453330516815186\n",
      "step:159 loss:0.24978268146514893\n",
      "step:160 loss:0.24041993916034698\n",
      "step:161 loss:0.2918681800365448\n",
      "step:162 loss:0.18914540112018585\n",
      "step:163 loss:0.2257201075553894\n",
      "step:164 loss:0.0970860943198204\n",
      "step:165 loss:0.07509169727563858\n",
      "step:166 loss:0.11245357990264893\n",
      "step:167 loss:0.14793427288532257\n",
      "step:168 loss:0.12322726845741272\n",
      "step:169 loss:0.22795601189136505\n",
      "step:170 loss:0.19678890705108643\n",
      "step:171 loss:0.16355843842029572\n",
      "step:172 loss:0.11405857652425766\n",
      "step:173 loss:0.10123565793037415\n",
      "step:174 loss:0.26774367690086365\n",
      "step:175 loss:0.23550951480865479\n",
      "step:176 loss:0.2559669315814972\n",
      "step:177 loss:0.15444491803646088\n",
      "step:178 loss:0.16686342656612396\n",
      "step:179 loss:0.1385587453842163\n",
      "step:180 loss:0.17071586847305298\n",
      "step:181 loss:0.20923586189746857\n",
      "step:182 loss:0.1820332407951355\n",
      "step:183 loss:0.19544874131679535\n",
      "step:184 loss:0.08050674200057983\n",
      "step:185 loss:0.11029499769210815\n",
      "step:186 loss:0.3058737814426422\n",
      "step:187 loss:0.28614136576652527\n",
      "step:188 loss:0.11449116468429565\n",
      "total_train_loss:0.18994412375019587, total_test_acc:0.9394608926204154\n",
      "------------------------------ \n",
      " epoch: 21\n",
      "step:0 loss:0.10808810591697693\n",
      "step:1 loss:0.1852770894765854\n",
      "step:2 loss:0.22805149853229523\n",
      "step:3 loss:0.1999492198228836\n",
      "step:4 loss:0.194267138838768\n",
      "step:5 loss:0.24337650835514069\n",
      "step:6 loss:0.17093698680400848\n",
      "step:7 loss:0.1086244061589241\n",
      "step:8 loss:0.11549519747495651\n",
      "step:9 loss:0.17184680700302124\n",
      "step:10 loss:0.06301682442426682\n",
      "step:11 loss:0.10746308416128159\n",
      "step:12 loss:0.10024210065603256\n",
      "step:13 loss:0.1057719960808754\n",
      "step:14 loss:0.13731512427330017\n",
      "step:15 loss:0.1589289903640747\n",
      "step:16 loss:0.13512296974658966\n",
      "step:17 loss:0.2539854347705841\n",
      "step:18 loss:0.21089313924312592\n",
      "step:19 loss:0.19838280975818634\n",
      "step:20 loss:0.19073422253131866\n",
      "step:21 loss:0.03682706877589226\n",
      "step:22 loss:0.06954345852136612\n",
      "step:23 loss:0.33945587277412415\n",
      "step:24 loss:0.18562716245651245\n",
      "step:25 loss:0.1611708104610443\n",
      "step:26 loss:0.11624587327241898\n",
      "step:27 loss:0.16485287249088287\n",
      "step:28 loss:0.08203066140413284\n",
      "step:29 loss:0.287554532289505\n",
      "step:30 loss:0.1923321634531021\n",
      "step:31 loss:0.16764211654663086\n",
      "step:32 loss:0.08231388032436371\n",
      "step:33 loss:0.1435084491968155\n",
      "step:34 loss:0.12249413877725601\n",
      "step:35 loss:0.15877144038677216\n",
      "step:36 loss:0.10889191180467606\n",
      "step:37 loss:0.20136934518814087\n",
      "step:38 loss:0.1756550669670105\n",
      "step:39 loss:0.0694064348936081\n",
      "step:40 loss:0.16665121912956238\n",
      "step:41 loss:0.11161840707063675\n",
      "step:42 loss:0.10341066867113113\n",
      "step:43 loss:0.13912735879421234\n",
      "step:44 loss:0.1243763193488121\n",
      "step:45 loss:0.3028976023197174\n",
      "step:46 loss:0.39227113127708435\n",
      "step:47 loss:0.2876380980014801\n",
      "step:48 loss:0.3143751621246338\n",
      "step:49 loss:0.2904345691204071\n",
      "step:50 loss:0.3635740280151367\n",
      "step:51 loss:0.3183094263076782\n",
      "step:52 loss:0.2130230814218521\n",
      "step:53 loss:0.07446278631687164\n",
      "step:54 loss:0.14427433907985687\n",
      "step:55 loss:0.3297373354434967\n",
      "step:56 loss:0.24267661571502686\n",
      "step:57 loss:0.13970410823822021\n",
      "step:58 loss:0.2676956355571747\n",
      "step:59 loss:0.10445966571569443\n",
      "step:60 loss:0.04414423927664757\n",
      "step:61 loss:0.2571733891963959\n",
      "step:62 loss:0.12011107802391052\n",
      "step:63 loss:0.2509266138076782\n",
      "step:64 loss:0.4118387997150421\n",
      "step:65 loss:0.14123211801052094\n",
      "step:66 loss:0.22851763665676117\n",
      "step:67 loss:0.17465554177761078\n",
      "step:68 loss:0.37530794739723206\n",
      "step:69 loss:0.19215010106563568\n",
      "step:70 loss:0.36788472533226013\n",
      "step:71 loss:0.1521906554698944\n",
      "step:72 loss:0.3234628140926361\n",
      "step:73 loss:0.18738305568695068\n",
      "step:74 loss:0.1815570592880249\n",
      "step:75 loss:0.21861553192138672\n",
      "step:76 loss:0.3368644416332245\n",
      "step:77 loss:0.21152351796627045\n",
      "step:78 loss:0.41341522336006165\n",
      "step:79 loss:0.1994616985321045\n",
      "step:80 loss:0.18924939632415771\n",
      "step:81 loss:0.17862842977046967\n",
      "step:82 loss:0.16473183035850525\n",
      "step:83 loss:0.13176240026950836\n",
      "step:84 loss:0.3427213430404663\n",
      "step:85 loss:0.32263824343681335\n",
      "step:86 loss:0.07565953582525253\n",
      "step:87 loss:0.3069504201412201\n",
      "step:88 loss:0.19850791990756989\n",
      "step:89 loss:0.13246792554855347\n",
      "step:90 loss:0.2029004693031311\n",
      "step:91 loss:0.26662150025367737\n",
      "step:92 loss:0.2845950722694397\n",
      "step:93 loss:0.16150444746017456\n",
      "step:94 loss:0.08766556531190872\n",
      "step:95 loss:0.1259623020887375\n",
      "step:96 loss:0.18959707021713257\n",
      "step:97 loss:0.38877761363983154\n",
      "step:98 loss:0.1616450697183609\n",
      "step:99 loss:0.1687426120042801\n",
      "step:100 loss:0.2661965787410736\n",
      "step:101 loss:0.2713865339756012\n",
      "step:102 loss:0.16615302860736847\n",
      "step:103 loss:0.1433071494102478\n",
      "step:104 loss:0.14570187032222748\n",
      "step:105 loss:0.1404270976781845\n",
      "step:106 loss:0.12984701991081238\n",
      "step:107 loss:0.1592663675546646\n",
      "step:108 loss:0.12208419293165207\n",
      "step:109 loss:0.19950026273727417\n",
      "step:110 loss:0.2559541165828705\n",
      "step:111 loss:0.1412690132856369\n",
      "step:112 loss:0.09653311222791672\n",
      "step:113 loss:0.16790492832660675\n",
      "step:114 loss:0.24956510961055756\n",
      "step:115 loss:0.09205494076013565\n",
      "step:116 loss:0.3065245747566223\n",
      "step:117 loss:0.1507127732038498\n",
      "step:118 loss:0.26686057448387146\n",
      "step:119 loss:0.26629558205604553\n",
      "step:120 loss:0.07599926739931107\n",
      "step:121 loss:0.27477937936782837\n",
      "step:122 loss:0.16703785955905914\n",
      "step:123 loss:0.09805699437856674\n",
      "step:124 loss:0.19879250228405\n",
      "step:125 loss:0.11504177004098892\n",
      "step:126 loss:0.17484574019908905\n",
      "step:127 loss:0.15271928906440735\n",
      "step:128 loss:0.10149458795785904\n",
      "step:129 loss:0.11567041277885437\n",
      "step:130 loss:0.12419316172599792\n",
      "step:131 loss:0.3040176331996918\n",
      "step:132 loss:0.2618395984172821\n",
      "step:133 loss:0.09538891911506653\n",
      "step:134 loss:0.10603273659944534\n",
      "step:135 loss:0.17283831536769867\n",
      "step:136 loss:0.12020715326070786\n",
      "step:137 loss:0.23760484158992767\n",
      "step:138 loss:0.03147434443235397\n",
      "step:139 loss:0.11233460903167725\n",
      "step:140 loss:0.15433098375797272\n",
      "step:141 loss:0.15156160295009613\n",
      "step:142 loss:0.08696234226226807\n",
      "step:143 loss:0.08806115388870239\n",
      "step:144 loss:0.09074021130800247\n",
      "step:145 loss:0.16938932240009308\n",
      "step:146 loss:0.2689153254032135\n",
      "step:147 loss:0.24784593284130096\n",
      "step:148 loss:0.18025100231170654\n",
      "step:149 loss:0.11157114058732986\n",
      "step:150 loss:0.3593907654285431\n",
      "step:151 loss:0.2096491903066635\n",
      "step:152 loss:0.20600034296512604\n",
      "step:153 loss:0.14179272949695587\n",
      "step:154 loss:0.14400966465473175\n",
      "step:155 loss:0.19837002456188202\n",
      "step:156 loss:0.16819047927856445\n",
      "step:157 loss:0.23846115171909332\n",
      "step:158 loss:0.13225950300693512\n",
      "step:159 loss:0.18478752672672272\n",
      "step:160 loss:0.09904507547616959\n",
      "step:161 loss:0.036957453936338425\n",
      "step:162 loss:0.12290018796920776\n",
      "step:163 loss:0.16624824702739716\n",
      "step:164 loss:0.14146749675273895\n",
      "step:165 loss:0.1098397746682167\n",
      "step:166 loss:0.327892929315567\n",
      "step:167 loss:0.12241549044847488\n",
      "step:168 loss:0.15986917912960052\n",
      "step:169 loss:0.11908882856369019\n",
      "step:170 loss:0.18256068229675293\n",
      "step:171 loss:0.1380259245634079\n",
      "step:172 loss:0.11680043488740921\n",
      "step:173 loss:0.10102585703134537\n",
      "step:174 loss:0.16237370669841766\n",
      "step:175 loss:0.14116299152374268\n",
      "step:176 loss:0.08806940168142319\n",
      "step:177 loss:0.0492023266851902\n",
      "step:178 loss:0.2354235202074051\n",
      "step:179 loss:0.18715935945510864\n",
      "step:180 loss:0.20388060808181763\n",
      "step:181 loss:0.1392277628183365\n",
      "step:182 loss:0.24547308683395386\n",
      "step:183 loss:0.10925790667533875\n",
      "step:184 loss:0.08218427747488022\n",
      "step:185 loss:0.12604741752147675\n",
      "step:186 loss:0.1404421627521515\n",
      "step:187 loss:0.15042641758918762\n",
      "step:188 loss:0.14038120210170746\n",
      "total_train_loss:0.18090015891543093, total_test_acc:0.8104286345558992\n",
      "------------------------------ \n",
      " epoch: 22\n",
      "step:0 loss:0.10607714205980301\n",
      "step:1 loss:0.06999457627534866\n",
      "step:2 loss:0.46168801188468933\n",
      "step:3 loss:0.15941277146339417\n",
      "step:4 loss:0.16329143941402435\n",
      "step:5 loss:0.10619501024484634\n",
      "step:6 loss:0.18769872188568115\n",
      "step:7 loss:0.35808709263801575\n",
      "step:8 loss:0.08315546065568924\n",
      "step:9 loss:0.29804807901382446\n",
      "step:10 loss:0.14736084640026093\n",
      "step:11 loss:0.16183535754680634\n",
      "step:12 loss:0.10866883397102356\n",
      "step:13 loss:0.23659680783748627\n",
      "step:14 loss:0.08409622311592102\n",
      "step:15 loss:0.1583685427904129\n",
      "step:16 loss:0.19281752407550812\n",
      "step:17 loss:0.2356000393629074\n",
      "step:18 loss:0.24298955500125885\n",
      "step:19 loss:0.2725290358066559\n",
      "step:20 loss:0.2270756959915161\n",
      "step:21 loss:0.17751942574977875\n",
      "step:22 loss:0.2310781031847\n",
      "step:23 loss:0.15868820250034332\n",
      "step:24 loss:0.08012796193361282\n",
      "step:25 loss:0.1602686643600464\n",
      "step:26 loss:0.1136355996131897\n",
      "step:27 loss:0.14603348076343536\n",
      "step:28 loss:0.205227792263031\n",
      "step:29 loss:0.18669818341732025\n",
      "step:30 loss:0.16290275752544403\n",
      "step:31 loss:0.19134551286697388\n",
      "step:32 loss:0.186102032661438\n",
      "step:33 loss:0.10103395581245422\n",
      "step:34 loss:0.17621958255767822\n",
      "step:35 loss:0.2601167857646942\n",
      "step:36 loss:0.14314047992229462\n",
      "step:37 loss:0.38854119181632996\n",
      "step:38 loss:0.12744317948818207\n",
      "step:39 loss:0.08539798855781555\n",
      "step:40 loss:0.13000622391700745\n",
      "step:41 loss:0.20736856758594513\n",
      "step:42 loss:0.05664324387907982\n",
      "step:43 loss:0.23515157401561737\n",
      "step:44 loss:0.13303211331367493\n",
      "step:45 loss:0.09853649884462357\n",
      "step:46 loss:0.19789521396160126\n",
      "step:47 loss:0.22914230823516846\n",
      "step:48 loss:0.10857974737882614\n",
      "step:49 loss:0.24220991134643555\n",
      "step:50 loss:0.19719381630420685\n",
      "step:51 loss:0.13976486027240753\n",
      "step:52 loss:0.11841937154531479\n",
      "step:53 loss:0.23031453788280487\n",
      "step:54 loss:0.3597065508365631\n",
      "step:55 loss:0.2628231644630432\n",
      "step:56 loss:0.15361617505550385\n",
      "step:57 loss:0.295928031206131\n",
      "step:58 loss:0.19106566905975342\n",
      "step:59 loss:0.2967934310436249\n",
      "step:60 loss:0.07922354340553284\n",
      "step:61 loss:0.19878190755844116\n",
      "step:62 loss:0.1160358116030693\n",
      "step:63 loss:0.26166990399360657\n",
      "step:64 loss:0.12873296439647675\n",
      "step:65 loss:0.10685016959905624\n",
      "step:66 loss:0.14331825077533722\n",
      "step:67 loss:0.18614286184310913\n",
      "step:68 loss:0.12302523851394653\n",
      "step:69 loss:0.1471889317035675\n",
      "step:70 loss:0.3315766751766205\n",
      "step:71 loss:0.15167789161205292\n",
      "step:72 loss:0.11162566393613815\n",
      "step:73 loss:0.1870274543762207\n",
      "step:74 loss:0.35820475220680237\n",
      "step:75 loss:0.13012951612472534\n",
      "step:76 loss:0.09104719012975693\n",
      "step:77 loss:0.07340169697999954\n",
      "step:78 loss:0.2881679832935333\n",
      "step:79 loss:0.09625164419412613\n",
      "step:80 loss:0.19156938791275024\n",
      "step:81 loss:0.23389585316181183\n",
      "step:82 loss:0.16071437299251556\n",
      "step:83 loss:0.29330524802207947\n",
      "step:84 loss:0.24243132770061493\n",
      "step:85 loss:0.12618808448314667\n",
      "step:86 loss:0.15204952657222748\n",
      "step:87 loss:0.08879866451025009\n",
      "step:88 loss:0.09380155801773071\n",
      "step:89 loss:0.10745543241500854\n",
      "step:90 loss:0.051859259605407715\n",
      "step:91 loss:0.3737322986125946\n",
      "step:92 loss:0.1600063592195511\n",
      "step:93 loss:0.10280091315507889\n",
      "step:94 loss:0.23585093021392822\n",
      "step:95 loss:0.24686157703399658\n",
      "step:96 loss:0.19030450284481049\n",
      "step:97 loss:0.07255122810602188\n",
      "step:98 loss:0.08927290886640549\n",
      "step:99 loss:0.08562406152486801\n",
      "step:100 loss:0.15889908373355865\n",
      "step:101 loss:0.13779811561107635\n",
      "step:102 loss:0.14634494483470917\n",
      "step:103 loss:0.29939714074134827\n",
      "step:104 loss:0.26361140608787537\n",
      "step:105 loss:0.14754854142665863\n",
      "step:106 loss:0.23644231259822845\n",
      "step:107 loss:0.25582367181777954\n",
      "step:108 loss:0.07510112226009369\n",
      "step:109 loss:0.14576484262943268\n",
      "step:110 loss:0.1438511461019516\n",
      "step:111 loss:0.1295856535434723\n",
      "step:112 loss:0.11489907652139664\n",
      "step:113 loss:0.17921794950962067\n",
      "step:114 loss:0.10227200388908386\n",
      "step:115 loss:0.12681595981121063\n",
      "step:116 loss:0.181349515914917\n",
      "step:117 loss:0.1454782634973526\n",
      "step:118 loss:0.22649163007736206\n",
      "step:119 loss:0.13534612953662872\n",
      "step:120 loss:0.0588008314371109\n",
      "step:121 loss:0.16810457408428192\n",
      "step:122 loss:0.172914519906044\n",
      "step:123 loss:0.32511070370674133\n",
      "step:124 loss:0.06783907860517502\n",
      "step:125 loss:0.17549574375152588\n",
      "step:126 loss:0.20887047052383423\n",
      "step:127 loss:0.06771507859230042\n",
      "step:128 loss:0.30129772424697876\n",
      "step:129 loss:0.14312200248241425\n",
      "step:130 loss:0.25880059599876404\n",
      "step:131 loss:0.1667667031288147\n",
      "step:132 loss:0.227471724152565\n",
      "step:133 loss:0.19397057592868805\n",
      "step:134 loss:0.17012447118759155\n",
      "step:135 loss:0.22476471960544586\n",
      "step:136 loss:0.1618926227092743\n",
      "step:137 loss:0.1961865872144699\n",
      "step:138 loss:0.10011044889688492\n",
      "step:139 loss:0.1920652836561203\n",
      "step:140 loss:0.08518306165933609\n",
      "step:141 loss:0.16930650174617767\n",
      "step:142 loss:0.2963884174823761\n",
      "step:143 loss:0.1856563538312912\n",
      "step:144 loss:0.10124271363019943\n",
      "step:145 loss:0.18247286975383759\n",
      "step:146 loss:0.12077348679304123\n",
      "step:147 loss:0.1829487830400467\n",
      "step:148 loss:0.17867527902126312\n",
      "step:149 loss:0.20056231319904327\n",
      "step:150 loss:0.33033454418182373\n",
      "step:151 loss:0.1629234403371811\n",
      "step:152 loss:0.16870637238025665\n",
      "step:153 loss:0.1772020012140274\n",
      "step:154 loss:0.12650088965892792\n",
      "step:155 loss:0.12731300294399261\n",
      "step:156 loss:0.06180088222026825\n",
      "step:157 loss:0.14653678238391876\n",
      "step:158 loss:0.16759033501148224\n",
      "step:159 loss:0.1989917755126953\n",
      "step:160 loss:0.1699962615966797\n",
      "step:161 loss:0.21726833283901215\n",
      "step:162 loss:0.11802220344543457\n",
      "step:163 loss:0.22420065104961395\n",
      "step:164 loss:0.29690277576446533\n",
      "step:165 loss:0.2634386718273163\n",
      "step:166 loss:0.1291586309671402\n",
      "step:167 loss:0.14560244977474213\n",
      "step:168 loss:0.12907682359218597\n",
      "step:169 loss:0.2096697837114334\n",
      "step:170 loss:0.26182955503463745\n",
      "step:171 loss:0.17651744186878204\n",
      "step:172 loss:0.09126336127519608\n",
      "step:173 loss:0.21969425678253174\n",
      "step:174 loss:0.19153769314289093\n",
      "step:175 loss:0.409781813621521\n",
      "step:176 loss:0.20743656158447266\n",
      "step:177 loss:0.08837482333183289\n",
      "step:178 loss:0.21188229322433472\n",
      "step:179 loss:0.11255200952291489\n",
      "step:180 loss:0.10640589147806168\n",
      "step:181 loss:0.23185420036315918\n",
      "step:182 loss:0.20963813364505768\n",
      "step:183 loss:0.2098298817873001\n",
      "step:184 loss:0.10591372847557068\n",
      "step:185 loss:0.089004285633564\n",
      "step:186 loss:0.15838465094566345\n",
      "step:187 loss:0.23518675565719604\n",
      "step:188 loss:0.040942639112472534\n",
      "total_train_loss:0.17767229543483637, total_test_acc:0.8820150243040212\n",
      "------------------------------ \n",
      " epoch: 23\n",
      "step:0 loss:0.1265859603881836\n",
      "step:1 loss:0.16329066455364227\n",
      "step:2 loss:0.28646120429039\n",
      "step:3 loss:0.10956364870071411\n",
      "step:4 loss:0.1162618100643158\n",
      "step:5 loss:0.15971460938453674\n",
      "step:6 loss:0.1581542044878006\n",
      "step:7 loss:0.3078812062740326\n",
      "step:8 loss:0.06887685507535934\n",
      "step:9 loss:0.087087482213974\n",
      "step:10 loss:0.22795705497264862\n",
      "step:11 loss:0.06934826821088791\n",
      "step:12 loss:0.059723805636167526\n",
      "step:13 loss:0.2601105868816376\n",
      "step:14 loss:0.056870684027671814\n",
      "step:15 loss:0.19236011803150177\n",
      "step:16 loss:0.10607779771089554\n",
      "step:17 loss:0.07606634497642517\n",
      "step:18 loss:0.149924173951149\n",
      "step:19 loss:0.10294509679079056\n",
      "step:20 loss:0.16790755093097687\n",
      "step:21 loss:0.1807805448770523\n",
      "step:22 loss:0.10586384683847427\n",
      "step:23 loss:0.1468188762664795\n",
      "step:24 loss:0.14569076895713806\n",
      "step:25 loss:0.08593815565109253\n",
      "step:26 loss:0.10605951398611069\n",
      "step:27 loss:0.13224442303180695\n",
      "step:28 loss:0.12361034750938416\n",
      "step:29 loss:0.16392762959003448\n",
      "step:30 loss:0.15824736654758453\n",
      "step:31 loss:0.13930092751979828\n",
      "step:32 loss:0.2490627020597458\n",
      "step:33 loss:0.15468521416187286\n",
      "step:34 loss:0.09061133861541748\n",
      "step:35 loss:0.18547701835632324\n",
      "step:36 loss:0.2353900671005249\n",
      "step:37 loss:0.06592294573783875\n",
      "step:38 loss:0.07494093477725983\n",
      "step:39 loss:0.29255345463752747\n",
      "step:40 loss:0.1961747258901596\n",
      "step:41 loss:0.2145395129919052\n",
      "step:42 loss:0.08896472305059433\n",
      "step:43 loss:0.1023220643401146\n",
      "step:44 loss:0.13604286313056946\n",
      "step:45 loss:0.14491255581378937\n",
      "step:46 loss:0.17261545360088348\n",
      "step:47 loss:0.059631675481796265\n",
      "step:48 loss:0.12092160433530807\n",
      "step:49 loss:0.16409026086330414\n",
      "step:50 loss:0.14691531658172607\n",
      "step:51 loss:0.30384698510169983\n",
      "step:52 loss:0.1129881963133812\n",
      "step:53 loss:0.13376453518867493\n",
      "step:54 loss:0.08946067839860916\n",
      "step:55 loss:0.15913644433021545\n",
      "step:56 loss:0.10304128378629684\n",
      "step:57 loss:0.074718177318573\n",
      "step:58 loss:0.12470293790102005\n",
      "step:59 loss:0.07227771729230881\n",
      "step:60 loss:0.1071520671248436\n",
      "step:61 loss:0.11593358963727951\n",
      "step:62 loss:0.22337090969085693\n",
      "step:63 loss:0.09405028820037842\n",
      "step:64 loss:0.27201464772224426\n",
      "step:65 loss:0.21330611407756805\n",
      "step:66 loss:0.14440886676311493\n",
      "step:67 loss:0.16685569286346436\n",
      "step:68 loss:0.13654464483261108\n",
      "step:69 loss:0.034021925181150436\n",
      "step:70 loss:0.25219839811325073\n",
      "step:71 loss:0.22350287437438965\n",
      "step:72 loss:0.10980906337499619\n",
      "step:73 loss:0.2760052978992462\n",
      "step:74 loss:0.1578025370836258\n",
      "step:75 loss:0.11706379055976868\n",
      "step:76 loss:0.09547188133001328\n",
      "step:77 loss:0.09518411755561829\n",
      "step:78 loss:0.13704365491867065\n",
      "step:79 loss:0.20623326301574707\n",
      "step:80 loss:0.06250061839818954\n",
      "step:81 loss:0.09523042291402817\n",
      "step:82 loss:0.17030298709869385\n",
      "step:83 loss:0.1309092491865158\n",
      "step:84 loss:0.20738796889781952\n",
      "step:85 loss:0.2954353094100952\n",
      "step:86 loss:0.13219162821769714\n",
      "step:87 loss:0.0914667472243309\n",
      "step:88 loss:0.08775793761014938\n",
      "step:89 loss:0.3195939064025879\n",
      "step:90 loss:0.07278518378734589\n",
      "step:91 loss:0.0748516321182251\n",
      "step:92 loss:0.07848171889781952\n",
      "step:93 loss:0.1519838124513626\n",
      "step:94 loss:0.22051745653152466\n",
      "step:95 loss:0.20502237975597382\n",
      "step:96 loss:0.17906731367111206\n",
      "step:97 loss:0.30762040615081787\n",
      "step:98 loss:0.30551081895828247\n",
      "step:99 loss:0.210660919547081\n",
      "step:100 loss:0.2991308271884918\n",
      "step:101 loss:0.19220523536205292\n",
      "step:102 loss:0.2012358456850052\n",
      "step:103 loss:0.16093717515468597\n",
      "step:104 loss:0.17851205170154572\n",
      "step:105 loss:0.054979901760816574\n",
      "step:106 loss:0.18262946605682373\n",
      "step:107 loss:0.1299271434545517\n",
      "step:108 loss:0.2583780586719513\n",
      "step:109 loss:0.16372670233249664\n",
      "step:110 loss:0.20625652372837067\n",
      "step:111 loss:0.4040413796901703\n",
      "step:112 loss:0.3218536972999573\n",
      "step:113 loss:0.11704665422439575\n",
      "step:114 loss:0.23395739495754242\n",
      "step:115 loss:0.10627105087041855\n",
      "step:116 loss:0.2502020001411438\n",
      "step:117 loss:0.2748105227947235\n",
      "step:118 loss:0.18784229457378387\n",
      "step:119 loss:0.12432899326086044\n",
      "step:120 loss:0.11653760820627213\n",
      "step:121 loss:0.22486861050128937\n",
      "step:122 loss:0.13872113823890686\n",
      "step:123 loss:0.226715087890625\n",
      "step:124 loss:0.18984372913837433\n",
      "step:125 loss:0.09585701674222946\n",
      "step:126 loss:0.08665450662374496\n",
      "step:127 loss:0.23113210499286652\n",
      "step:128 loss:0.12967480719089508\n",
      "step:129 loss:0.12859229743480682\n",
      "step:130 loss:0.1271253079175949\n",
      "step:131 loss:0.18527697026729584\n",
      "step:132 loss:0.08831057697534561\n",
      "step:133 loss:0.17927052080631256\n",
      "step:134 loss:0.13121691346168518\n",
      "step:135 loss:0.16598288714885712\n",
      "step:136 loss:0.043167274445295334\n",
      "step:137 loss:0.320380300283432\n",
      "step:138 loss:0.27332618832588196\n",
      "step:139 loss:0.23529481887817383\n",
      "step:140 loss:0.12487170845270157\n",
      "step:141 loss:0.2999059855937958\n",
      "step:142 loss:0.09626322239637375\n",
      "step:143 loss:0.19329078495502472\n",
      "step:144 loss:0.14495064318180084\n",
      "step:145 loss:0.19734574854373932\n",
      "step:146 loss:0.12679336965084076\n",
      "step:147 loss:0.23190931975841522\n",
      "step:148 loss:0.0854487344622612\n",
      "step:149 loss:0.1618533581495285\n",
      "step:150 loss:0.11407586932182312\n",
      "step:151 loss:0.21686623990535736\n",
      "step:152 loss:0.07641396671533585\n",
      "step:153 loss:0.16139276325702667\n",
      "step:154 loss:0.10992293804883957\n",
      "step:155 loss:0.1539265215396881\n",
      "step:156 loss:0.34813058376312256\n",
      "step:157 loss:0.19508470594882965\n",
      "step:158 loss:0.1861834079027176\n",
      "step:159 loss:0.16979223489761353\n",
      "step:160 loss:0.09051395207643509\n",
      "step:161 loss:0.06449619680643082\n",
      "step:162 loss:0.3148001432418823\n",
      "step:163 loss:0.07558902353048325\n",
      "step:164 loss:0.28644460439682007\n",
      "step:165 loss:0.1370880901813507\n",
      "step:166 loss:0.08172736316919327\n",
      "step:167 loss:0.17663146555423737\n",
      "step:168 loss:0.16050653159618378\n",
      "step:169 loss:0.08012876659631729\n",
      "step:170 loss:0.14550955593585968\n",
      "step:171 loss:0.04099441319704056\n",
      "step:172 loss:0.1502770036458969\n",
      "step:173 loss:0.14999082684516907\n",
      "step:174 loss:0.1781104952096939\n",
      "step:175 loss:0.19345952570438385\n",
      "step:176 loss:0.03594525530934334\n",
      "step:177 loss:0.2852207124233246\n",
      "step:178 loss:0.3156159222126007\n",
      "step:179 loss:0.08868902921676636\n",
      "step:180 loss:0.1407981514930725\n",
      "step:181 loss:0.10905648022890091\n",
      "step:182 loss:0.4371738135814667\n",
      "step:183 loss:0.1249857172369957\n",
      "step:184 loss:0.09412196278572083\n",
      "step:185 loss:0.0776766985654831\n",
      "step:186 loss:0.15822668373584747\n",
      "step:187 loss:0.2354522943496704\n",
      "step:188 loss:0.08323261141777039\n",
      "total_train_loss:0.16203043584097573, total_test_acc:0.9350419796730004\n",
      "------------------------------ \n",
      " epoch: 24\n",
      "step:0 loss:0.20518742501735687\n",
      "step:1 loss:0.09201648831367493\n",
      "step:2 loss:0.16864897310733795\n",
      "step:3 loss:0.1999635249376297\n",
      "step:4 loss:0.1112256720662117\n",
      "step:5 loss:0.27147147059440613\n",
      "step:6 loss:0.15617918968200684\n",
      "step:7 loss:0.1703609973192215\n",
      "step:8 loss:0.09714320302009583\n",
      "step:9 loss:0.08333351463079453\n",
      "step:10 loss:0.18880055844783783\n",
      "step:11 loss:0.17252515256404877\n",
      "step:12 loss:0.11342117190361023\n",
      "step:13 loss:0.12074575573205948\n",
      "step:14 loss:0.19018037617206573\n",
      "step:15 loss:0.13440048694610596\n",
      "step:16 loss:0.043160099536180496\n",
      "step:17 loss:0.2258128523826599\n",
      "step:18 loss:0.14133568108081818\n",
      "step:19 loss:0.10738448053598404\n",
      "step:20 loss:0.1319386065006256\n",
      "step:21 loss:0.20696747303009033\n",
      "step:22 loss:0.10119455307722092\n",
      "step:23 loss:0.11665449291467667\n",
      "step:24 loss:0.10549677163362503\n",
      "step:25 loss:0.16596315801143646\n",
      "step:26 loss:0.20051278173923492\n",
      "step:27 loss:0.1623370200395584\n",
      "step:28 loss:0.16240409016609192\n",
      "step:29 loss:0.10991921275854111\n",
      "step:30 loss:0.12732750177383423\n",
      "step:31 loss:0.10258505493402481\n",
      "step:32 loss:0.05479426681995392\n",
      "step:33 loss:0.09217817336320877\n",
      "step:34 loss:0.11352448910474777\n",
      "step:35 loss:0.1778431534767151\n",
      "step:36 loss:0.1398538500070572\n",
      "step:37 loss:0.18241851031780243\n",
      "step:38 loss:0.16563202440738678\n",
      "step:39 loss:0.1529746949672699\n",
      "step:40 loss:0.05378246679902077\n",
      "step:41 loss:0.15928258001804352\n",
      "step:42 loss:0.07036435604095459\n",
      "step:43 loss:0.1610671728849411\n",
      "step:44 loss:0.1005139946937561\n",
      "step:45 loss:0.20920908451080322\n",
      "step:46 loss:0.22798626124858856\n",
      "step:47 loss:0.08954233676195145\n",
      "step:48 loss:0.10776007175445557\n",
      "step:49 loss:0.08261013776063919\n",
      "step:50 loss:0.1180352047085762\n",
      "step:51 loss:0.10461297631263733\n",
      "step:52 loss:0.0727999359369278\n",
      "step:53 loss:0.13798777759075165\n",
      "step:54 loss:0.10136470198631287\n",
      "step:55 loss:0.16992740333080292\n",
      "step:56 loss:0.09620899707078934\n",
      "step:57 loss:0.0795564129948616\n",
      "step:58 loss:0.22600175440311432\n",
      "step:59 loss:0.10861217230558395\n",
      "step:60 loss:0.17412419617176056\n",
      "step:61 loss:0.21754653751850128\n",
      "step:62 loss:0.08791474252939224\n",
      "step:63 loss:0.18452729284763336\n",
      "step:64 loss:0.060887422412633896\n",
      "step:65 loss:0.09734676033258438\n",
      "step:66 loss:0.2743367552757263\n",
      "step:67 loss:0.3539727032184601\n",
      "step:68 loss:0.13338437676429749\n",
      "step:69 loss:0.08957213163375854\n",
      "step:70 loss:0.21073444187641144\n",
      "step:71 loss:0.027896041050553322\n",
      "step:72 loss:0.198753222823143\n",
      "step:73 loss:0.1697102189064026\n",
      "step:74 loss:0.16926591098308563\n",
      "step:75 loss:0.18657328188419342\n",
      "step:76 loss:0.11340253800153732\n",
      "step:77 loss:0.12370666116476059\n",
      "step:78 loss:0.041145145893096924\n",
      "step:79 loss:0.08009902387857437\n",
      "step:80 loss:0.1619793176651001\n",
      "step:81 loss:0.05651910603046417\n",
      "step:82 loss:0.13967978954315186\n",
      "step:83 loss:0.30335119366645813\n",
      "step:84 loss:0.2823355197906494\n",
      "step:85 loss:0.12191814184188843\n",
      "step:86 loss:0.12388592958450317\n",
      "step:87 loss:0.1326303333044052\n",
      "step:88 loss:0.13234813511371613\n",
      "step:89 loss:0.16500133275985718\n",
      "step:90 loss:0.0874992311000824\n",
      "step:91 loss:0.3071034252643585\n",
      "step:92 loss:0.10654830187559128\n",
      "step:93 loss:0.2514623701572418\n",
      "step:94 loss:0.28081318736076355\n",
      "step:95 loss:0.13774293661117554\n",
      "step:96 loss:0.11974561214447021\n",
      "step:97 loss:0.16444654762744904\n",
      "step:98 loss:0.15484413504600525\n",
      "step:99 loss:0.15455256402492523\n",
      "step:100 loss:0.1075546070933342\n",
      "step:101 loss:0.10726387053728104\n",
      "step:102 loss:0.10649356991052628\n",
      "step:103 loss:0.14447805285453796\n",
      "step:104 loss:0.1981978565454483\n",
      "step:105 loss:0.2957707345485687\n",
      "step:106 loss:0.32453060150146484\n",
      "step:107 loss:0.17594443261623383\n",
      "step:108 loss:0.33407315611839294\n",
      "step:109 loss:0.2864217162132263\n",
      "step:110 loss:0.2857191562652588\n",
      "step:111 loss:0.16353945434093475\n",
      "step:112 loss:0.3976595401763916\n",
      "step:113 loss:0.2716088891029358\n",
      "step:114 loss:0.19807110726833344\n",
      "step:115 loss:0.07589023560285568\n",
      "step:116 loss:0.09043396264314651\n",
      "step:117 loss:0.17202128469944\n",
      "step:118 loss:0.12031996995210648\n",
      "step:119 loss:0.2730863392353058\n",
      "step:120 loss:0.25871962308883667\n",
      "step:121 loss:0.2234211564064026\n",
      "step:122 loss:0.11452368646860123\n",
      "step:123 loss:0.16603004932403564\n",
      "step:124 loss:0.13449828326702118\n",
      "step:125 loss:0.15068911015987396\n",
      "step:126 loss:0.2965332567691803\n",
      "step:127 loss:0.18350185453891754\n",
      "step:128 loss:0.1420733481645584\n",
      "step:129 loss:0.1486942619085312\n",
      "step:130 loss:0.1405039280653\n",
      "step:131 loss:0.04217543825507164\n",
      "step:132 loss:0.1389418989419937\n",
      "step:133 loss:0.09293800592422485\n",
      "step:134 loss:0.15149769186973572\n",
      "step:135 loss:0.2681151330471039\n",
      "step:136 loss:0.19104653596878052\n",
      "step:137 loss:0.12059047073125839\n",
      "step:138 loss:0.1971893459558487\n",
      "step:139 loss:0.13459628820419312\n",
      "step:140 loss:0.1896258443593979\n",
      "step:141 loss:0.2338053435087204\n",
      "step:142 loss:0.10854020714759827\n",
      "step:143 loss:0.15641655027866364\n",
      "step:144 loss:0.12328780442476273\n",
      "step:145 loss:0.06778451800346375\n",
      "step:146 loss:0.10178649425506592\n",
      "step:147 loss:0.1251024454832077\n",
      "step:148 loss:0.15497085452079773\n",
      "step:149 loss:0.1352386474609375\n",
      "step:150 loss:0.166733518242836\n",
      "step:151 loss:0.11922583729028702\n",
      "step:152 loss:0.2195684164762497\n",
      "step:153 loss:0.17861713469028473\n",
      "step:154 loss:0.15604479610919952\n",
      "step:155 loss:0.1794602870941162\n",
      "step:156 loss:0.19831304252147675\n",
      "step:157 loss:0.03661292418837547\n",
      "step:158 loss:0.19661211967468262\n",
      "step:159 loss:0.19836759567260742\n",
      "step:160 loss:0.4036670923233032\n",
      "step:161 loss:0.25756072998046875\n",
      "step:162 loss:0.1436026394367218\n",
      "step:163 loss:0.1925051212310791\n",
      "step:164 loss:0.11577799171209335\n",
      "step:165 loss:0.15765124559402466\n",
      "step:166 loss:0.0966460332274437\n",
      "step:167 loss:0.07018736004829407\n",
      "step:168 loss:0.21343570947647095\n",
      "step:169 loss:0.21985195577144623\n",
      "step:170 loss:0.1470089703798294\n",
      "step:171 loss:0.12379684299230576\n",
      "step:172 loss:0.12577754259109497\n",
      "step:173 loss:0.12592971324920654\n",
      "step:174 loss:0.1486160010099411\n",
      "step:175 loss:0.16522832214832306\n",
      "step:176 loss:0.18947382271289825\n",
      "step:177 loss:0.06908714026212692\n",
      "step:178 loss:0.24684427678585052\n",
      "step:179 loss:0.11273326724767685\n",
      "step:180 loss:0.3018667697906494\n",
      "step:181 loss:0.18115223944187164\n",
      "step:182 loss:0.23449981212615967\n",
      "step:183 loss:0.2104567140340805\n",
      "step:184 loss:0.13532310724258423\n",
      "step:185 loss:0.22540248930454254\n",
      "step:186 loss:0.17076803743839264\n",
      "step:187 loss:0.16653594374656677\n",
      "step:188 loss:0.11017514765262604\n",
      "total_train_loss:0.16040097655569938, total_test_acc:0.6641626159964649\n",
      "------------------------------ \n",
      " epoch: 25\n",
      "step:0 loss:0.060713306069374084\n",
      "step:1 loss:0.057948339730501175\n",
      "step:2 loss:0.3201626241207123\n",
      "step:3 loss:0.07308118790388107\n",
      "step:4 loss:0.08855558186769485\n",
      "step:5 loss:0.11204572767019272\n",
      "step:6 loss:0.19341810047626495\n",
      "step:7 loss:0.18457680940628052\n",
      "step:8 loss:0.18231654167175293\n",
      "step:9 loss:0.3779003918170929\n",
      "step:10 loss:0.15284337103366852\n",
      "step:11 loss:0.23602110147476196\n",
      "step:12 loss:0.194189190864563\n",
      "step:13 loss:0.11708799004554749\n",
      "step:14 loss:0.14698030054569244\n",
      "step:15 loss:0.1142830178141594\n",
      "step:16 loss:0.10882782936096191\n",
      "step:17 loss:0.14710181951522827\n",
      "step:18 loss:0.24424344301223755\n",
      "step:19 loss:0.06639166921377182\n",
      "step:20 loss:0.1813286691904068\n",
      "step:21 loss:0.20090092718601227\n",
      "step:22 loss:0.11992451548576355\n",
      "step:23 loss:0.19583307206630707\n",
      "step:24 loss:0.2082691192626953\n",
      "step:25 loss:0.10597746819257736\n",
      "step:26 loss:0.07738020271062851\n",
      "step:27 loss:0.07402040809392929\n",
      "step:28 loss:0.14137639105319977\n",
      "step:29 loss:0.24449658393859863\n",
      "step:30 loss:0.28281542658805847\n",
      "step:31 loss:0.17372329533100128\n",
      "step:32 loss:0.15468423068523407\n",
      "step:33 loss:0.10450822860002518\n",
      "step:34 loss:0.18801657855510712\n",
      "step:35 loss:0.1267198771238327\n",
      "step:36 loss:0.08383583277463913\n",
      "step:37 loss:0.24580800533294678\n",
      "step:38 loss:0.046293798834085464\n",
      "step:39 loss:0.2442927360534668\n",
      "step:40 loss:0.17983855307102203\n",
      "step:41 loss:0.1216607391834259\n",
      "step:42 loss:0.04130399972200394\n",
      "step:43 loss:0.039948929101228714\n",
      "step:44 loss:0.062209855765104294\n",
      "step:45 loss:0.24245788156986237\n",
      "step:46 loss:0.1666119247674942\n",
      "step:47 loss:0.17798085510730743\n",
      "step:48 loss:0.302720308303833\n",
      "step:49 loss:0.37773799896240234\n",
      "step:50 loss:0.2395527958869934\n",
      "step:51 loss:0.18338952958583832\n",
      "step:52 loss:0.18795879185199738\n",
      "step:53 loss:0.2841890752315521\n",
      "step:54 loss:0.2962794303894043\n",
      "step:55 loss:0.19143664836883545\n",
      "step:56 loss:0.19498352706432343\n",
      "step:57 loss:0.19166964292526245\n",
      "step:58 loss:0.3724711835384369\n",
      "step:59 loss:0.14372748136520386\n",
      "step:60 loss:0.1876480132341385\n",
      "step:61 loss:0.14604757726192474\n",
      "step:62 loss:0.11575660109519958\n",
      "step:63 loss:0.140390083193779\n",
      "step:64 loss:0.2117409110069275\n",
      "step:65 loss:0.2577667236328125\n",
      "step:66 loss:0.08211006969213486\n",
      "step:67 loss:0.11285800486803055\n",
      "step:68 loss:0.13232369720935822\n",
      "step:69 loss:0.21790015697479248\n",
      "step:70 loss:0.1521577388048172\n",
      "step:71 loss:0.16287098824977875\n",
      "step:72 loss:0.07516570389270782\n",
      "step:73 loss:0.14229615032672882\n",
      "step:74 loss:0.15677280724048615\n",
      "step:75 loss:0.22167830169200897\n",
      "step:76 loss:0.17769885063171387\n",
      "step:77 loss:0.07285362482070923\n",
      "step:78 loss:0.12528641521930695\n",
      "step:79 loss:0.20936761796474457\n",
      "step:80 loss:0.0624542273581028\n",
      "step:81 loss:0.16579709947109222\n",
      "step:82 loss:0.17584128677845\n",
      "step:83 loss:0.11220976710319519\n",
      "step:84 loss:0.11435193568468094\n",
      "step:85 loss:0.10416244715452194\n",
      "step:86 loss:0.09252318739891052\n",
      "step:87 loss:0.13643209636211395\n",
      "step:88 loss:0.07947086542844772\n",
      "step:89 loss:0.06736884266138077\n",
      "step:90 loss:0.11823412030935287\n",
      "step:91 loss:0.17931623756885529\n",
      "step:92 loss:0.10513292998075485\n",
      "step:93 loss:0.1919141411781311\n",
      "step:94 loss:0.2460346817970276\n",
      "step:95 loss:0.03423988074064255\n",
      "step:96 loss:0.2084301859140396\n",
      "step:97 loss:0.142759770154953\n",
      "step:98 loss:0.14024576544761658\n",
      "step:99 loss:0.2804499566555023\n",
      "step:100 loss:0.058640316128730774\n",
      "step:101 loss:0.05950058624148369\n",
      "step:102 loss:0.15812070667743683\n",
      "step:103 loss:0.12245259433984756\n",
      "step:104 loss:0.07189038395881653\n",
      "step:105 loss:0.07560913264751434\n",
      "step:106 loss:0.10516461730003357\n",
      "step:107 loss:0.19448478519916534\n",
      "step:108 loss:0.15135546028614044\n",
      "step:109 loss:0.08394672721624374\n",
      "step:110 loss:0.21817970275878906\n",
      "step:111 loss:0.11716148257255554\n",
      "step:112 loss:0.13624893128871918\n",
      "step:113 loss:0.21139083802700043\n",
      "step:114 loss:0.164345845580101\n",
      "step:115 loss:0.24724702537059784\n",
      "step:116 loss:0.20283637940883636\n",
      "step:117 loss:0.271465927362442\n",
      "step:118 loss:0.17244188487529755\n",
      "step:119 loss:0.14815251529216766\n",
      "step:120 loss:0.13508465886116028\n",
      "step:121 loss:0.035149212926626205\n",
      "step:122 loss:0.1148170530796051\n",
      "step:123 loss:0.12345244735479355\n",
      "step:124 loss:0.12075125426054001\n",
      "step:125 loss:0.1450762152671814\n",
      "step:126 loss:0.19617736339569092\n",
      "step:127 loss:0.27862271666526794\n",
      "step:128 loss:0.22938640415668488\n",
      "step:129 loss:0.10107948631048203\n",
      "step:130 loss:0.24017590284347534\n",
      "step:131 loss:0.17207634449005127\n",
      "step:132 loss:0.14217647910118103\n",
      "step:133 loss:0.13704092800617218\n",
      "step:134 loss:0.1967661827802658\n",
      "step:135 loss:0.11729076504707336\n",
      "step:136 loss:0.08042003214359283\n",
      "step:137 loss:0.086161769926548\n",
      "step:138 loss:0.10176100581884384\n",
      "step:139 loss:0.37690767645835876\n",
      "step:140 loss:0.13517142832279205\n",
      "step:141 loss:0.09248317033052444\n",
      "step:142 loss:0.20007939636707306\n",
      "step:143 loss:0.09547889977693558\n",
      "step:144 loss:0.03407518193125725\n",
      "step:145 loss:0.07530762255191803\n",
      "step:146 loss:0.16682030260562897\n",
      "step:147 loss:0.34728169441223145\n",
      "step:148 loss:0.20248864591121674\n",
      "step:149 loss:0.15802548825740814\n",
      "step:150 loss:0.1343221664428711\n",
      "step:151 loss:0.08470480889081955\n",
      "step:152 loss:0.21586759388446808\n",
      "step:153 loss:0.1398567408323288\n",
      "step:154 loss:0.17186444997787476\n",
      "step:155 loss:0.18166100978851318\n",
      "step:156 loss:0.16507652401924133\n",
      "step:157 loss:0.15502212941646576\n",
      "step:158 loss:0.1577584147453308\n",
      "step:159 loss:0.16484074294567108\n",
      "step:160 loss:0.1399892121553421\n",
      "step:161 loss:0.14288842678070068\n",
      "step:162 loss:0.16506144404411316\n",
      "step:163 loss:0.15477660298347473\n",
      "step:164 loss:0.11048009246587753\n",
      "step:165 loss:0.08131688088178635\n",
      "step:166 loss:0.1552858203649521\n",
      "step:167 loss:0.13894771039485931\n",
      "step:168 loss:0.17954343557357788\n",
      "step:169 loss:0.1176246777176857\n",
      "step:170 loss:0.0656464621424675\n",
      "step:171 loss:0.1583307534456253\n",
      "step:172 loss:0.22630728781223297\n",
      "step:173 loss:0.12088358402252197\n",
      "step:174 loss:0.07071299105882645\n",
      "step:175 loss:0.14601142704486847\n",
      "step:176 loss:0.08794156461954117\n",
      "step:177 loss:0.15699303150177002\n",
      "step:178 loss:0.14946742355823517\n",
      "step:179 loss:0.24415016174316406\n",
      "step:180 loss:0.1299332231283188\n",
      "step:181 loss:0.25163182616233826\n",
      "step:182 loss:0.11944060772657394\n",
      "step:183 loss:0.34126296639442444\n",
      "step:184 loss:0.06495270878076553\n",
      "step:185 loss:0.14416854083538055\n",
      "step:186 loss:0.10291504859924316\n",
      "step:187 loss:0.24005991220474243\n",
      "step:188 loss:0.21646849811077118\n",
      "total_train_loss:0.15870995106215172, total_test_acc:0.8908528501988511\n",
      "------------------------------ \n",
      " epoch: 26\n",
      "step:0 loss:0.06107824668288231\n",
      "step:1 loss:0.10767772048711777\n",
      "step:2 loss:0.17842011153697968\n",
      "step:3 loss:0.1523655354976654\n",
      "step:4 loss:0.17087142169475555\n",
      "step:5 loss:0.05749811604619026\n",
      "step:6 loss:0.2585400640964508\n",
      "step:7 loss:0.12535257637500763\n",
      "step:8 loss:0.1601426899433136\n",
      "step:9 loss:0.06606724113225937\n",
      "step:10 loss:0.09550147503614426\n",
      "step:11 loss:0.04228624701499939\n",
      "step:12 loss:0.16344596445560455\n",
      "step:13 loss:0.11284702271223068\n",
      "step:14 loss:0.08342645317316055\n",
      "step:15 loss:0.13254541158676147\n",
      "step:16 loss:0.22759373486042023\n",
      "step:17 loss:0.10854742676019669\n",
      "step:18 loss:0.12273753434419632\n",
      "step:19 loss:0.17898361384868622\n",
      "step:20 loss:0.24956955015659332\n",
      "step:21 loss:0.09389820694923401\n",
      "step:22 loss:0.2232617288827896\n",
      "step:23 loss:0.10173419117927551\n",
      "step:24 loss:0.17102433741092682\n",
      "step:25 loss:0.21476171910762787\n",
      "step:26 loss:0.19508326053619385\n",
      "step:27 loss:0.08840811252593994\n",
      "step:28 loss:0.14790314435958862\n",
      "step:29 loss:0.10609535127878189\n",
      "step:30 loss:0.13037808239459991\n",
      "step:31 loss:0.21262477338314056\n",
      "step:32 loss:0.10883960872888565\n",
      "step:33 loss:0.1919635534286499\n",
      "step:34 loss:0.17295204102993011\n",
      "step:35 loss:0.09910014271736145\n",
      "step:36 loss:0.1919432133436203\n",
      "step:37 loss:0.15431977808475494\n",
      "step:38 loss:0.1306527853012085\n",
      "step:39 loss:0.06716997921466827\n",
      "step:40 loss:0.05024753510951996\n",
      "step:41 loss:0.13989003002643585\n",
      "step:42 loss:0.24187785387039185\n",
      "step:43 loss:0.10656440258026123\n",
      "step:44 loss:0.1595192402601242\n",
      "step:45 loss:0.07450435310602188\n",
      "step:46 loss:0.18561847507953644\n",
      "step:47 loss:0.03724963590502739\n",
      "step:48 loss:0.17209278047084808\n",
      "step:49 loss:0.2948492467403412\n",
      "step:50 loss:0.14613094925880432\n",
      "step:51 loss:0.10259122401475906\n",
      "step:52 loss:0.1138140857219696\n",
      "step:53 loss:0.08886256814002991\n",
      "step:54 loss:0.12296361476182938\n",
      "step:55 loss:0.0423889122903347\n",
      "step:56 loss:0.1222539022564888\n",
      "step:57 loss:0.16630342602729797\n",
      "step:58 loss:0.10790649801492691\n",
      "step:59 loss:0.11384064704179764\n",
      "step:60 loss:0.032855018973350525\n",
      "step:61 loss:0.2630927562713623\n",
      "step:62 loss:0.24574798345565796\n",
      "step:63 loss:0.09896595031023026\n",
      "step:64 loss:0.17607778310775757\n",
      "step:65 loss:0.23176872730255127\n",
      "step:66 loss:0.16279925405979156\n",
      "step:67 loss:0.09315177798271179\n",
      "step:68 loss:0.2735258638858795\n",
      "step:69 loss:0.03235013782978058\n",
      "step:70 loss:0.14448316395282745\n",
      "step:71 loss:0.2048882395029068\n",
      "step:72 loss:0.02915920875966549\n",
      "step:73 loss:0.08404495567083359\n",
      "step:74 loss:0.1412382870912552\n",
      "step:75 loss:0.034096185117959976\n",
      "step:76 loss:0.07021155208349228\n",
      "step:77 loss:0.1590857058763504\n",
      "step:78 loss:0.10642541199922562\n",
      "step:79 loss:0.08634823560714722\n",
      "step:80 loss:0.06439953297376633\n",
      "step:81 loss:0.05873756483197212\n",
      "step:82 loss:0.2555845081806183\n",
      "step:83 loss:0.1239941343665123\n",
      "step:84 loss:0.11522188782691956\n",
      "step:85 loss:0.10901334136724472\n",
      "step:86 loss:0.11770185828208923\n",
      "step:87 loss:0.14302080869674683\n",
      "step:88 loss:0.12182572484016418\n",
      "step:89 loss:0.12609367072582245\n",
      "step:90 loss:0.10860822349786758\n",
      "step:91 loss:0.06928911060094833\n",
      "step:92 loss:0.08437377959489822\n",
      "step:93 loss:0.19390511512756348\n",
      "step:94 loss:0.2059016227722168\n",
      "step:95 loss:0.11903264373540878\n",
      "step:96 loss:0.2573423385620117\n",
      "step:97 loss:0.08599529415369034\n",
      "step:98 loss:0.13072794675827026\n",
      "step:99 loss:0.08709967881441116\n",
      "step:100 loss:0.11237385123968124\n",
      "step:101 loss:0.21328844130039215\n",
      "step:102 loss:0.2223428636789322\n",
      "step:103 loss:0.09711983054876328\n",
      "step:104 loss:0.09126222878694534\n",
      "step:105 loss:0.19279539585113525\n",
      "step:106 loss:0.024328581988811493\n",
      "step:107 loss:0.043882325291633606\n",
      "step:108 loss:0.10286123305559158\n",
      "step:109 loss:0.1320217251777649\n",
      "step:110 loss:0.09321638941764832\n",
      "step:111 loss:0.15009205043315887\n",
      "step:112 loss:0.1707324981689453\n",
      "step:113 loss:0.30340567231178284\n",
      "step:114 loss:0.22113056480884552\n",
      "step:115 loss:0.3326149880886078\n",
      "step:116 loss:0.1522655487060547\n",
      "step:117 loss:0.038120005279779434\n",
      "step:118 loss:0.11964932829141617\n",
      "step:119 loss:0.12196404486894608\n",
      "step:120 loss:0.11691556125879288\n",
      "step:121 loss:0.23939509689807892\n",
      "step:122 loss:0.08020299673080444\n",
      "step:123 loss:0.22608397901058197\n",
      "step:124 loss:0.19130776822566986\n",
      "step:125 loss:0.11387670040130615\n",
      "step:126 loss:0.2196670025587082\n",
      "step:127 loss:0.34022989869117737\n",
      "step:128 loss:0.30676159262657166\n",
      "step:129 loss:0.09683459252119064\n",
      "step:130 loss:0.16695934534072876\n",
      "step:131 loss:0.08097868412733078\n",
      "step:132 loss:0.17808479070663452\n",
      "step:133 loss:0.16756552457809448\n",
      "step:134 loss:0.22023789584636688\n",
      "step:135 loss:0.1611485481262207\n",
      "step:136 loss:0.26322242617607117\n",
      "step:137 loss:0.111164890229702\n",
      "step:138 loss:0.07011479139328003\n",
      "step:139 loss:0.16066035628318787\n",
      "step:140 loss:0.20246361196041107\n",
      "step:141 loss:0.24953949451446533\n",
      "step:142 loss:0.09869605302810669\n",
      "step:143 loss:0.06588023900985718\n",
      "step:144 loss:0.19450032711029053\n",
      "step:145 loss:0.14396946132183075\n",
      "step:146 loss:0.09467009454965591\n",
      "step:147 loss:0.1649235635995865\n",
      "step:148 loss:0.07998679578304291\n",
      "step:149 loss:0.05580800399184227\n",
      "step:150 loss:0.18815253674983978\n",
      "step:151 loss:0.28940922021865845\n",
      "step:152 loss:0.10538040846586227\n",
      "step:153 loss:0.09442561864852905\n",
      "step:154 loss:0.061286598443984985\n",
      "step:155 loss:0.1404370814561844\n",
      "step:156 loss:0.1502818912267685\n",
      "step:157 loss:0.3092443645000458\n",
      "step:158 loss:0.05997077748179436\n",
      "step:159 loss:0.07159185409545898\n",
      "step:160 loss:0.1343335211277008\n",
      "step:161 loss:0.15748490393161774\n",
      "step:162 loss:0.21731030941009521\n",
      "step:163 loss:0.15797506272792816\n",
      "step:164 loss:0.34258028864860535\n",
      "step:165 loss:0.20416206121444702\n",
      "step:166 loss:0.17656511068344116\n",
      "step:167 loss:0.3250344693660736\n",
      "step:168 loss:0.2632571756839752\n",
      "step:169 loss:0.37475502490997314\n",
      "step:170 loss:0.10728459805250168\n",
      "step:171 loss:0.11824682354927063\n",
      "step:172 loss:0.30362468957901\n",
      "step:173 loss:0.1624264121055603\n",
      "step:174 loss:0.12307668477296829\n",
      "step:175 loss:0.27256402373313904\n",
      "step:176 loss:0.15096507966518402\n",
      "step:177 loss:0.20022577047348022\n",
      "step:178 loss:0.17789267003536224\n",
      "step:179 loss:0.05445946753025055\n",
      "step:180 loss:0.19076712429523468\n",
      "step:181 loss:0.07381993532180786\n",
      "step:182 loss:0.191750630736351\n",
      "step:183 loss:0.20855577290058136\n",
      "step:184 loss:0.19320780038833618\n",
      "step:185 loss:0.18902848660945892\n",
      "step:186 loss:0.38723668456077576\n",
      "step:187 loss:0.0737895667552948\n",
      "step:188 loss:0.1701394021511078\n",
      "total_train_loss:0.15141108557780056, total_test_acc:0.7812638091029607\n",
      "------------------------------ \n",
      " epoch: 27\n",
      "step:0 loss:0.03306765481829643\n",
      "step:1 loss:0.2390243411064148\n",
      "step:2 loss:0.30644670128822327\n",
      "step:3 loss:0.07017675787210464\n",
      "step:4 loss:0.16876022517681122\n",
      "step:5 loss:0.23156563937664032\n",
      "step:6 loss:0.20231883227825165\n",
      "step:7 loss:0.14803560078144073\n",
      "step:8 loss:0.11215100437402725\n",
      "step:9 loss:0.08404844254255295\n",
      "step:10 loss:0.16623617708683014\n",
      "step:11 loss:0.18979237973690033\n",
      "step:12 loss:0.17424128949642181\n",
      "step:13 loss:0.15309204161167145\n",
      "step:14 loss:0.15082475543022156\n",
      "step:15 loss:0.1468343287706375\n",
      "step:16 loss:0.2221304178237915\n",
      "step:17 loss:0.11924289911985397\n",
      "step:18 loss:0.0877731665968895\n",
      "step:19 loss:0.1332009881734848\n",
      "step:20 loss:0.04148712754249573\n",
      "step:21 loss:0.14459200203418732\n",
      "step:22 loss:0.3431185185909271\n",
      "step:23 loss:0.08372387290000916\n",
      "step:24 loss:0.21028675138950348\n",
      "step:25 loss:0.18764890730381012\n",
      "step:26 loss:0.21101485192775726\n",
      "step:27 loss:0.12123597413301468\n",
      "step:28 loss:0.1420273780822754\n",
      "step:29 loss:0.08634406328201294\n",
      "step:30 loss:0.2863655090332031\n",
      "step:31 loss:0.2711184620857239\n",
      "step:32 loss:0.1614355891942978\n",
      "step:33 loss:0.1867953985929489\n",
      "step:34 loss:0.23536016047000885\n",
      "step:35 loss:0.1260215789079666\n",
      "step:36 loss:0.1490144580602646\n",
      "step:37 loss:0.11367741227149963\n",
      "step:38 loss:0.2023286670446396\n",
      "step:39 loss:0.12097813934087753\n",
      "step:40 loss:0.19058363139629364\n",
      "step:41 loss:0.08791738748550415\n",
      "step:42 loss:0.1707959771156311\n",
      "step:43 loss:0.101945661008358\n",
      "step:44 loss:0.14101891219615936\n",
      "step:45 loss:0.15463754534721375\n",
      "step:46 loss:0.12686796486377716\n",
      "step:47 loss:0.2789056897163391\n",
      "step:48 loss:0.07933729141950607\n",
      "step:49 loss:0.11733606457710266\n",
      "step:50 loss:0.12871520221233368\n",
      "step:51 loss:0.09141749143600464\n",
      "step:52 loss:0.11453628540039062\n",
      "step:53 loss:0.15431243181228638\n",
      "step:54 loss:0.07795820385217667\n",
      "step:55 loss:0.07006175071001053\n",
      "step:56 loss:0.14944744110107422\n",
      "step:57 loss:0.31113600730895996\n",
      "step:58 loss:0.13219448924064636\n",
      "step:59 loss:0.14454686641693115\n",
      "step:60 loss:0.11996490508317947\n",
      "step:61 loss:0.1505698710680008\n",
      "step:62 loss:0.2560836970806122\n",
      "step:63 loss:0.14736051857471466\n",
      "step:64 loss:0.18492673337459564\n",
      "step:65 loss:0.13303497433662415\n",
      "step:66 loss:0.1379440873861313\n",
      "step:67 loss:0.06236317381262779\n",
      "step:68 loss:0.12540598213672638\n",
      "step:69 loss:0.07931958884000778\n",
      "step:70 loss:0.10011477023363113\n",
      "step:71 loss:0.26542526483535767\n",
      "step:72 loss:0.08205937594175339\n",
      "step:73 loss:0.057147517800331116\n",
      "step:74 loss:0.25516027212142944\n",
      "step:75 loss:0.2308022528886795\n",
      "step:76 loss:0.13006038963794708\n",
      "step:77 loss:0.12328273057937622\n",
      "step:78 loss:0.1238240897655487\n",
      "step:79 loss:0.09500662237405777\n",
      "step:80 loss:0.14502479135990143\n",
      "step:81 loss:0.1932726949453354\n",
      "step:82 loss:0.11019141227006912\n",
      "step:83 loss:0.11464866250753403\n",
      "step:84 loss:0.20502211153507233\n",
      "step:85 loss:0.1100965216755867\n",
      "step:86 loss:0.08424850553274155\n",
      "step:87 loss:0.17120365798473358\n",
      "step:88 loss:0.051938947290182114\n",
      "step:89 loss:0.12738217413425446\n",
      "step:90 loss:0.24165527522563934\n",
      "step:91 loss:0.0713147446513176\n",
      "step:92 loss:0.1262815147638321\n",
      "step:93 loss:0.06769094616174698\n",
      "step:94 loss:0.07032265514135361\n",
      "step:95 loss:0.14216382801532745\n",
      "step:96 loss:0.26181885600090027\n",
      "step:97 loss:0.11974338442087173\n",
      "step:98 loss:0.18323880434036255\n",
      "step:99 loss:0.15161921083927155\n",
      "step:100 loss:0.059752900153398514\n",
      "step:101 loss:0.13029704988002777\n",
      "step:102 loss:0.14498668909072876\n",
      "step:103 loss:0.0961460992693901\n",
      "step:104 loss:0.15926562249660492\n",
      "step:105 loss:0.2824566662311554\n",
      "step:106 loss:0.12942908704280853\n",
      "step:107 loss:0.2806929051876068\n",
      "step:108 loss:0.12523742020130157\n",
      "step:109 loss:0.2767655551433563\n",
      "step:110 loss:0.15128006041049957\n",
      "step:111 loss:0.055711861699819565\n",
      "step:112 loss:0.029456989839673042\n",
      "step:113 loss:0.13554039597511292\n",
      "step:114 loss:0.2946440875530243\n",
      "step:115 loss:0.12264595180749893\n",
      "step:116 loss:0.07928339391946793\n",
      "step:117 loss:0.17331261932849884\n",
      "step:118 loss:0.15952229499816895\n",
      "step:119 loss:0.18687398731708527\n",
      "step:120 loss:0.18023858964443207\n",
      "step:121 loss:0.2931000292301178\n",
      "step:122 loss:0.2775462567806244\n",
      "step:123 loss:0.14110352098941803\n",
      "step:124 loss:0.1236497238278389\n",
      "step:125 loss:0.12645690143108368\n",
      "step:126 loss:0.31753769516944885\n",
      "step:127 loss:0.232474222779274\n",
      "step:128 loss:0.12197732925415039\n",
      "step:129 loss:0.20031340420246124\n",
      "step:130 loss:0.2220180481672287\n",
      "step:131 loss:0.11993954330682755\n",
      "step:132 loss:0.051382917910814285\n",
      "step:133 loss:0.18418549001216888\n",
      "step:134 loss:0.2148085981607437\n",
      "step:135 loss:0.09213069826364517\n",
      "step:136 loss:0.2884719669818878\n",
      "step:137 loss:0.23709648847579956\n",
      "step:138 loss:0.11859020590782166\n",
      "step:139 loss:0.05904625356197357\n",
      "step:140 loss:0.17245160043239594\n",
      "step:141 loss:0.203849658370018\n",
      "step:142 loss:0.1708703190088272\n",
      "step:143 loss:0.08733251690864563\n",
      "step:144 loss:0.11166510730981827\n",
      "step:145 loss:0.16046534478664398\n",
      "step:146 loss:0.18215887248516083\n",
      "step:147 loss:0.2378232330083847\n",
      "step:148 loss:0.17699570953845978\n",
      "step:149 loss:0.0736013874411583\n",
      "step:150 loss:0.19323718547821045\n",
      "step:151 loss:0.07803899794816971\n",
      "step:152 loss:0.055529043078422546\n",
      "step:153 loss:0.08298506587743759\n",
      "step:154 loss:0.07291648536920547\n",
      "step:155 loss:0.2333323359489441\n",
      "step:156 loss:0.15189601480960846\n",
      "step:157 loss:0.19353246688842773\n",
      "step:158 loss:0.23399527370929718\n",
      "step:159 loss:0.06233471259474754\n",
      "step:160 loss:0.18971967697143555\n",
      "step:161 loss:0.1273781806230545\n",
      "step:162 loss:0.12403452396392822\n",
      "step:163 loss:0.1693384051322937\n",
      "step:164 loss:0.019336987286806107\n",
      "step:165 loss:0.09302925318479538\n",
      "step:166 loss:0.05221753194928169\n",
      "step:167 loss:0.11129219084978104\n",
      "step:168 loss:0.09369628876447678\n",
      "step:169 loss:0.08236078172922134\n",
      "step:170 loss:0.05089579522609711\n",
      "step:171 loss:0.119942307472229\n",
      "step:172 loss:0.1051434576511383\n",
      "step:173 loss:0.15744538605213165\n",
      "step:174 loss:0.05181075260043144\n",
      "step:175 loss:0.13472548127174377\n",
      "step:176 loss:0.09834277629852295\n",
      "step:177 loss:0.12997525930404663\n",
      "step:178 loss:0.14020253717899323\n",
      "step:179 loss:0.1230001375079155\n",
      "step:180 loss:0.1879623532295227\n",
      "step:181 loss:0.09310287982225418\n",
      "step:182 loss:0.04033597931265831\n",
      "step:183 loss:0.15975363552570343\n",
      "step:184 loss:0.21353565156459808\n",
      "step:185 loss:0.2482544183731079\n",
      "step:186 loss:0.11956004053354263\n",
      "step:187 loss:0.2449054718017578\n",
      "step:188 loss:0.050873272120952606\n",
      "total_train_loss:0.14967286458952314, total_test_acc:0.860362350861688\n",
      "------------------------------ \n",
      " epoch: 28\n",
      "step:0 loss:0.1336311250925064\n",
      "step:1 loss:0.1500123292207718\n",
      "step:2 loss:0.059506941586732864\n",
      "step:3 loss:0.061392705887556076\n",
      "step:4 loss:0.14848127961158752\n",
      "step:5 loss:0.09624017030000687\n",
      "step:6 loss:0.22408747673034668\n",
      "step:7 loss:0.01058692391961813\n",
      "step:8 loss:0.05563525855541229\n",
      "step:9 loss:0.12489483505487442\n",
      "step:10 loss:0.1764969825744629\n",
      "step:11 loss:0.10840871185064316\n",
      "step:12 loss:0.11869612336158752\n",
      "step:13 loss:0.02916105091571808\n",
      "step:14 loss:0.24193204939365387\n",
      "step:15 loss:0.08454675227403641\n",
      "step:16 loss:0.25674229860305786\n",
      "step:17 loss:0.17434799671173096\n",
      "step:18 loss:0.3241212069988251\n",
      "step:19 loss:0.16119535267353058\n",
      "step:20 loss:0.1146000325679779\n",
      "step:21 loss:0.15659159421920776\n",
      "step:22 loss:0.0847940668463707\n",
      "step:23 loss:0.19917184114456177\n",
      "step:24 loss:0.11673667281866074\n",
      "step:25 loss:0.07583801448345184\n",
      "step:26 loss:0.21039485931396484\n",
      "step:27 loss:0.20382462441921234\n",
      "step:28 loss:0.20920707285404205\n",
      "step:29 loss:0.1754959225654602\n",
      "step:30 loss:0.07760152965784073\n",
      "step:31 loss:0.07564049959182739\n",
      "step:32 loss:0.06230676546692848\n",
      "step:33 loss:0.0774245634675026\n",
      "step:34 loss:0.17448200285434723\n",
      "step:35 loss:0.1299169808626175\n",
      "step:36 loss:0.08094392716884613\n",
      "step:37 loss:0.14218103885650635\n",
      "step:38 loss:0.16343383491039276\n",
      "step:39 loss:0.057673219591379166\n",
      "step:40 loss:0.24498702585697174\n",
      "step:41 loss:0.2524760067462921\n",
      "step:42 loss:0.23012734949588776\n",
      "step:43 loss:0.1557675153017044\n",
      "step:44 loss:0.16728486120700836\n",
      "step:45 loss:0.21083612740039825\n",
      "step:46 loss:0.11149457097053528\n",
      "step:47 loss:0.11248306185007095\n",
      "step:48 loss:0.1408015489578247\n",
      "step:49 loss:0.20448805391788483\n",
      "step:50 loss:0.06618247181177139\n",
      "step:51 loss:0.163950115442276\n",
      "step:52 loss:0.10471384972333908\n",
      "step:53 loss:0.052525851875543594\n",
      "step:54 loss:0.15672940015792847\n",
      "step:55 loss:0.11263688653707504\n",
      "step:56 loss:0.2239772528409958\n",
      "step:57 loss:0.08333507925271988\n",
      "step:58 loss:0.1444963812828064\n",
      "step:59 loss:0.15705977380275726\n",
      "step:60 loss:0.14335580170154572\n",
      "step:61 loss:0.1502377837896347\n",
      "step:62 loss:0.18165336549282074\n",
      "step:63 loss:0.10022830963134766\n",
      "step:64 loss:0.05960138514637947\n",
      "step:65 loss:0.3627246618270874\n",
      "step:66 loss:0.21639291942119598\n",
      "step:67 loss:0.06769586354494095\n",
      "step:68 loss:0.13089148700237274\n",
      "step:69 loss:0.0813341736793518\n",
      "step:70 loss:0.15874123573303223\n",
      "step:71 loss:0.23825740814208984\n",
      "step:72 loss:0.2799678146839142\n",
      "step:73 loss:0.1698913425207138\n",
      "step:74 loss:0.2204989641904831\n",
      "step:75 loss:0.10631942003965378\n",
      "step:76 loss:0.34449052810668945\n",
      "step:77 loss:0.11338231712579727\n",
      "step:78 loss:0.08538541197776794\n",
      "step:79 loss:0.17788220942020416\n",
      "step:80 loss:0.1173907220363617\n",
      "step:81 loss:0.07686196267604828\n",
      "step:82 loss:0.18935273587703705\n",
      "step:83 loss:0.20862452685832977\n",
      "step:84 loss:0.15656638145446777\n",
      "step:85 loss:0.09546098113059998\n",
      "step:86 loss:0.12421530485153198\n",
      "step:87 loss:0.16544798016548157\n",
      "step:88 loss:0.07889590412378311\n",
      "step:89 loss:0.10836001485586166\n",
      "step:90 loss:0.1891002506017685\n",
      "step:91 loss:0.12371773272752762\n",
      "step:92 loss:0.11419019848108292\n",
      "step:93 loss:0.12051670998334885\n",
      "step:94 loss:0.08791924268007278\n",
      "step:95 loss:0.16678093373775482\n",
      "step:96 loss:0.06328480690717697\n",
      "step:97 loss:0.04623739793896675\n",
      "step:98 loss:0.23614205420017242\n",
      "step:99 loss:0.13337518274784088\n",
      "step:100 loss:0.07872756570577621\n",
      "step:101 loss:0.10000760108232498\n",
      "step:102 loss:0.15091601014137268\n",
      "step:103 loss:0.10894877463579178\n",
      "step:104 loss:0.1802406907081604\n",
      "step:105 loss:0.14137153327465057\n",
      "step:106 loss:0.10554352402687073\n",
      "step:107 loss:0.10792940855026245\n",
      "step:108 loss:0.14989914000034332\n",
      "step:109 loss:0.0884583368897438\n",
      "step:110 loss:0.10832402110099792\n",
      "step:111 loss:0.11602454632520676\n",
      "step:112 loss:0.15419228374958038\n",
      "step:113 loss:0.17753897607326508\n",
      "step:114 loss:0.10215098410844803\n",
      "step:115 loss:0.14899803698062897\n",
      "step:116 loss:0.14983822405338287\n",
      "step:117 loss:0.2343296855688095\n",
      "step:118 loss:0.20424775779247284\n",
      "step:119 loss:0.06268136948347092\n",
      "step:120 loss:0.060045838356018066\n",
      "step:121 loss:0.04892103374004364\n",
      "step:122 loss:0.21299390494823456\n",
      "step:123 loss:0.1473943591117859\n",
      "step:124 loss:0.23204205930233002\n",
      "step:125 loss:0.03461853042244911\n",
      "step:126 loss:0.18645435571670532\n",
      "step:127 loss:0.07162796705961227\n",
      "step:128 loss:0.13023953139781952\n",
      "step:129 loss:0.20060020685195923\n",
      "step:130 loss:0.1881924271583557\n",
      "step:131 loss:0.05940580740571022\n",
      "step:132 loss:0.10595995932817459\n",
      "step:133 loss:0.07058946043252945\n",
      "step:134 loss:0.08763296157121658\n",
      "step:135 loss:0.14565102756023407\n",
      "step:136 loss:0.2040606290102005\n",
      "step:137 loss:0.05656793341040611\n",
      "step:138 loss:0.196771040558815\n",
      "step:139 loss:0.07542457431554794\n",
      "step:140 loss:0.059787947684526443\n",
      "step:141 loss:0.16362518072128296\n",
      "step:142 loss:0.11116380244493484\n",
      "step:143 loss:0.19024892151355743\n",
      "step:144 loss:0.06903352588415146\n",
      "step:145 loss:0.11480232328176498\n",
      "step:146 loss:0.06270385533571243\n",
      "step:147 loss:0.07481206208467484\n",
      "step:148 loss:0.13154320418834686\n",
      "step:149 loss:0.10290440171957016\n",
      "step:150 loss:0.09320486336946487\n",
      "step:151 loss:0.1284756064414978\n",
      "step:152 loss:0.05515766143798828\n",
      "step:153 loss:0.09404760599136353\n",
      "step:154 loss:0.2798304557800293\n",
      "step:155 loss:0.10119179636240005\n",
      "step:156 loss:0.12925998866558075\n",
      "step:157 loss:0.094931460916996\n",
      "step:158 loss:0.11753588914871216\n",
      "step:159 loss:0.11155668646097183\n",
      "step:160 loss:0.10821602493524551\n",
      "step:161 loss:0.11932247877120972\n",
      "step:162 loss:0.10943850874900818\n",
      "step:163 loss:0.09779298305511475\n",
      "step:164 loss:0.020421599969267845\n",
      "step:165 loss:0.08995485305786133\n",
      "step:166 loss:0.065902478992939\n",
      "step:167 loss:0.06272817403078079\n",
      "step:168 loss:0.13514213263988495\n",
      "step:169 loss:0.038428809493780136\n",
      "step:170 loss:0.1662294715642929\n",
      "step:171 loss:0.18531042337417603\n",
      "step:172 loss:0.24414557218551636\n",
      "step:173 loss:0.08279599994421005\n",
      "step:174 loss:0.17259247601032257\n",
      "step:175 loss:0.13809345662593842\n",
      "step:176 loss:0.1702210158109665\n",
      "step:177 loss:0.08803347498178482\n",
      "step:178 loss:0.06362113356590271\n",
      "step:179 loss:0.2307499647140503\n",
      "step:180 loss:0.2907842695713043\n",
      "step:181 loss:0.16071301698684692\n",
      "step:182 loss:0.18192356824874878\n",
      "step:183 loss:0.14521032571792603\n",
      "step:184 loss:0.12300416082143784\n",
      "step:185 loss:0.10484468191862106\n",
      "step:186 loss:0.1190374568104744\n",
      "step:187 loss:0.1501368135213852\n",
      "step:188 loss:0.10978221893310547\n",
      "total_train_loss:0.13650031972854854, total_test_acc:0.9248784798939461\n",
      "------------------------------ \n",
      " epoch: 29\n",
      "step:0 loss:0.08057234436273575\n",
      "step:1 loss:0.1564316600561142\n",
      "step:2 loss:0.14169731736183167\n",
      "step:3 loss:0.1439005732536316\n",
      "step:4 loss:0.09037434309720993\n",
      "step:5 loss:0.1641119420528412\n",
      "step:6 loss:0.16466952860355377\n",
      "step:7 loss:0.18098761141300201\n",
      "step:8 loss:0.07015503197908401\n",
      "step:9 loss:0.08454439789056778\n",
      "step:10 loss:0.07727969437837601\n",
      "step:11 loss:0.1319321244955063\n",
      "step:12 loss:0.3031133711338043\n",
      "step:13 loss:0.08125630766153336\n",
      "step:14 loss:0.10735460370779037\n",
      "step:15 loss:0.05976074934005737\n",
      "step:16 loss:0.149856835603714\n",
      "step:17 loss:0.24336381256580353\n",
      "step:18 loss:0.1092008575797081\n",
      "step:19 loss:0.10225928574800491\n",
      "step:20 loss:0.08682187646627426\n",
      "step:21 loss:0.1034141555428505\n",
      "step:22 loss:0.055044498294591904\n",
      "step:23 loss:0.06907926499843597\n",
      "step:24 loss:0.14600640535354614\n",
      "step:25 loss:0.09997141361236572\n",
      "step:26 loss:0.05757971480488777\n",
      "step:27 loss:0.21801801025867462\n",
      "step:28 loss:0.217115581035614\n",
      "step:29 loss:0.12429553270339966\n",
      "step:30 loss:0.10145468264818192\n",
      "step:31 loss:0.2546970546245575\n",
      "step:32 loss:0.06789285689592361\n",
      "step:33 loss:0.3649878203868866\n",
      "step:34 loss:0.25291135907173157\n",
      "step:35 loss:0.36953362822532654\n",
      "step:36 loss:0.10445504635572433\n",
      "step:37 loss:0.1469423472881317\n",
      "step:38 loss:0.24248696863651276\n",
      "step:39 loss:0.1944451481103897\n",
      "step:40 loss:0.12005161494016647\n",
      "step:41 loss:0.17442606389522552\n",
      "step:42 loss:0.0890885666012764\n",
      "step:43 loss:0.39712080359458923\n",
      "step:44 loss:0.29732808470726013\n",
      "step:45 loss:0.10368906706571579\n",
      "step:46 loss:0.16241438686847687\n",
      "step:47 loss:0.12794612348079681\n",
      "step:48 loss:0.08536352962255478\n",
      "step:49 loss:0.07887066155672073\n",
      "step:50 loss:0.1440371870994568\n",
      "step:51 loss:0.11710930615663528\n",
      "step:52 loss:0.09359428286552429\n",
      "step:53 loss:0.10030820965766907\n",
      "step:54 loss:0.18055593967437744\n",
      "step:55 loss:0.24422045052051544\n",
      "step:56 loss:0.22274398803710938\n",
      "step:57 loss:0.14236131310462952\n",
      "step:58 loss:0.095704585313797\n",
      "step:59 loss:0.18391871452331543\n",
      "step:60 loss:0.26322200894355774\n",
      "step:61 loss:0.05969427153468132\n",
      "step:62 loss:0.1575898975133896\n",
      "step:63 loss:0.0835764929652214\n",
      "step:64 loss:0.0855853483080864\n",
      "step:65 loss:0.1422475129365921\n",
      "step:66 loss:0.2657719552516937\n",
      "step:67 loss:0.12152073532342911\n",
      "step:68 loss:0.21385477483272552\n",
      "step:69 loss:0.13402025401592255\n",
      "step:70 loss:0.08935073763132095\n",
      "step:71 loss:0.27081993222236633\n",
      "step:72 loss:0.10287857055664062\n",
      "step:73 loss:0.14127452671527863\n",
      "step:74 loss:0.09697077423334122\n",
      "step:75 loss:0.2788355350494385\n",
      "step:76 loss:0.15411145985126495\n",
      "step:77 loss:0.14640972018241882\n",
      "step:78 loss:0.0994473323225975\n",
      "step:79 loss:0.1455194503068924\n",
      "step:80 loss:0.1119130477309227\n",
      "step:81 loss:0.056214988231658936\n",
      "step:82 loss:0.12231660634279251\n",
      "step:83 loss:0.1277809888124466\n",
      "step:84 loss:0.13672471046447754\n",
      "step:85 loss:0.2294696718454361\n",
      "step:86 loss:0.15117961168289185\n",
      "step:87 loss:0.11217594146728516\n",
      "step:88 loss:0.13541370630264282\n",
      "step:89 loss:0.24320127069950104\n",
      "step:90 loss:0.07819081097841263\n",
      "step:91 loss:0.1728062480688095\n",
      "step:92 loss:0.17524679005146027\n",
      "step:93 loss:0.2660382091999054\n",
      "step:94 loss:0.07308071106672287\n",
      "step:95 loss:0.13095395267009735\n",
      "step:96 loss:0.16262950003147125\n",
      "step:97 loss:0.2999051511287689\n",
      "step:98 loss:0.3119146227836609\n",
      "step:99 loss:0.09527042508125305\n",
      "step:100 loss:0.09225853532552719\n",
      "step:101 loss:0.16391268372535706\n",
      "step:102 loss:0.1808345764875412\n",
      "step:103 loss:0.30912119150161743\n",
      "step:104 loss:0.17567051947116852\n",
      "step:105 loss:0.2173175811767578\n",
      "step:106 loss:0.13279637694358826\n",
      "step:107 loss:0.09087011218070984\n",
      "step:108 loss:0.07685016840696335\n",
      "step:109 loss:0.11089674383401871\n",
      "step:110 loss:0.15146504342556\n",
      "step:111 loss:0.17324751615524292\n",
      "step:112 loss:0.11632325500249863\n",
      "step:113 loss:0.08500930666923523\n",
      "step:114 loss:0.09370070695877075\n",
      "step:115 loss:0.09471231698989868\n",
      "step:116 loss:0.16462185978889465\n",
      "step:117 loss:0.11542002111673355\n",
      "step:118 loss:0.26923954486846924\n",
      "step:119 loss:0.20783384144306183\n",
      "step:120 loss:0.13892607390880585\n",
      "step:121 loss:0.10647094994783401\n",
      "step:122 loss:0.33898282051086426\n",
      "step:123 loss:0.029526570811867714\n",
      "step:124 loss:0.19061434268951416\n",
      "step:125 loss:0.15748876333236694\n",
      "step:126 loss:0.20113228261470795\n",
      "step:127 loss:0.14524567127227783\n",
      "step:128 loss:0.12486175447702408\n",
      "step:129 loss:0.07403863221406937\n",
      "step:130 loss:0.07427169382572174\n",
      "step:131 loss:0.21136391162872314\n",
      "step:132 loss:0.06408267468214035\n",
      "step:133 loss:0.1789378672838211\n",
      "step:134 loss:0.1302453726530075\n",
      "step:135 loss:0.2329624891281128\n",
      "step:136 loss:0.10242580622434616\n",
      "step:137 loss:0.20764295756816864\n",
      "step:138 loss:0.27059659361839294\n",
      "step:139 loss:0.13128885626792908\n",
      "step:140 loss:0.30941054224967957\n",
      "step:141 loss:0.14893120527267456\n",
      "step:142 loss:0.06658606976270676\n",
      "step:143 loss:0.19262678921222687\n",
      "step:144 loss:0.26435238122940063\n",
      "step:145 loss:0.17343445122241974\n",
      "step:146 loss:0.10935192555189133\n",
      "step:147 loss:0.09406346082687378\n",
      "step:148 loss:0.294124573469162\n",
      "step:149 loss:0.07992944866418839\n",
      "step:150 loss:0.16036348044872284\n",
      "step:151 loss:0.08666014671325684\n",
      "step:152 loss:0.10099034756422043\n",
      "step:153 loss:0.08037059009075165\n",
      "step:154 loss:0.05492275580763817\n",
      "step:155 loss:0.06448622792959213\n",
      "step:156 loss:0.17610429227352142\n",
      "step:157 loss:0.15774016082286835\n",
      "step:158 loss:0.17615456879138947\n",
      "step:159 loss:0.08047489076852798\n",
      "step:160 loss:0.06305154412984848\n",
      "step:161 loss:0.3022044897079468\n",
      "step:162 loss:0.04546153545379639\n",
      "step:163 loss:0.17089952528476715\n",
      "step:164 loss:0.1535855382680893\n",
      "step:165 loss:0.10035359859466553\n",
      "step:166 loss:0.06862261146306992\n",
      "step:167 loss:0.0654773935675621\n",
      "step:168 loss:0.11508189886808395\n",
      "step:169 loss:0.10848700255155563\n",
      "step:170 loss:0.05483800545334816\n",
      "step:171 loss:0.06516328454017639\n",
      "step:172 loss:0.06237078830599785\n",
      "step:173 loss:0.13792942464351654\n",
      "step:174 loss:0.14092622697353363\n",
      "step:175 loss:0.08947614580392838\n",
      "step:176 loss:0.1355385035276413\n",
      "step:177 loss:0.20080958306789398\n",
      "step:178 loss:0.054466694593429565\n",
      "step:179 loss:0.14277707040309906\n",
      "step:180 loss:0.06077998876571655\n",
      "step:181 loss:0.050410691648721695\n",
      "step:182 loss:0.06357463449239731\n",
      "step:183 loss:0.17545540630817413\n",
      "step:184 loss:0.12622056901454926\n",
      "step:185 loss:0.14970381557941437\n",
      "step:186 loss:0.14110110700130463\n",
      "step:187 loss:0.05375722423195839\n",
      "step:188 loss:0.09837672859430313\n",
      "total_train_loss:0.1464209798822536, total_test_acc:0.9575784357048166\n"
     ]
    }
   ],
   "source": [
    "epochs = 30 # 训练10轮次\n",
    "best_acc = 0.0 # 保存正确率最高的一次迭代的权重参数\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(oxhnet.parameters(), lr = 0.002)\n",
    "# -------------------------------------------------- #\n",
    "#（5）网络训练\n",
    "# -------------------------------------------------- #\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    print('-'*30, '\\n', 'epoch:', epoch)\n",
    "    \n",
    "    # 将模型设置为训练模型, dropout层和BN层只在训练时起作用\n",
    "    oxhnet.train()\n",
    "    \n",
    "    # 计算训练一个epoch的总损失\n",
    "    running_loss = 0.0\n",
    "    r_loss = 0.0\n",
    "    # 每个step训练一个batch\n",
    "    #\n",
    "    # 用其来代替train_loader实现每epoch重新数据强化：DataLoader(dataset=train_data, batch_size=48, shuffle=True, collate_fn=train_collate_fn, num_workers = 8)\n",
    "    for step, data in enumerate(DataLoader(dataset=train_data, batch_size=48, shuffle=True, collate_fn=train_collate_fn, num_workers = 8)):\n",
    "        # data中包含图像及其对应的标签\n",
    "        images, labels = data\n",
    "        \n",
    "        # 梯度清零，因为每次计算梯度是一个累加\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = oxhnet(images.to(device))\n",
    "        \n",
    "        # 计算预测值和真实值的交叉熵损失\n",
    "        loss = loss_function(outputs, labels.to(device))\n",
    "        \n",
    "        # 梯度计算\n",
    "        loss.backward()\n",
    "        \n",
    "        # 权重更新\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 累加每个step的损失\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # 打印每个step的损失\n",
    "        print(f'step:{step} loss:{loss}')\n",
    "\n",
    "        # 每个epoch写100次平均损失函数数值大小\n",
    "        r_loss += loss.item()\n",
    "        if (step+1)%10 == 0: # 100*10 datapoints\n",
    "            avg_loss = r_loss / 10\n",
    "            writer.add_scalar('training loss_10batchesPerPoint', avg_loss, epoch * len(train_loader) + step)\n",
    "            r_loss = 0.0\n",
    "    \n",
    "# -------------------------------------------------- #\n",
    "#（6）网络验证\n",
    "# -------------------------------------------------- #\n",
    "    oxhnet.eval()  # 切换为验证模型，BN和Dropout不起作用\n",
    "    \n",
    "    acc = 0.0   # 验证集准确率\n",
    "    ac = 0.0\n",
    "    \n",
    "    with torch.no_grad():  # 下面不进行梯度计算\n",
    "        \n",
    "        # 每次验证一个batch, 该batch内所有图片进行预测，然后和标准标签对比看正确率，再进行累加，恐怖啊，要预测10轮\n",
    "        for i, data_test in enumerate(test_loader):\n",
    "        # for data_test in val_loader:\n",
    "            \n",
    "            # 获取验证集的图片和标签\n",
    "            test_images, test_labels = data_test\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = oxhnet(test_images.to(device))\n",
    "            \n",
    "            # 预测分数的最大值\n",
    "            predict_y = torch.max(outputs, dim=1)[1]\n",
    "            \n",
    "            # 累加每个step的准确率\n",
    "            acc += (predict_y == test_labels.to(device)).sum().item()\n",
    "            \n",
    "            ac += (predict_y == test_labels.to(device)).sum().item()\n",
    "            if (i+1)%10 == 0: # 20 *10 datapoints\n",
    "                avg_acc = ac / (10*20)\n",
    "                writer.add_scalar('prediction accuracy_10batcheserPoint', avg_acc, epoch * len(test_loader) + i)\n",
    "                ac = 0.0\n",
    "        \n",
    "        # 计算所有图片的平均准确率\n",
    "        acc_test = acc / len(test_data)\n",
    "        \n",
    "        # 打印每个epoch的训练损失和验证准确率\n",
    "        print(f'total_train_loss:{running_loss/step}, total_test_acc:{acc_test}')\n",
    " \n",
    "        \n",
    "# -------------------------------------------------- #\n",
    "#（7）权重保存\n",
    "# -------------------------------------------------- #\n",
    "        # 保存最好的准确率的权重\n",
    "        if acc_test > best_acc:\n",
    "            # 更新最佳的准确率\n",
    "            best_acc = acc_test\n",
    "            # 保存的权重名称\n",
    "            savename = savepath + 'resnet50ForDR_5features(CellWall).pth'\n",
    "            # 保存当前权重\n",
    "            torch.save(oxhnet.state_dict(), savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c519e40-08e6-4062-98a4-c163890b2a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "oxhnet.eval()  # Set the model to evaluation mode\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = oxhnet(images)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 计算混淆矩阵和分类报告\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "class_report = classification_report(all_labels, all_predictions, target_names=class_names, output_dict=True)\n",
    "\n",
    "# 将混淆矩阵转换为 DataFrame 并明确标识预测和真实\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix, \n",
    "    index=[f'{cn}_true' for cn in class_names], \n",
    "    columns=[f'{cn}_pred' for cn in class_names]\n",
    ")\n",
    "\n",
    "# 将分类报告转换为 DataFrame\n",
    "class_report_df = pd.DataFrame(class_report).transpose()\n",
    "\n",
    "# 确保输出目录存在\n",
    "output_dir = './runs/20241221_ResNet50_5features_ForDR(CellWall)'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 写入 CSV\n",
    "output_csv = os.path.join(output_dir, '20241221confusion_matrix_and_classification_report.csv')\n",
    "conf_matrix_df.to_csv(output_csv, mode='w')  # Write mode\n",
    "class_report_df.to_csv(output_csv, mode='a')  # Append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76440f28-2a1a-44ad-9229-9c238f43323e",
   "metadata": {},
   "outputs": [],
   "source": [
    "oxhnet.eval()  # Set model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        outputs = oxhnet(images.to(device))\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        top_p, top_class = probabilities.topk(2, dim=1)  # Get top 2 predictions\n",
    "        _, predicted = torch.max(probabilities, 1)  # Get the index of max log-probability\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "        if i == 0:  # Process only the first batch\n",
    "            plt.figure(figsize=(15, 12))  # Define the size of the figure\n",
    "\n",
    "            for j in range(images.shape[0]):\n",
    "                plt.subplot(4, 5, j + 1)\n",
    "                img = images[j].cpu().squeeze()  # Assuming images are already in CPU and grayscale\n",
    "                plt.imshow(img, cmap='gray')\n",
    "                title_text = f\"True: {class_names[labels[j]]}\\n\" \\\n",
    "                             f\"1st: {class_names[top_class[j][0]]} ({top_p[j][0]:.2f})\\n\" \\\n",
    "                             f\"2nd: {class_names[top_class[j][1]]} ({top_p[j][1]:.2f})\"\n",
    "                plt.title(title_text, fontsize=10)\n",
    "                plt.axis('off')\n",
    "\n",
    "            plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "            result_directory = './0729DR_SingleCell_padding_Classified'\n",
    "            if not os.path.exists(result_directory):\n",
    "                os.makedirs(result_directory)\n",
    "            image_filename = os.path.join(result_directory, 'prediction_results.png')\n",
    "            plt.savefig(image_filename)\n",
    "            plt.show()\n",
    "            break  # Stop after processing the first batch\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = (correct / total) * 100\n",
    "print(f'Accuracy of the network on the train images: {accuracy:.2f}%')\n",
    "print('Test Accuracy: {:.2f}%'.format(100 * correct / total))\n",
    "print('True labels:     ', labels)\n",
    "print('Predicted labels:', predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
